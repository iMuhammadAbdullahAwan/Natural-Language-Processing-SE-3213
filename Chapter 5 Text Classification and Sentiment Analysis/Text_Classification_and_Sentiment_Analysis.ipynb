{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1612eb7-7bf9-464c-9ee0-9c85b8b9897c",
   "metadata": {},
   "source": [
    "As a beginner with a passion for NLP and a long-term goal in R&D, you’re working through the updated course outline to build a strong foundation, having completed Chapters 1–4 (Introduction to NLP, Text Preprocessing, Text Representation, Basic NLP Tasks). Below is a detailed, beginner-friendly version of **Chapter 5: Text Classification and Sentiment Analysis**, tailored for someone with basic Python skills and knowledge of preprocessing, text representation, and POS/NER from prior chapters. This chapter introduces supervised learning and text classification, focusing on sentiment analysis, a key NLP task with wide applications in research and industry. It aligns with your R&D aspirations by emphasizing hands-on practice, research connections, and portfolio-building, while keeping the content engaging and accessible.\n",
    "\n",
    "This chapter includes:\n",
    "- **Theory**: Clear explanations of supervised learning, text classification, and sentiment analysis, designed for beginners.\n",
    "- **Practical**: Step-by-step tasks using free tools (Scikit-learn, SpaCy) and a new dataset (IMDB reviews) for consistency and depth.\n",
    "- **Mini-Project**: A Sentiment Classifier to predict movie review sentiment, building coding skills and a portfolio piece.\n",
    "- **Resources**: Free, beginner-friendly materials.\n",
    "- **Debugging Tips**: Solutions to common beginner issues.\n",
    "- **Checkpoints**: Quizzes and tasks to confirm mastery.\n",
    "- **R&D Focus**: Links to research concepts (e.g., evaluation metrics) to inspire your long-term goal.\n",
    "\n",
    "The dataset (IMDB reviews) leverages your familiarity from Chapter 2 but focuses on classification, ensuring continuity while introducing new challenges. The content is structured for self-paced learning, with clear steps to build confidence and prepare for advanced NLP research.\n",
    "\n",
    "**Time Estimate**: ~20 hours (spread over 1–2 weeks, 10–12 hours/week).  \n",
    "**Tools**: Free (Google Colab, Scikit-learn, SpaCy, Pandas).  \n",
    "**Dataset**: [IMDB Dataset of 50k Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) (free on Kaggle).  \n",
    "**Prerequisites**: Basic Python (Chapter 1), text preprocessing (Chapter 2), text representation (Chapter 3), POS/NER (Chapter 4); Colab or Anaconda setup with libraries (`pip install scikit-learn spacy pandas`).  \n",
    "**Date**: June 19, 2025.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter 5: Text Classification and Sentiment Analysis**\n",
    "\n",
    "*Goal*: Learn supervised learning and build a text classifier for sentiment analysis, mastering evaluation metrics and preparing for research-grade NLP tasks.\n",
    "\n",
    "### **Theory (5 hours)**\n",
    "\n",
    "#### **What is Text Classification?**\n",
    "- **Definition**: Assigning predefined labels to text based on its content, a supervised learning task.\n",
    "  - Example: Labeling a movie review as “positive” or “negative.”\n",
    "- **Why It Matters**: Powers applications like spam detection, topic classification, and sentiment analysis.\n",
    "- **R&D Relevance**: In research, text classification is used for tasks like detecting bias in social media or classifying medical reports.\n",
    "\n",
    "#### **Key Concepts**\n",
    "1. **Supervised Learning**:\n",
    "   - **What**: Training a model on labeled data (input text + correct labels) to predict labels for new text.\n",
    "   - **Example**: Training on IMDB reviews (text + “positive”/“negative”) to predict sentiment.\n",
    "   - **Steps**:\n",
    "     - Collect labeled data (e.g., reviews).\n",
    "     - Split into training (80%) and test (20%) sets.\n",
    "     - Train a model (e.g., Logistic Regression).\n",
    "     - Evaluate on test set.\n",
    "   - **Key Idea**: Models learn patterns (e.g., “great” → positive) from training data.\n",
    "2. **Text Classification**:\n",
    "   - **Types**:\n",
    "     - Binary: Two classes (e.g., positive vs. negative).\n",
    "     - Multi-class: Multiple classes (e.g., news topics: sports, business, tech).\n",
    "     - Multi-label: Multiple labels per text (e.g., a review is both “positive” and “funny”).\n",
    "   - **Models**:\n",
    "     - Logistic Regression: Simple, interpretable, good for text.\n",
    "     - Naive Bayes: Probabilistic, fast for small datasets.\n",
    "     - Neural Networks: Advanced, used in research (e.g., BERT, Chapter 9).\n",
    "   - **Features**: Use BoW or TF-IDF (Chapter 3) as input to models.\n",
    "3. **Sentiment Analysis**:\n",
    "   - **What**: Classifying text by emotional tone (e.g., positive, negative, neutral).\n",
    "   - **Example**: “I loved this movie!” → Positive; “It was boring” → Negative.\n",
    "   - **Applications**: Analyzing X posts, customer reviews, or political speeches.\n",
    "   - **Challenges**: Sarcasm, ambiguity (e.g., “Great job!” can be negative).\n",
    "4. **Evaluation Metrics**:\n",
    "   - **Accuracy**: % of correct predictions (good for balanced data).\n",
    "   - **Precision**: % of positive predictions that are correct (important for imbalanced data).\n",
    "   - **Recall**: % of actual positives correctly predicted.\n",
    "   - **F1 Score**: Harmonic mean of precision and recall (balances both).\n",
    "   - **Example**: If 90/100 predictions are correct, accuracy = 90%. If 10/10 predicted positives are correct, precision = 100%.\n",
    "   - **Research Insight**: F1 score is widely used in NLP papers for robust evaluation.\n",
    "\n",
    "#### **Trade-Offs**\n",
    "- **Logistic Regression vs. Naive Bayes**:\n",
    "  - Logistic Regression: More accurate, slower on large data.\n",
    "  - Naive Bayes: Faster, assumes word independence (less realistic).\n",
    "- **BoW vs. TF-IDF**:\n",
    "  - BoW: Simpler, but common words dominate.\n",
    "  - TF-IDF: Weights rare words, better for classification.\n",
    "- **Challenges**: Imbalanced data (e.g., more positive reviews) or noisy text (e.g., typos) can hurt performance.\n",
    "\n",
    "#### **Resources**\n",
    "- [Scikit-learn Text Classification Tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html): Beginner guide to classifiers.\n",
    "- [Google’s ML Crash Course](https://developers.google.com/machine-learning/crash-course/classification): Supervised learning basics.\n",
    "- [Jurafsky’s NLP Book, Chapter 4](https://web.stanford.edu/~jurafsky/slp3/4.pdf): Free PDF on classification.\n",
    "- **R&D Resource**: Skim the introduction of [Socher, 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) (5 minutes) for sentiment analysis in research.\n",
    "\n",
    "#### **Learning Tips**\n",
    "- Note why F1 score is better than accuracy for imbalanced data.\n",
    "- Search X for #NLP or #SentimentAnalysis to see real-world examples (I can analyze posts if you share links).\n",
    "- Think about using sentiment analysis for your R&D goal (e.g., analyzing X posts for public opinion).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical (10 hours)**\n",
    "\n",
    "*Goal*: Build and evaluate a text classifier for sentiment analysis, applying preprocessing and representation skills.\n",
    "\n",
    "#### **Setup**\n",
    "- **Environment**: Google Colab (free GPU) or Anaconda (from Chapter 1).\n",
    "- **Libraries**: Install (run in Colab or terminal):\n",
    "  ```bash\n",
    "  pip install scikit-learn spacy pandas\n",
    "  python -m spacy download en_core_web_sm\n",
    "  ```\n",
    "- **Dataset**: [IMDB Dataset of 50k Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n",
    "  - Download: Sign up for Kaggle, download `IMDB Dataset.csv`.\n",
    "  - Why? Familiar from Chapter 2, labeled (positive/negative), ideal for classification.\n",
    "  - Columns: `review` (text), `sentiment` (positive/negative).\n",
    "- **Load Data**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  df = pd.read_csv('IMDB Dataset.csv')[:1000]  # Use 1000 reviews for speed\n",
    "  print(df.head())\n",
    "  ```\n",
    "\n",
    "#### **Tasks**\n",
    "1. **Preprocessing (2 hours)**:\n",
    "   - Clean and preprocess reviews using Chapter 2 skills (remove URLs, lowercase, lemmatize).\n",
    "   - Code:\n",
    "     ```python\n",
    "     import spacy\n",
    "     import re\n",
    "     nlp = spacy.load(\"en_core_web_sm\")\n",
    "     def preprocess(text):\n",
    "         cleaned = re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', text.lower())\n",
    "         doc = nlp(cleaned)\n",
    "         return ' '.join([token.lemma_ for token in doc if token.text not in nlp.Defaults.stop_words and token.is_alpha])\n",
    "     df['cleaned_text'] = df['review'].apply(preprocess)\n",
    "     print(df['cleaned_text'].head())\n",
    "     ```\n",
    "   - Output: Cleaned text (e.g., “love movie great”).\n",
    "2. **Feature Extraction with TF-IDF (2 hours)**:\n",
    "   - Convert text to TF-IDF vectors (Chapter 3).\n",
    "   - Code:\n",
    "     ```python\n",
    "     from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "     vectorizer = TfidfVectorizer(max_features=5000)\n",
    "     X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "     y = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "     print(X.shape)  # (1000, 5000)\n",
    "     ```\n",
    "   - Output: Matrix shape (1000 reviews × 5000 features).\n",
    "3. **Train Logistic Regression (2 hours)**:\n",
    "   - Split data and train a classifier.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from sklearn.model_selection import train_test_split\n",
    "     from sklearn.linear_model import LogisticRegression\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "     model = LogisticRegression(max_iter=1000)\n",
    "     model.fit(X_train, y_train)\n",
    "     y_pred = model.predict(X_test)\n",
    "     print(y_pred[:10])\n",
    "     ```\n",
    "   - Output: Predicted labels (e.g., [1, 0, 1]).\n",
    "4. **Evaluate Model (2 hours)**:\n",
    "   - Compute accuracy, precision, recall, F1 score.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "     print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "     print(f\"Precision: {precision_score(y_test, y_pred):.2f}\")\n",
    "     print(f\"Recall: {recall_score(y_test, y_pred):.2f}\")\n",
    "     print(f\"F1 Score: {f1_score(y_test, y_pred):.2f}\")\n",
    "     ```\n",
    "   - Output: e.g., Accuracy: 0.85, F1: 0.84.\n",
    "5. **Try Naive Bayes (2 hours)**:\n",
    "   - Compare with Logistic Regression.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from sklearn.naive_bayes import MultinomialNB\n",
    "     nb_model = MultinomialNB()\n",
    "     nb_model.fit(X_train, y_train)\n",
    "     y_pred_nb = nb_model.predict(X_test)\n",
    "     print(f\"Naive Bayes F1: {f1_score(y_test, y_pred_nb):.2f}\")\n",
    "     ```\n",
    "   - Output: e.g., F1: 0.82.\n",
    "\n",
    "#### **Debugging Tips**\n",
    "- Low accuracy? Check class balance (`df['sentiment'].value_counts()`) or increase `max_features`.\n",
    "- Model not converging? Increase `max_iter` in LogisticRegression.\n",
    "- SpaCy slow? Process in batches (e.g., 200 reviews at a time).\n",
    "- Memory issues? Reduce to 500 reviews (`df[:500]`).\n",
    "- CSV error? Use absolute path (e.g., `/content/IMDB Dataset.csv` in Colab).\n",
    "\n",
    "#### **Resources**\n",
    "- [Scikit-learn Classification](https://scikit-learn.org/stable/modules/linear_model.html): Logistic Regression guide.\n",
    "- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html): Evaluation metrics.\n",
    "- [SpaCy Preprocessing](https://spacy.io/usage/linguistic-features): Lemmatization review.\n",
    "- [Kaggle Pandas](https://www.kaggle.com/learn/pandas): Data handling.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mini-Project: Sentiment Classifier (5 hours)**\n",
    "\n",
    "*Goal*: Build a sentiment classifier for IMDB reviews, evaluate its performance, and create a portfolio piece for R&D.\n",
    "\n",
    "- **Task**: Train a Logistic Regression classifier on 1,000 IMDB reviews, predict sentiment, and report evaluation metrics.\n",
    "- **Input**: `IMDB Dataset.csv` (first 1,000 reviews).\n",
    "- **Output**: \n",
    "  - CSV with predictions: `review`, `true_sentiment`, `predicted_sentiment`.\n",
    "  - Text file with metrics: `accuracy`, `precision`, `recall`, `F1`.\n",
    "  - Plot of precision-recall curve.\n",
    "- **Steps**:\n",
    "  1. Preprocess reviews (clean, lemmatize).\n",
    "  2. Create TF-IDF features.\n",
    "  3. Train Logistic Regression on 80% data, test on 20%.\n",
    "  4. Compute and save metrics.\n",
    "  5. Plot precision-recall curve.\n",
    "- **Example Output**:\n",
    "  - CSV:\n",
    "    ```csv\n",
    "    review,true_sentiment,predicted_sentiment\n",
    "    \"I loved this movie!\",positive,positive\n",
    "    \"It was boring.\",negative,negative\n",
    "    ```\n",
    "  - Text:\n",
    "    ```\n",
    "    Accuracy: 0.85\n",
    "    Precision: 0.86\n",
    "    Recall: 0.84\n",
    "    F1 Score: 0.85\n",
    "    ```\n",
    "  - Plot: Precision-recall curve showing trade-off.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  import re\n",
    "  import spacy\n",
    "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  # Load SpaCy\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "  # Preprocess\n",
    "  def preprocess(text):\n",
    "      cleaned = re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', text.lower())\n",
    "      doc = nlp(cleaned)\n",
    "      return ' '.join([token.lemma_ for token in doc if token.text not in nlp.Defaults.stop_words and token.is_alpha])\n",
    "\n",
    "  # Load data\n",
    "  df = pd.read_csv('IMDB Dataset.csv')[:1000]\n",
    "  df['cleaned_text'] = df['review'].apply(preprocess)\n",
    "\n",
    "  # TF-IDF\n",
    "  vectorizer = TfidfVectorizer(max_features=5000)\n",
    "  X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "  y = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "  # Train-test split\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Train model\n",
    "  model = LogisticRegression(max_iter=1000)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Metrics\n",
    "  metrics = {\n",
    "      'Accuracy': accuracy_score(y_test, y_pred),\n",
    "      'Precision': precision_score(y_test, y_pred),\n",
    "      'Recall': recall_score(y_test, y_pred),\n",
    "      'F1 Score': f1_score(y_test, y_pred)\n",
    "  }\n",
    "  with open('sentiment_metrics.txt', 'w') as f:\n",
    "      for k, v in metrics.items():\n",
    "          f.write(f\"{k}: {v:.2f}\\n\")\n",
    "\n",
    "  # Save predictions\n",
    "  results = pd.DataFrame({\n",
    "      'review': df['review'].iloc[-200:],  # Last 200 (test set)\n",
    "      'true_sentiment': df['sentiment'].iloc[-200:],\n",
    "      'predicted_sentiment': ['positive' if p == 1 else 'negative' for p in y_pred]\n",
    "  })\n",
    "  results.to_csv('sentiment_predictions.csv', index=False)\n",
    "\n",
    "  # Precision-recall curve\n",
    "  y_scores = model.predict_proba(X_test)[:, 1]\n",
    "  precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "  plt.plot(recall, precision)\n",
    "  plt.xlabel('Recall')\n",
    "  plt.ylabel('Precision')\n",
    "  plt.title('Precision-Recall Curve')\n",
    "  plt.savefig('pr_curve.png')\n",
    "  plt.show()\n",
    "  ```\n",
    "- **Tools**: Scikit-learn, SpaCy, Pandas, Matplotlib.\n",
    "- **Variation**: Try Naive Bayes or increase `max_features` to compare performance.\n",
    "- **Debugging Tips**:\n",
    "  - CSV not saving? Use absolute path (e.g., `/content/sentiment_predictions.csv`).\n",
    "  - Plot not showing? Run `plt.show()` or use `%matplotlib inline` in Colab.\n",
    "  - Low F1? Check preprocessing or class balance.\n",
    "- **Resources**:\n",
    "  - [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics).\n",
    "  - [Matplotlib Plotting](https://matplotlib.org/stable/users/explain/quick_start.html).\n",
    "- **R&D Tip**: Add this project to your GitHub portfolio. Document preprocessing and evaluation choices to show research rigor.\n",
    "\n",
    "---\n",
    "\n",
    "### **Checkpoints**\n",
    "\n",
    "1. **Quiz (30 minutes)**:\n",
    "   - Questions:\n",
    "     1. What is supervised learning in the context of text classification?\n",
    "     2. How does sentiment analysis differ from general text classification?\n",
    "     3. Why is F1 score preferred over accuracy for imbalanced data?\n",
    "     4. What role does TF-IDF play in classification?\n",
    "   - Answers (example):\n",
    "     1. Training a model on labeled text to predict labels for new text.\n",
    "     2. Sentiment analysis classifies emotional tone (e.g., positive/negative); general classification includes other tasks (e.g., spam detection).\n",
    "     3. F1 balances precision and recall, robust for uneven class distributions.\n",
    "     4. TF-IDF converts text to weighted vectors, highlighting important words.\n",
    "   - **Task**: Write answers in a notebook or share on X with #NLP.\n",
    "\n",
    "2. **Task (30 minutes)**:\n",
    "   - Check `sentiment_predictions.csv`: Are predictions logical (e.g., “I loved it” → positive)?\n",
    "   - Inspect `sentiment_metrics.txt`: Is F1 score >0.80? If not, revisit preprocessing.\n",
    "   - Verify `pr_curve.png`: Does it show a trade-off (high precision, low recall)?\n",
    "   - Save files to GitHub; share on X for feedback.\n",
    "   - **R&D Connection**: Evaluating metrics like F1 is a research skill for model validation.\n",
    "\n",
    "---\n",
    "\n",
    "### **R&D Focus**\n",
    "\n",
    "- **Why It Matters**: Sentiment analysis is a key research area, used in social media analysis, customer feedback, and bias detection.\n",
    "- **Action**: Skim the introduction of [Socher, 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) (5 minutes). Note how it discusses recursive neural networks for sentiment.\n",
    "- **Community**: Share your metrics or PR curve on X with #NLP or [Hugging Face Discord](https://huggingface.co/join-discord). Ask for feedback on model performance.\n",
    "- **Research Insight**: Experiment with `max_features` (e.g., 5000 vs. 10000) to see its impact on F1, mimicking research optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Execution Plan**\n",
    "\n",
    "**Total Time**: ~20 hours (1–2 weeks, 10–12 hours/week).  \n",
    "- **Day 1–2**: Theory (5 hours). Read Scikit-learn tutorial, Jurafsky Chapter 4, note evaluation metrics.  \n",
    "- **Day 3–5**: Practical (10 hours). Complete tasks (preprocessing, TF-IDF, Logistic Regression, evaluation).  \n",
    "- **Day 6–7**: Mini-Project (5 hours). Build Sentiment Classifier, save CSV/text/plot, share on GitHub/X.  \n",
    "\n",
    "**Tips for Success**:\n",
    "- **Stay Motivated**: Think about using sentiment analysis for your R&D goal (e.g., analyzing X posts for sentiment trends).  \n",
    "- **Debugging**: Search errors on [Stack Overflow](https://stackoverflow.com/) or ask in Hugging Face Discord.  \n",
    "- **Portfolio**: Add `sentiment_predictions.csv`, `sentiment_metrics.txt`, `pr_curve.png`, and code to GitHub with comments explaining steps.  \n",
    "- **Foundation Check**: If you complete the mini-project in <5 hours and achieve F1 >0.80, you’re ready for Chapter 6 (Language Models).  \n",
    "- **Variation**: If you want a new dataset, try [Twitter Sentiment](https://www.kaggle.com/datasets/kazanova/sentiment140) for shorter texts.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Chapter is Ideal for You**\n",
    "\n",
    "- **Beginner-Friendly**: Simple explanations, step-by-step code, and free tools make classification accessible.  \n",
    "- **Practical**: Hands-on tasks and a mini-project build coding skills for research applications.  \n",
    "- **Research-Oriented**: Connects classification to research tasks, with paper references for R&D.  \n",
    "- **Engaging**: IMDB reviews are relatable, keeping your passion alive.  \n",
    "- **Structured**: Clear timeline, debugging tips, and checkpoints ensure progress.  \n",
    "\n",
    "This chapter strengthens your NLP foundation by mastering text classification and sentiment analysis, essential for R&D, while building a portfolio piece. If you want a detailed code walkthrough (e.g., precision-recall curve), a different dataset (e.g., Twitter), or help with specific issues (e.g., low F1 score), let me know! Ready to start with the theory or setup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eef3fd-4d43-41fa-aace-336f26356c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
