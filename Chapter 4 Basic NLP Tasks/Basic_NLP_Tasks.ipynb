{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbad5d3-f7f9-4f7b-af23-bd1196323040",
   "metadata": {},
   "source": [
    "As a beginner with a passion for NLP and a long-term goal in R&D, you’re revisiting the basics to build a strong foundation and have completed Chapters 1–3 (Introduction to NLP, Text Preprocessing, Text Representation) from the updated course outline. Below is a detailed, beginner-friendly version of **Chapter 4: Basic NLP Tasks**, designed for someone with basic Python skills and preprocessing/representation knowledge from prior chapters. This chapter introduces fundamental NLP tasks like Part-of-Speech (POS) tagging and Named Entity Recognition (NER), which are essential for understanding text structure and extracting meaningful information, key skills for NLP research and applications.\n",
    "\n",
    "This chapter includes:\n",
    "- **Theory**: Simple explanations of POS tagging and NER, tailored for beginners.\n",
    "- **Practical**: Step-by-step tasks using free tools (SpaCy, NLTK) and a new dataset (Wikipedia articles) to avoid repetition.\n",
    "- **Mini-Project**: An Entity Extractor to identify entities in news text, building coding skills and portfolio material.\n",
    "- **Resources**: Free, beginner-friendly materials.\n",
    "- **Debugging Tips**: Solutions to common beginner issues.\n",
    "- **Checkpoints**: Quizzes and tasks to confirm mastery.\n",
    "- **R&D Focus**: Connections to research (e.g., NER evaluation metrics) to align with your R&D goals.\n",
    "\n",
    "The dataset (Wikipedia articles) is fresh compared to your previous work (e.g., Gutenberg, IMDB, BBC News), ensuring variety. The content is structured for self-paced learning, with clear steps to build confidence and prepare for advanced NLP research.\n",
    "\n",
    "**Time Estimate**: ~15 hours (spread over 1–2 weeks, 7–10 hours/week).  \n",
    "**Tools**: Free (Google Colab, SpaCy, NLTK, Pandas).  \n",
    "**Dataset**: [Wikipedia NLP Dataset](https://www.kaggle.com/datasets/dhruvilp/nlp-wiki-dataset) (free on Kaggle) or a single Wikipedia article scraped with BeautifulSoup.  \n",
    "**Prerequisites**: Basic Python (Chapter 1), text preprocessing (Chapter 2), text representation (Chapter 3); Colab or Anaconda setup with libraries (`pip install spacy nltk pandas beautifulsoup4`).  \n",
    "**Date**: June 17, 2025 (as provided).\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter 4: Basic NLP Tasks**\n",
    "\n",
    "*Goal*: Learn Part-of-Speech (POS) tagging and Named Entity Recognition (NER) to analyze text structure and extract key information, building foundational skills for NLP research.\n",
    "\n",
    "### **Theory (4 hours)**\n",
    "\n",
    "#### **What are Basic NLP Tasks?**\n",
    "- **Definition**: Tasks that analyze text structure (syntax) and extract specific information (e.g., entities), forming the building blocks of advanced NLP systems.\n",
    "  - Example: Identifying “Apple” as an organization in “Apple released a new iPhone.”\n",
    "- **Why They Matter**: POS tagging and NER help models understand sentence roles (e.g., nouns vs. verbs) and key entities (e.g., people, places), critical for tasks like chatbots or information extraction.\n",
    "- **R&D Relevance**: In research, POS and NER are used in applications like question answering, dialogue systems, and knowledge graph construction.\n",
    "\n",
    "#### **Key Tasks**\n",
    "1. **Part-of-Speech (POS) Tagging**:\n",
    "   - **What**: Assigning grammatical tags (e.g., noun, verb, adjective) to each word in a sentence.\n",
    "   - **Example**: “The cat runs” → [(“The”, DET), (“cat”, NOUN), (“runs”, VERB)].\n",
    "   - **Tags**: Common tags include NOUN, VERB, ADJ (adjective), ADV (adverb), DET (determiner).\n",
    "   - **Why**: Helps understand sentence structure (syntax), useful for parsing or text generation.\n",
    "   - **Tool**: SpaCy (accurate, beginner-friendly) or NLTK (simpler but less robust).\n",
    "2. **Named Entity Recognition (NER)**:\n",
    "   - **What**: Identifying and classifying named entities (e.g., PERSON, ORGANIZATION, LOCATION) in text.\n",
    "   - **Example**: “Barack Obama visited London” → [(“Barack Obama”, PERSON), (“London”, LOCATION)].\n",
    "   - **Entity Types**: PERSON, ORG (organization), GPE (geopolitical entity), DATE, etc.\n",
    "   - **Why**: Extracts key information for tasks like search engines or knowledge extraction.\n",
    "   - **Tool**: SpaCy (pre-trained models) or NLTK (basic NER).\n",
    "3. **Dependency Parsing (Intro)**:\n",
    "   - **What**: Analyzing relationships between words (e.g., subject-verb connections).\n",
    "   - **Example**: In “The cat runs,” “cat” is the subject of “runs.”\n",
    "   - **Why**: Provides deeper syntactic analysis, used in advanced tasks like question answering.\n",
    "   - **Tool**: SpaCy’s `displacy` for visualization.\n",
    "\n",
    "#### **Applications**\n",
    "- **POS Tagging**: Simplifying text for chatbots (e.g., extracting verbs for intent recognition).\n",
    "- **NER**: Building knowledge graphs (e.g., linking “Apple” to “Tim Cook”).\n",
    "- **Research Insight**: In R&D, POS and NER are evaluated with metrics like precision and F1 score to improve model accuracy.\n",
    "\n",
    "#### **Trade-Offs**\n",
    "- **SpaCy vs. NLTK**:\n",
    "  - SpaCy: More accurate, pre-trained models, but heavier.\n",
    "  - NLTK: Simpler, better for learning, but less accurate for NER.\n",
    "- **NER Challenges**: Ambiguity (e.g., “Washington” as person or place) requires context, a focus in research.\n",
    "\n",
    "#### **Resources**\n",
    "- [SpaCy Linguistic Features](https://spacy.io/usage/linguistic-features): POS and NER guide.\n",
    "- [NLTK Book, Chapter 7](https://www.nltk.org/book/ch07.html): Basic POS and NER.\n",
    "- [Stanford CS224N Lecture 3](https://www.youtube.com/watch?v=rmVRLeJRklI): Free video on POS/NER (optional).\n",
    "- **R&D Resource**: Skim the introduction of [Chen & Manning, 2014](https://nlp.stanford.edu/pubs/Chen-Manning-2014.pdf) (5 minutes) for parsing in research.\n",
    "\n",
    "#### **Learning Tips**\n",
    "- Note 3 applications of POS/NER (e.g., extracting entities for a chatbot).\n",
    "- Search X for #NLP or #NER to see real-world examples (I can analyze posts if you share links).\n",
    "- Think about how NER could help your R&D goal (e.g., extracting entities from X posts for bias analysis).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical (8 hours)**\n",
    "\n",
    "*Goal*: Apply POS tagging and NER to a real dataset, building coding skills and understanding text analysis.\n",
    "\n",
    "#### **Setup**\n",
    "- **Environment**: Google Colab (free GPU) or Anaconda (from Chapter 1).\n",
    "- **Libraries**: Install (run in Colab or terminal):\n",
    "  ```bash\n",
    "  pip install spacy nltk pandas beautifulsoup4\n",
    "  python -m spacy download en_core_web_sm\n",
    "  ```\n",
    "  ```python\n",
    "  import nltk\n",
    "  nltk.download('averaged_perceptron_tagger')\n",
    "  nltk.download('maxent_ne_chunker')\n",
    "  nltk.download('words')\n",
    "  ```\n",
    "- **Dataset**: [Wikipedia NLP Dataset](https://www.kaggle.com/datasets/dhruvilp/nlp-wiki-dataset) or scrape a single Wikipedia article (e.g., “Artificial Intelligence”).\n",
    "  - **Option 1: Kaggle**: Download `wiki.csv` (contains Wikipedia article text).\n",
    "  - **Option 2: Scrape**:\n",
    "    ```python\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "    with open('wiki_article.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "    ```\n",
    "  - Why? Wikipedia articles are rich in entities (people, organizations, dates), ideal for POS and NER practice.\n",
    "- **Load Data**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  # For Kaggle dataset\n",
    "  df = pd.read_csv('wiki.csv')[:10]  # Use 10 articles for speed\n",
    "  # OR for scraped article\n",
    "  with open('wiki_article.txt') as f:\n",
    "      text = f.read()\n",
    "  ```\n",
    "\n",
    "#### **Tasks**\n",
    "1. **POS Tagging with NLTK (2 hours)**:\n",
    "   - Tag words with parts of speech using NLTK.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from nltk import pos_tag, word_tokenize\n",
    "     text = df['text'].iloc[0] if 'df' in globals() else text[:1000]  # Limit for speed\n",
    "     tokens = word_tokenize(text)\n",
    "     pos_tags = pos_tag(tokens)\n",
    "     print(pos_tags[:10])  # First 10 tags\n",
    "     ```\n",
    "   - Output: List of (word, tag) pairs, e.g., [(“Artificial”, JJ), (“Intelligence”, NN)].\n",
    "2. **POS Tagging with SpaCy (2 hours)**:\n",
    "   - Use SpaCy for POS tagging and compare with NLTK.\n",
    "   - Code:\n",
    "     ```python\n",
    "     import spacy\n",
    "     nlp = spacy.load(\"en_core_web_sm\")\n",
    "     doc = nlp(text)\n",
    "     pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "     print(pos_tags[:10])\n",
    "     ```\n",
    "   - Output: Similar to NLTK but with SpaCy’s tags (e.g., NOUN, VERB).\n",
    "3. **NER with SpaCy (2 hours)**:\n",
    "   - Extract entities (PERSON, ORG, GPE, etc.).\n",
    "   - Code:\n",
    "     ```python\n",
    "     doc = nlp(text)\n",
    "     entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "     print(entities[:10])  # First 10 entities\n",
    "     ```\n",
    "   - Output: e.g., [(“Alan Turing”, PERSON), (“Google”, ORG), (“London”, GPE)].\n",
    "4. **Dependency Parsing Visualization (2 hours)**:\n",
    "   - Visualize sentence structure with SpaCy’s displaCy.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from spacy import displacy\n",
    "     doc = nlp(text[:200])  # Short sentence for visualization\n",
    "     displacy.render(doc, style=\"dep\", jupyter=True)  # Use jupyter=False if not in Colab\n",
    "     ```\n",
    "   - Output: Interactive dependency tree (e.g., “cat” → subject of “runs”).\n",
    "\n",
    "#### **Debugging Tips**\n",
    "- SpaCy model fails? Run `python -m spacy download en_core_web_sm`.\n",
    "- NLTK POS/NER fails? Run `nltk.download('averaged_perceptron_tagger')`, `nltk.download('maxent_ne_chunker')`, `nltk.download('words')`.\n",
    "- DisplaCy not showing? Use Colab (`jupyter=True`) or save as HTML (`displacy.render(doc, style=\"dep\", options={\"compact\": True}, page=True)`).\n",
    "- Memory issues? Limit text to 1,000 characters or 5 articles.\n",
    "- Scraping fails? Check internet or use Kaggle dataset.\n",
    "\n",
    "#### **Resources**\n",
    "- [SpaCy Linguistic Features](https://spacy.io/usage/linguistic-features#pos-tagging): POS and NER guide.\n",
    "- [NLTK Book, Chapter 7](https://www.nltk.org/book/ch07.html): POS and NER basics.\n",
    "- [BeautifulSoup Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): Scraping guide.\n",
    "- [Kaggle Pandas](https://www.kaggle.com/learn/pandas): Data loading.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mini-Project: Entity Extractor (3 hours)**\n",
    "\n",
    "*Goal*: Extract POS tags and entities from Wikipedia articles, saving results to a CSV and visualizing a dependency tree, creating a portfolio piece for R&D.\n",
    "\n",
    "- **Task**: Process 10 Wikipedia articles (or one scraped article), extract POS tags and entities, and visualize one sentence’s dependency tree.\n",
    "- **Input**: `wiki.csv` (first 10 articles) or `wiki_article.txt`.\n",
    "- **Output**: \n",
    "  - CSV with columns: `text`, `pos_tags`, `entities`.\n",
    "  - Dependency tree visualization (saved as `dep_tree.html`).\n",
    "- **Steps**:\n",
    "  1. Load text (Kaggle CSV or scraped article).\n",
    "  2. Preprocess: Clean text (remove URLs, lowercase from Chapter 2).\n",
    "  3. Extract POS tags and entities with SpaCy.\n",
    "  4. Save results to `wiki_entities.csv`.\n",
    "  5. Visualize one sentence’s dependency tree.\n",
    "- **Example Output**:\n",
    "  - CSV:\n",
    "    ```csv\n",
    "    text,pos_tags,entities\n",
    "    \"Alan Turing was a scientist\",\"[('Alan', 'PROPN'), ('Turing', 'PROPN'), ('was', 'AUX'), ('a', 'DET'), ('scientist', 'NOUN')]\",\"[('Alan Turing', 'PERSON')]\"\n",
    "    ```\n",
    "  - Visualization: Dependency tree for “Alan Turing was a scientist.”\n",
    "- **Code**:\n",
    "  ```python\n",
    "  import spacy\n",
    "  import pandas as pd\n",
    "  import re\n",
    "  from spacy import displacy\n",
    "\n",
    "  # Load SpaCy\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "  # Load data\n",
    "  try:\n",
    "      df = pd.read_csv('wiki.csv')[:10]\n",
    "      texts = df['text']\n",
    "  except:\n",
    "      with open('wiki_article.txt') as f:\n",
    "          texts = [f.read()[:1000]]  # Limit for speed\n",
    "\n",
    "  # Preprocessing and extraction\n",
    "  data = []\n",
    "  for text in texts:\n",
    "      cleaned = re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', text.lower())\n",
    "      doc = nlp(cleaned[:1000])  # Limit for speed\n",
    "      pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "      entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "      data.append([text[:100], pos_tags[:10], entities[:5]])  # Truncate for CSV\n",
    "\n",
    "  # Save to CSV\n",
    "  pd.DataFrame(data, columns=['text', 'pos_tags', 'entities']).to_csv('wiki_entities.csv')\n",
    "\n",
    "  # Visualize dependency tree\n",
    "  doc = nlp(texts[0][:200])  # First sentence\n",
    "  displacy.render(doc, style=\"dep\", options={\"compact\": True}, page=True, minify=True)\n",
    "  with open('dep_tree.html', 'w') as f:\n",
    "      f.write(displacy.render(doc, style=\"dep\", page=True))\n",
    "  ```\n",
    "- **Tools**: SpaCy, Pandas, BeautifulSoup (if scraping).\n",
    "- **Variation**: If you used NLTK in Chapters 1–3, focus on SpaCy. Try NLTK for POS:\n",
    "  ```python\n",
    "  from nltk import pos_tag, word_tokenize\n",
    "  tokens = word_tokenize(text)\n",
    "  pos_tags = pos_tag(tokens)\n",
    "  ```\n",
    "- **Debugging Tips**:\n",
    "  - CSV not saving? Use absolute path (e.g., `/content/wiki_entities.csv` in Colab).\n",
    "  - DisplaCy fails? Run in Colab with `jupyter=True` or save as HTML.\n",
    "  - Few entities? Use `en_core_web_lg` for better NER (`python -m spacy download en_core_web_lg`).\n",
    "- **Resources**:\n",
    "  - [SpaCy NER](https://spacy.io/usage/linguistic-features#named-entities).\n",
    "  - [DisplaCy Guide](https://spacy.io/usage/visualizers).\n",
    "- **R&D Tip**: Add this project to your GitHub portfolio. Document POS/NER choices to show research thinking.\n",
    "\n",
    "---\n",
    "\n",
    "### **Checkpoints**\n",
    "\n",
    "1. **Quiz (30 minutes)**:\n",
    "   - Questions:\n",
    "     1. What is POS tagging, and why is it useful?\n",
    "     2. What does NER identify in text?\n",
    "     3. How does dependency parsing differ from POS tagging?\n",
    "     4. Why is SpaCy preferred for NER in research?\n",
    "   - Answers (example):\n",
    "     1. POS tagging assigns grammatical roles (e.g., NOUN) to words, aiding syntax analysis.\n",
    "     2. NER identifies entities like PERSON, ORG, GPE.\n",
    "     3. Dependency parsing shows word relationships (e.g., subject-verb); POS tagging only tags words.\n",
    "     4. SpaCy’s pre-trained models are more accurate and faster for NER.\n",
    "   - **Task**: Write answers in a notebook or share on X with #NLP.\n",
    "\n",
    "2. **Task (30 minutes)**:\n",
    "   - Check `wiki_entities.csv`: Are entities logical (e.g., “Alan Turing” as PERSON)?\n",
    "   - Inspect `dep_tree.html`: Does the tree show correct relationships (e.g., subject-verb)?\n",
    "   - Save CSV and HTML to GitHub; share on X for feedback.\n",
    "   - **R&D Connection**: Validating entities and syntax is a research skill for building reliable models.\n",
    "\n",
    "---\n",
    "\n",
    "### **R&D Focus**\n",
    "\n",
    "- **Why It Matters**: POS and NER are foundational for research tasks like knowledge extraction and dialogue systems.\n",
    "- **Action**: Skim the introduction of [Chen & Manning, 2014](https://nlp.stanford.edu/pubs/Chen-Manning-2014.pdf) (5 minutes). Note how parsing improves NLP models.\n",
    "- **Community**: Share your CSV or dependency tree on X with #NLP or [Hugging Face Discord](https://huggingface.co/join-discord). Ask for feedback on entity accuracy.\n",
    "- **Research Insight**: Experiment with `en_core_web_sm` vs. `en_core_web_lg` to see NER performance differences, mimicking research evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Execution Plan**\n",
    "\n",
    "**Total Time**: ~15 hours (1–2 weeks, 7–10 hours/week).  \n",
    "- **Day 1–2**: Theory (4 hours). Read SpaCy guide, NLTK Chapter 7, note POS/NER applications.  \n",
    "- **Day 3–5**: Practical (8 hours). Complete tasks (POS, NER, dependency parsing).  \n",
    "- **Day 6–7**: Mini-Project (3 hours). Build Entity Extractor, save CSV/HTML, share on GitHub/X.  \n",
    "\n",
    "**Tips for Success**:\n",
    "- **Stay Motivated**: Think about using NER for your R&D goal (e.g., extracting entities from X posts).  \n",
    "- **Debugging**: Search errors on [Stack Overflow](https://stackoverflow.com/) or ask in Hugging Face Discord.  \n",
    "- **Portfolio**: Add `wiki_entities.csv`, `dep_tree.html`, and code to GitHub with comments explaining steps.  \n",
    "- **Foundation Check**: If you complete the mini-project in <3 hours and understand quiz answers, you’re ready for Chapter 5 (Text Classification).  \n",
    "- **Variation**: If you used news or reviews previously, Wikipedia’s structured text offers a new challenge.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Chapter is Ideal for You**\n",
    "\n",
    "- **Beginner-Friendly**: Simple explanations, step-by-step code, and free tools make POS/NER accessible.  \n",
    "- **Practical**: Hands-on tasks and a mini-project build coding skills for research analysis.  \n",
    "- **Research-Oriented**: Connects POS/NER to research tasks, with paper references for R&D.  \n",
    "- **Engaging**: Wikipedia articles are rich in entities, keeping your passion alive.  \n",
    "- **Structured**: Clear timeline, debugging tips, and checkpoints ensure progress.  \n",
    "\n",
    "This chapter strengthens your NLP foundation by mastering POS tagging and NER, essential for R&D, while building a portfolio piece. If you want a detailed code walkthrough (e.g., NER), a different dataset (e.g., Reddit), or help with specific issues (e.g., scraping), let me know! Ready to start with the theory or setup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b124c-3841-4d15-8f01-22d70e749236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
