{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "Eaay77UGmjDM",
        "outputId": "413f1ca2-2746-41f2-ba5a-3feee23dcb7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting numpy>=1.19.5 (from scikit-learn)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "e351c00a20124096a3a85b60f5f9a4f1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install scikit-learn gensim matplotlib seaborn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz3phTgbmmpO",
        "outputId": "fbd9dca1-83cd-4530-f1d8-7c9dfe8f2602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                               Text  \\\n",
            "0           0  US consumer confidence up\\n\\nConsumers' confid...   \n",
            "1           1  The 'ticking budget' facing the US\\n\\nThe budg...   \n",
            "2           2  Mitsubishi in Peugeot link talks\\n\\nTrouble-hi...   \n",
            "3           3  BMW reveals new models pipeline\\n\\nBMW is prep...   \n",
            "4           4  World leaders gather to face uncertainty\\n\\nMo...   \n",
            "\n",
            "                                             Summary  \n",
            "0  Wal-Mart, the largest US retailer, has said it...  \n",
            "1  Brute force budget cuts or spending caps would...  \n",
            "2  Trouble-hit Mitsubishi Motors is in talks with...  \n",
            "3  Typically it takes about three years from when...  \n",
            "4  More than 2,000 business and political leaders...  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/bbc_news_data.csv')  # Use 200 articles for speed\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W3Z24trpoKx",
        "outputId": "8298ded9-03af-43c2-b79d-09c6c914820e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2225, 1000)\n",
            "['000' '10' '100' '11' '12' '13' '14' '15' '16' '17']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "texts = df['Text']\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "bow_matrix = vectorizer.fit_transform(texts)\n",
        "print(bow_matrix.shape)  # (200, 1000)\n",
        "print(vectorizer.get_feature_names_out()[:10])  # First 10 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbqQWrY8poHc",
        "outputId": "63bfeaad-a352-4829-e4a4-901eabb83b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2225, 1000)\n",
            "['000' '10' '100' '11' '12' '13' '14' '15' '16' '17']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "print(tfidf_matrix.shape)\n",
        "print(vectorizer.get_feature_names_out()[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9c6HgOgvpoD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0f3926-8a82-4dbd-d1e5-b309028ed41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.7580614  -0.46328846 -0.16201992  0.30051965 -0.27837047  0.31323785\n",
            "  0.36420265  0.68751067 -0.7091084  -0.66670966 -0.06539743 -0.2151784\n",
            " -0.04964463 -0.39891583 -0.21194486 -0.10011972 -0.49945042  0.8452056\n",
            "  0.01825659 -0.37681946 -0.14904799 -0.26714993  1.0820867  -1.1476531\n",
            "  0.6415985  -0.44779834  0.34342757  0.6049159  -0.6721146  -0.00857114\n",
            "  0.2910533   0.21107951  0.7197886  -0.6757999  -0.16668555  0.14617321\n",
            " -1.0004002  -0.2459907  -0.4880838  -0.02149073  0.38531977 -0.6840683\n",
            " -0.52103513 -0.25027612  0.4416578  -0.4242215   0.1656143  -0.4507223\n",
            " -0.18233964  0.09088071  0.13498309 -0.02028232 -0.14351861 -0.3086081\n",
            " -0.18209137 -0.1758936   0.21805595 -0.02816815 -1.1756265   0.02277546\n",
            "  0.30353752 -0.32909873  0.90321064 -0.6483222  -0.3216958   0.38963005\n",
            "  0.4155281   0.30359912 -0.8283791   0.43221182  0.02336066  0.5658029\n",
            "  0.11600278  0.07292151  0.08520783  0.18338297  0.7872399   0.20134814\n",
            "  0.05976127  0.44556963  0.020968   -0.3087853  -0.12535176 -0.70765555\n",
            " -0.6212517  -0.56406623  0.6343085  -0.4730572  -0.1578284   0.75657254\n",
            "  0.11527787 -0.0012989  -0.20914784 -0.07939428  0.42363498  0.6754272\n",
            "  0.24233516 -0.4821637   0.13619293  0.01952852]\n",
            "[('economic', 0.8619655966758728), ('growth', 0.8396356105804443), ('deficit', 0.8319665789604187), ('europe', 0.8303236961364746), ('economy', 0.829407274723053)]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "# Preprocess (from Chapter 2 skills)\n",
        "texts = df['Text'].apply(lambda x: re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', x.lower()))\n",
        "tokenized_texts = [text.split() for text in texts]\n",
        "model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=5)\n",
        "print(model.wv['market'])  # Vector for \"market\"\n",
        "print(model.wv.most_similar('market', topn=5))  # Similar words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zgYptrV5poA-"
      },
      "outputs": [],
      "source": [
        "# from sklearn.manifold import TSNE\n",
        "# import matplotlib.pyplot as plt\n",
        "# words = list(model.wv.index_to_key)[:50]\n",
        "# embeddings = [model.wv[word] for word in words]\n",
        "# tsne = TSNE(n_components=2, random_state=42)\n",
        "# embeddings_2d = tsne.fit_transform(embeddings)\n",
        "# plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
        "# for i, word in enumerate(words):\n",
        "#     plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2l9jRm5kpn9w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/bbc_news_data.csv')[:200]\n",
        "df['Text'] = df['Text'].apply(lambda x: re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', x.lower()))\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(df['Text'])\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "top_terms = []\n",
        "for Summary in df['Summary'].unique():\n",
        "    Summary_texts = df[df['Summary'] == Summary]['Text']\n",
        "    Summary_tfidf = vectorizer.transform(Summary_texts).toarray()\n",
        "    avg_tfidf = Summary_tfidf.mean(axis=0)\n",
        "    top_indices = avg_tfidf.argsort()[-10:]\n",
        "    terms = [feature_names[i] for i in top_indices]\n",
        "    top_terms.append([Summary, terms])\n",
        "pd.DataFrame(top_terms, columns=['Summary', 'top_terms']).to_csv('news_analysis.csv')\n",
        "\n",
        "# Word2Vec\n",
        "tokenized_texts = [text.split() for text in df['Text']]\n",
        "model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=5)\n",
        "words = list(model.wv.index_to_key)[:50]\n",
        "embeddings = [model.wv[word] for word in words]\n",
        "\n",
        "# t-SNE\n",
        "# tsne = TSNE(n_components=2, random_state=42)\n",
        "# embeddings_2d = tsne.fit_transform(embeddings)\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
        "# for i, word in enumerate(words):\n",
        "#     plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
        "# plt.savefig('tsne_plot.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwP0la2LtLJg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}