{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9061ed-4d93-4aab-9d28-c1dbfc68ff18",
   "metadata": {},
   "source": [
    "As a beginner with a passion for NLP and a long-term goal in R&D, you’re revisiting the basics to build a strong foundation, having completed Chapters 1 (Introduction to NLP) and 2 (Text Preprocessing) from the updated course outline. Below is a detailed, beginner-friendly version of **Chapter 3: Text Representation**, designed for someone with basic Python skills (from Chapters 1–2) and no prior machine learning experience. This chapter focuses on transforming text into numerical formats for NLP models, a critical skill for research and practical applications. It aligns with your R&D aspirations by emphasizing hands-on practice, research connections, and portfolio-building, while keeping the content engaging and accessible.\n",
    "\n",
    "This chapter includes:\n",
    "- **Theory**: Clear explanations of Bag of Words (BoW), TF-IDF, and Word Embeddings, tailored for beginners.\n",
    "- **Practical**: Step-by-step tasks using free tools (Scikit-learn, Gensim) and a new dataset (BBC News) to avoid repetition.\n",
    "- **Mini-Project**: A News Analyzer to extract key terms and visualize embeddings, reinforcing coding and research skills.\n",
    "- **Resources**: Free, beginner-friendly materials.\n",
    "- **Debugging Tips**: Solutions to common beginner issues.\n",
    "- **Checkpoints**: Quizzes and tasks to confirm mastery.\n",
    "- **R&D Focus**: Links to research concepts (e.g., embeddings in NLP models) to inspire your long-term goal.\n",
    "\n",
    "The dataset (BBC News) is fresh compared to your previous work (e.g., Gutenberg, IMDB), ensuring variety. The content is structured for self-paced learning, with clear steps to build confidence and prepare for advanced NLP research.\n",
    "\n",
    "**Time Estimate**: ~20 hours (spread over 1–2 weeks, 10–12 hours/week).  \n",
    "**Tools**: Free (Google Colab, Scikit-learn, Gensim, Matplotlib, Pandas).  \n",
    "**Dataset**: [BBC News](https://www.kaggle.com/datasets/pariza/bbc-news) (free on Kaggle).  \n",
    "**Prerequisites**: Basic Python (lists, loops, file I/O from Chapter 1); text preprocessing (Chapter 2); Colab or Anaconda setup with libraries (`pip install scikit-learn gensim matplotlib seaborn pandas`).  \n",
    "**Date**: June 16, 2025 (as provided).\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter 3: Text Representation**\n",
    "\n",
    "*Goal*: Learn to convert text into numerical formats (BoW, TF-IDF, Word Embeddings) for NLP models, understanding their strengths and weaknesses, and building skills for research-grade data representation.\n",
    "\n",
    "### **Theory (5 hours)**\n",
    "\n",
    "#### **What is Text Representation?**\n",
    "- **Definition**: Converting text into numbers that NLP models can process, as computers can’t directly understand words.\n",
    "  - Example: Turning “I love movies” into a vector like [1, 1, 1] for analysis.\n",
    "- **Why It Matters**: Models like classifiers or neural networks need numerical inputs. Representation affects model accuracy and interpretability.\n",
    "- **R&D Relevance**: In research, choosing the right representation (e.g., embeddings for semantics) is critical for tasks like sentiment analysis or machine translation.\n",
    "\n",
    "#### **Key Representation Techniques**\n",
    "1. **Bag of Words (BoW)**:\n",
    "   - **What**: Represents text as a vector of word counts or presence (1 if word appears, 0 if not).\n",
    "   - **Example**: For “I love movies” and “I hate movies” with vocabulary [“I”, “love”, “hate”, “movies”]:\n",
    "     - “I love movies” → [1, 1, 0, 1]\n",
    "     - “I hate movies” → [1, 0, 1, 1]\n",
    "   - **Pros**: Simple, fast, works for basic tasks (e.g., spam detection).\n",
    "   - **Cons**: Ignores word order and meaning (e.g., “dog bites man” = “man bites dog”).\n",
    "   - **Tool**: Scikit-learn’s `CountVectorizer`.\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "   - **What**: Weights words based on frequency in a document (TF) and rarity across all documents (IDF).\n",
    "   - **Example**: In movie reviews, “the” has low TF-IDF (common), but “cinematography” has high TF-IDF (rare, meaningful).\n",
    "   - **Formula**: TF-IDF = TF (word count in document) × IDF (log(total documents / documents with word)).\n",
    "   - **Pros**: Highlights important words, better for tasks like text classification.\n",
    "   - **Cons**: Still ignores word order and semantics.\n",
    "   - **Tool**: Scikit-learn’s `TfidfVectorizer`.\n",
    "3. **Word Embeddings (Word2Vec)**:\n",
    "   - **What**: Dense vectors (e.g., 100-dimensional) capturing word meanings based on context.\n",
    "   - **Example**: “dog” and “puppy” have similar vectors (e.g., [0.5, -0.2, …] vs. [0.48, -0.18, …]) because they appear in similar contexts.\n",
    "   - **Key Idea**: Words with similar meanings are close in vector space (e.g., “king” - “man” + “woman” ≈ “queen”).\n",
    "   - **Pros**: Captures semantics, ideal for advanced tasks (e.g., sentiment, translation).\n",
    "   - **Cons**: Requires training data, computationally heavy.\n",
    "   - **Tool**: Gensim’s `Word2Vec`.\n",
    "\n",
    "#### **Trade-Offs**\n",
    "- **BoW**: Simple but loses meaning (e.g., no context for “good” vs. “great”).\n",
    "- **TF-IDF**: Better than BoW by weighting rare words, but still no semantics.\n",
    "- **Word2Vec**: Captures meaning but needs more data and computation.\n",
    "- **Research Insight**: In R&D, embeddings like Word2Vec or BERT (Chapter 9) are preferred for tasks requiring context (e.g., chatbots).\n",
    "\n",
    "#### **Visualization with t-SNE**\n",
    "- **What**: t-SNE (t-Distributed Stochastic Neighbor Embedding) reduces high-dimensional embeddings (e.g., 100D Word2Vec vectors) to 2D for plotting.\n",
    "- **Why**: Helps visualize word relationships (e.g., “dog” near “puppy” in a scatter plot).\n",
    "- **Tool**: Scikit-learn’s `TSNE`.\n",
    "\n",
    "#### **Resources**\n",
    "- [Scikit-learn Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html): BoW and TF-IDF guide.\n",
    "- [Gensim Word2Vec Tutorial](https://radimrehurek.com/gensim/models/word2vec.html): Embedding basics.\n",
    "- [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/): Visual explanation of embeddings.\n",
    "- [t-SNE Guide](https://scikit-learn.org/stable/modules/manifold.html#t-sne): Visualization basics.\n",
    "- **R&D Resource**: Skim the abstract of the [Word2Vec paper](https://arxiv.org/abs/1301.3781) (Mikolov, 2013) to understand embedding research.\n",
    "\n",
    "#### **Learning Tips**\n",
    "- Note how TF-IDF differs from BoW (weighting rare words).\n",
    "- Search X for #NLP to see discussions on embeddings (I can analyze posts if you share links).\n",
    "- Think about using embeddings for your R&D goal (e.g., detecting sentiment in X posts).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical (10 hours)**\n",
    "\n",
    "*Goal*: Apply BoW, TF-IDF, and Word2Vec to a real dataset, building coding skills and understanding numerical representations.\n",
    "\n",
    "#### **Setup**\n",
    "- **Environment**: Google Colab (free GPU) or Anaconda (from Chapter 1).\n",
    "- **Libraries**: Install (run in Colab or terminal):\n",
    "  ```bash\n",
    "  pip install scikit-learn gensim matplotlib seaborn pandas\n",
    "  ```\n",
    "- **Dataset**: [BBC News](https://www.kaggle.com/datasets/pariza/bbc-news).\n",
    "  - Download: Sign up for Kaggle, download `bbc_news.csv`.\n",
    "  - Why? News articles are diverse (business, sports, tech), great for practicing representations.\n",
    "  - Alternative: [20 Newsgroups](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) via Scikit-learn.\n",
    "- **Load Data**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  df = pd.read_csv('bbc_news.csv')[:200]  # Use 200 articles for speed\n",
    "  print(df.head())\n",
    "  ```\n",
    "\n",
    "#### **Tasks**\n",
    "1. **Bag of Words (BoW) (2 hours)**:\n",
    "   - Create a BoW matrix using Scikit-learn’s `CountVectorizer`.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from sklearn.feature_extraction.text import CountVectorizer\n",
    "     texts = df['text']\n",
    "     vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "     bow_matrix = vectorizer.fit_transform(texts)\n",
    "     print(bow_matrix.shape)  # (200, 1000)\n",
    "     print(vectorizer.get_feature_names_out()[:10])  # First 10 words\n",
    "     ```\n",
    "   - Output: Matrix shape (e.g., 200 documents × 1000 words), sample vocabulary.\n",
    "2. **TF-IDF (2 hours)**:\n",
    "   - Create a TF-IDF matrix using `TfidfVectorizer`.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "     vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "     tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "     print(tfidf_matrix.shape)\n",
    "     print(vectorizer.get_feature_names_out()[:10])\n",
    "     ```\n",
    "   - Output: Similar to BoW but with weighted values (e.g., 0.45 for “market”).\n",
    "3. **Word2Vec (3 hours)**:\n",
    "   - Train a Word2Vec model with Gensim on preprocessed text.\n",
    "   - Code:\n",
    "     ```python\n",
    "     import re\n",
    "     from gensim.models import Word2Vec\n",
    "     # Preprocess (from Chapter 2 skills)\n",
    "     texts = df['text'].apply(lambda x: re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', x.lower()))\n",
    "     tokenized_texts = [text.split() for text in texts]\n",
    "     model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=5)\n",
    "     print(model.wv['market'])  # Vector for \"market\"\n",
    "     print(model.wv.most_similar('market', topn=5))  # Similar words\n",
    "     ```\n",
    "   - Output: 100D vector for “market”; similar words like “economy,” “business.”\n",
    "4. **Visualize Embeddings with t-SNE (3 hours)**:\n",
    "   - Plot 50 frequent words’ embeddings in 2D.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from sklearn.manifold import TSNE\n",
    "     import matplotlib.pyplot as plt\n",
    "     words = list(model.wv.index_to_key)[:50]\n",
    "     embeddings = [model.wv[word] for word in words]\n",
    "     tsne = TSNE(n_components=2, random_state=42)\n",
    "     embeddings_2d = tsne.fit_transform(embeddings)\n",
    "     plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "     for i, word in enumerate(words):\n",
    "         plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "     plt.show()\n",
    "     ```\n",
    "   - Output: Scatter plot with words like “market,” “economy” clustered together.\n",
    "\n",
    "#### **Debugging Tips**\n",
    "- `CountVectorizer` fails? Ensure `stop_words='english'` or update Scikit-learn (`pip install --upgrade scikit-learn`).\n",
    "- t-SNE slow? Reduce to 50 words or use Colab GPU.\n",
    "- Word2Vec error? Update Gensim (`pip install --upgrade gensim`) or reduce `min_count`.\n",
    "- Memory issues? Limit to 100 articles (`df[:100]`) or batch process.\n",
    "- Plot not showing? Run `plt.show()` or use `%matplotlib inline` in Colab.\n",
    "\n",
    "#### **Resources**\n",
    "- [Scikit-learn Text Tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html): BoW and TF-IDF.\n",
    "- [Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html): Embedding guide.\n",
    "- [Matplotlib Plotting](https://matplotlib.org/stable/users/explain/quick_start.html): Visualization basics.\n",
    "- [Kaggle Pandas](https://www.kaggle.com/learn/pandas): Data loading.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mini-Project: News Analyzer (5 hours)**\n",
    "\n",
    "*Goal*: Analyze BBC News articles to extract key terms (TF-IDF) and visualize word relationships (Word2Vec + t-SNE), building a portfolio piece for R&D.\n",
    "\n",
    "- **Task**: Process 200 BBC News articles, extract top 10 TF-IDF terms per category (e.g., business, sports), train Word2Vec, and visualize embeddings.\n",
    "- **Input**: `bbc_news.csv` (first 200 articles).\n",
    "- **Output**: \n",
    "  - List of top 10 TF-IDF terms for 2 categories (e.g., business, sports).\n",
    "  - t-SNE plot of 50 words’ embeddings.\n",
    "  - Save results to `news_analysis.csv` and `tsne_plot.png`.\n",
    "- **Steps**:\n",
    "  1. Load and preprocess articles (clean, lowercase using Chapter 2 skills).\n",
    "  2. Create TF-IDF matrix and extract top terms per category.\n",
    "  3. Train Word2Vec on preprocessed text.\n",
    "  4. Visualize 50 words with t-SNE.\n",
    "  5. Save results.\n",
    "- **Example Output**:\n",
    "  - CSV:\n",
    "    ```csv\n",
    "    category,top_terms\n",
    "    business,\"market,economy,company,bank,profit\"\n",
    "    sports,\"game,team,player,match,win\"\n",
    "    ```\n",
    "  - Plot: Scatter with “market” near “economy,” “game” near “player.”\n",
    "- **Code**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  import re\n",
    "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "  from gensim.models import Word2Vec\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  # Load data\n",
    "  df = pd.read_csv('bbc_news.csv')[:200]\n",
    "  df['text'] = df['text'].apply(lambda x: re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', x.lower()))\n",
    "\n",
    "  # TF-IDF\n",
    "  vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "  tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "  feature_names = vectorizer.get_feature_names_out()\n",
    "  top_terms = []\n",
    "  for category in df['category'].unique():\n",
    "      category_texts = df[df['category'] == category]['text']\n",
    "      category_tfidf = vectorizer.transform(category_texts).toarray()\n",
    "      avg_tfidf = category_tfidf.mean(axis=0)\n",
    "      top_indices = avg_tfidf.argsort()[-10:]\n",
    "      terms = [feature_names[i] for i in top_indices]\n",
    "      top_terms.append([category, terms])\n",
    "  pd.DataFrame(top_terms, columns=['category', 'top_terms']).to_csv('news_analysis.csv')\n",
    "\n",
    "  # Word2Vec\n",
    "  tokenized_texts = [text.split() for text in df['text']]\n",
    "  model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=5)\n",
    "  words = list(model.wv.index_to_key)[:50]\n",
    "  embeddings = [model.wv[word] for word in words]\n",
    "\n",
    "  # t-SNE\n",
    "  tsne = TSNE(n_components=2, random_state=42)\n",
    "  embeddings_2d = tsne.fit_transform(embeddings)\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "  for i, word in enumerate(words):\n",
    "      plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "  plt.savefig('tsne_plot.png')\n",
    "  plt.show()\n",
    "  ```\n",
    "- **Tools**: Scikit-learn, Gensim, Matplotlib, Pandas.\n",
    "- **Variation**: If you used Scikit-learn previously, try [Gensim’s TfidfModel](https://radimrehurek.com/gensim/models/tfidfmodel.html) for TF-IDF.\n",
    "- **Debugging Tips**:\n",
    "  - CSV not saving? Use absolute path (e.g., `/content/news_analysis.csv` in Colab).\n",
    "  - t-SNE plot cluttered? Adjust `plt.figure(figsize=(10, 8))` or reduce words.\n",
    "  - Word2Vec fails? Check `min_count` or update Gensim.\n",
    "- **Resources**:\n",
    "  - [Scikit-learn Text Tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).\n",
    "  - [Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "- **R&D Tip**: Add this project to your GitHub portfolio. Document how TF-IDF and Word2Vec differ, showcasing research thinking.\n",
    "\n",
    "---\n",
    "\n",
    "### **Checkpoints**\n",
    "\n",
    "1. **Quiz (30 minutes)**:\n",
    "   - Questions:\n",
    "     1. What is text representation in NLP?\n",
    "     2. How does BoW differ from TF-IDF?\n",
    "     3. Why do word embeddings capture meaning better than BoW?\n",
    "     4. What does t-SNE do in the context of embeddings?\n",
    "   - Answers (example):\n",
    "     1. Converting text to numbers for models.\n",
    "     2. BoW counts words; TF-IDF weights rare words higher.\n",
    "     3. Embeddings use context to place similar words (e.g., “dog,” “puppy”) close in vector space.\n",
    "     4. t-SNE reduces high-dimensional embeddings to 2D for visualization.\n",
    "   - **Task**: Write answers in a notebook or share on X with #NLP.\n",
    "\n",
    "2. **Task (30 minutes)**:\n",
    "   - Check `news_analysis.csv`: Are top terms logical (e.g., “market” for business)?\n",
    "   - Inspect t-SNE plot: Are similar words (e.g., “game,” “player”) clustered?\n",
    "   - Save CSV and plot to GitHub; share on X for feedback.\n",
    "   - **R&D Connection**: Validating term relevance and clusters is a research skill for analyzing model inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **R&D Focus**\n",
    "\n",
    "- **Why It Matters**: Text representation is central to NLP research, as it determines how well models understand language (e.g., embeddings in BERT).\n",
    "- **Action**: Skim the abstract of the [Word2Vec paper](https://arxiv.org/abs/1301.3781) (5 minutes). Note how it mentions “semantic relationships” (e.g., “king” ≈ “queen”).\n",
    "- **Community**: Share your t-SNE plot or top terms on X with #NLP or [Hugging Face Discord](https://huggingface.co/join-discord). Ask for feedback on visualization clarity.\n",
    "- **Research Insight**: Experiment with `min_count` in Word2Vec (e.g., 5 vs. 10) to see its impact on embeddings, mimicking research experimentation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Execution Plan**\n",
    "\n",
    "**Total Time**: ~20 hours (1–2 weeks, 10–12 hours/week).  \n",
    "- **Day 1–2**: Theory (5 hours). Read Scikit-learn guide, Word2Vec tutorial, note BoW vs. TF-IDF vs. embeddings.  \n",
    "- **Day 3–5**: Practical (10 hours). Complete tasks (BoW, TF-IDF, Word2Vec, t-SNE).  \n",
    "- **Day 6–7**: Mini-Project (5 hours). Build News Analyzer, save CSV/plot, share on GitHub/X.  \n",
    "\n",
    "**Tips for Success**:\n",
    "- **Stay Motivated**: Think about using embeddings for your R&D goal (e.g., clustering X posts by topic).  \n",
    "- **Debugging**: Search errors on [Stack Overflow](https://stackoverflow.com/) or ask in Hugging Face Discord.  \n",
    "- **Portfolio**: Add `news_analysis.csv`, `tsne_plot.png`, and code to GitHub with comments explaining steps.  \n",
    "- **Foundation Check**: If you complete the mini-project in <5 hours and understand quiz answers, you’re ready for Chapter 4 (Basic NLP Tasks).  \n",
    "- **Variation**: If you used Twitter or IMDB previously, BBC News offers new challenges (e.g., category-specific terms).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Chapter is Ideal for You**\n",
    "\n",
    "- **Beginner-Friendly**: Simple explanations, step-by-step code, and free tools make representations accessible.  \n",
    "- **Practical**: Hands-on tasks and a mini-project build coding skills for research data preparation.  \n",
    "- **Research-Oriented**: Connects representations to model performance, with paper references for R&D.  \n",
    "- **Engaging**: BBC News dataset is diverse (business, sports), keeping your passion alive.  \n",
    "- **Structured**: Clear timeline, debugging tips, and checkpoints ensure progress.  \n",
    "\n",
    "This chapter strengthens your NLP foundation by mastering text representation, a core skill for R&D, while building a portfolio piece. If you want a detailed code walkthrough (e.g., t-SNE), a different dataset (e.g., Reddit), or help with specific issues (e.g., Word2Vec setup), let me know! Ready to start with the theory or setup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac328aa-e33b-46af-89bc-6785c362d9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
