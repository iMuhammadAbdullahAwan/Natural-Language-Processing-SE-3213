{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f2c77b-1785-4a06-aa67-c1c5b539a256",
   "metadata": {},
   "source": [
    "As a beginner with a passion for NLP and a long-term goal in R&D, you’re progressing through the updated course outline to build a strong foundation, having completed Chapters 1–6 (Introduction to NLP, Text Preprocessing, Text Representation, Basic NLP Tasks, Text Classification and Sentiment Analysis, Language Models and N-Grams). Below is a detailed, beginner-friendly version of **Chapter 7: Syntax and Parsing**, designed for someone with basic Python skills and knowledge from prior chapters. This chapter dives into syntactic analysis, focusing on dependency parsing and constituency parsing, which are crucial for understanding sentence structure and enabling advanced NLP tasks like question answering or machine translation. It aligns with your R&D aspirations by emphasizing hands-on practice, research connections, and portfolio-building, while keeping the content engaging and accessible.\n",
    "\n",
    "This chapter includes:\n",
    "- **Theory**: Clear explanations of syntax, dependency parsing, and constituency parsing, tailored for beginners.\n",
    "- **Practical**: Step-by-step tasks using free tools (SpaCy, NLTK) and a new dataset (news headlines) to avoid repetition.\n",
    "- **Mini-Project**: A Sentence Parser to analyze sentence structures, building coding skills and a portfolio piece.\n",
    "- **Resources**: Free, beginner-friendly materials.\n",
    "- **Debugging Tips**: Solutions to common beginner issues.\n",
    "- **Checkpoints**: Quizzes and tasks to confirm mastery.\n",
    "- **R&D Focus**: Links to research concepts (e.g., parsing evaluation) to inspire your long-term goal.\n",
    "\n",
    "The dataset (news headlines) is fresh compared to your previous work (e.g., Gutenberg, IMDB, BBC News, Wikipedia, X posts), ensuring variety and relevance to real-world NLP. The content is structured for self-paced learning, with clear steps to build confidence and prepare for advanced NLP research.\n",
    "\n",
    "**Time Estimate**: ~15 hours (spread over 1–2 weeks, 7–10 hours/week).  \n",
    "**Tools**: Free (Google Colab, SpaCy, NLTK, Pandas).  \n",
    "**Dataset**: [News Headlines Dataset](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection) (free on Kaggle).  \n",
    "**Prerequisites**: Basic Python (Chapter 1), text preprocessing (Chapter 2), text representation (Chapter 3), POS/NER (Chapter 4), text classification (Chapter 5), language models (Chapter 6); Colab or Anaconda setup with libraries (`pip install spacy nltk pandas`).  \n",
    "**Date**: June 23, 2025, 03:56 PM PKT.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter 7: Syntax and Parsing**\n",
    "\n",
    "*Goal*: Understand syntactic analysis, implement dependency and constituency parsing, and analyze sentence structures, preparing for research-grade NLP tasks.\n",
    "\n",
    "### **Theory (4 hours)**\n",
    "\n",
    "#### **What is Syntax and Parsing?**\n",
    "- **Definition**: Syntax is the study of sentence structure (how words combine to form sentences). Parsing is the process of analyzing a sentence to determine its syntactic structure.\n",
    "  - Example: For “The cat sleeps,” parsing identifies “cat” as the subject and “sleeps” as the verb.\n",
    "- **Why It Matters**: Parsing enables machines to understand sentence roles, critical for tasks like question answering, chatbots, and translation.\n",
    "- **R&D Relevance**: In research, parsing improves model accuracy in tasks like semantic role labeling or dialogue systems.\n",
    "\n",
    "#### **Key Concepts**\n",
    "1. **Dependency Parsing**:\n",
    "   - **What**: Represents sentence structure as a tree where words (nodes) are connected by directed edges (dependencies) showing relationships (e.g., subject, object).\n",
    "   - **Example**: In “The cat sleeps,” “cat” is the subject of “sleeps,” and “The” is a determiner for “cat.”\n",
    "   - **Output**: A tree with labeled edges (e.g., nsubj for subject, det for determiner).\n",
    "   - **Why**: Captures word relationships, useful for understanding meaning.\n",
    "   - **Tool**: SpaCy (accurate, beginner-friendly).\n",
    "2. **Constituency Parsing**:\n",
    "   - **What**: Represents sentence structure as a hierarchical tree of phrases (e.g., noun phrase [NP], verb phrase [VP]).\n",
    "   - **Example**: “The cat sleeps” → [S [NP The cat] [VP sleeps]], where S is sentence, NP is noun phrase, VP is verb phrase.\n",
    "   - **Output**: A nested tree of phrase labels.\n",
    "   - **Why**: Captures phrase-level structure, useful for tasks like text generation.\n",
    "   - **Tool**: NLTK (simpler, includes basic parsers).\n",
    "3. **Part-of-Speech (POS) Integration** (from Chapter 4):\n",
    "   - **What**: POS tags (e.g., NOUN, VERB) are inputs to parsers, helping identify word roles.\n",
    "   - **Example**: “cat” (NOUN) is likely a subject or object.\n",
    "   - **Why**: Improves parsing accuracy.\n",
    "4. **Evaluation**:\n",
    "   - **Dependency Parsing**: Unlabeled Attachment Score (UAS) measures % of correct dependency edges.\n",
    "   - **Constituency Parsing**: Parseval F1 score compares predicted and true phrase trees.\n",
    "   - **Human Evaluation**: Check if parse trees make sense (e.g., correct subject-verb links).\n",
    "   - **Research Insight**: UAS and Parseval are standard metrics in NLP parsing research.\n",
    "\n",
    "#### **Trade-Offs**\n",
    "- **Dependency vs. Constituency Parsing**:\n",
    "  - Dependency: Simpler, focuses on word-to-word links, better for short sentences.\n",
    "  - Constituency: More detailed, captures phrases, better for complex sentences.\n",
    "- **SpaCy vs. NLTK**:\n",
    "  - SpaCy: Pre-trained, accurate for dependency parsing, but less flexible.\n",
    "  - NLTK: Simpler for constituency parsing, but less accurate without training.\n",
    "- **Challenges**: Ambiguity (e.g., “I saw the man with a telescope” has multiple parses) and long sentences are hard to parse accurately.\n",
    "\n",
    "#### **Resources**\n",
    "- [SpaCy Dependency Parsing](https://spacy.io/usage/linguistic-features#dependency-parse): Beginner guide.\n",
    "- [NLTK Book, Chapter 8](https://www.nltk.org/book/ch08.html): Constituency parsing basics.\n",
    "- [Stanford CS224N Lecture 5](https://www.youtube.com/watch?v=rmVRLeJRklI): Free video on parsing (optional).\n",
    "- **R&D Resource**: Skim the introduction of [Dozat & Manning, 2017](https://nlp.stanford.edu/pubs/Dozat-Manning-2017.pdf) (5 minutes) for dependency parsing advancements.\n",
    "\n",
    "#### **Learning Tips**\n",
    "- Note how dependency parsing differs from constituency parsing.\n",
    "- Search X for #NLP or #Parsing to see discussions (I can analyze posts if you share links).\n",
    "- Think about using parsing for your R&D goal (e.g., analyzing X post structures for dialogue systems).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical (8 hours)**\n",
    "\n",
    "*Goal*: Implement dependency and constituency parsing on real text, building coding skills and understanding sentence structure.\n",
    "\n",
    "#### **Setup**\n",
    "- **Environment**: Google Colab (free GPU) or Anaconda (from Chapter 1).\n",
    "- **Libraries**: Install (run in Colab or terminal):\n",
    "  ```bash\n",
    "  pip install spacy nltk pandas\n",
    "  python -m spacy download en_core_web_sm\n",
    "  ```\n",
    "  ```python\n",
    "  import nltk\n",
    "  nltk.download('punkt')\n",
    "  nltk.download('averaged_perceptron_tagger')\n",
    "  ```\n",
    "- **Dataset**: [News Headlines Dataset](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection).\n",
    "  - Download: Sign up for Kaggle, download `Sarcasm_Headlines_Dataset_v2.json`.\n",
    "  - Why? Short, structured headlines are ideal for parsing practice.\n",
    "  - Columns: `headline` (text), `is_sarcastic` (label, ignored here).\n",
    "- **Load Data**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  df = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)[:200]  # Use 200 headlines\n",
    "  print(df['headline'].head())\n",
    "  ```\n",
    "\n",
    "#### **Tasks**\n",
    "1. **Dependency Parsing with SpaCy (2 hours)**:\n",
    "   - Parse headlines to extract dependency trees.\n",
    "   - Code:\n",
    "     ```python\n",
    "     import spacy\n",
    "     nlp = spacy.load(\"en_core_web_sm\")\n",
    "     headline = df['headline'].iloc[0]\n",
    "     doc = nlp(headline)\n",
    "     dependencies = [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "     print(dependencies)\n",
    "     ```\n",
    "   - Output: e.g., [(“Apple”, “nsubj”, “releases”), (“releases”, “ROOT”, “releases”), (“iPhone”, “dobj”, “releases”)].\n",
    "2. **Visualize Dependency Tree (2 hours)**:\n",
    "   - Use SpaCy’s displaCy to visualize trees.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from spacy import displacy\n",
    "     doc = nlp(headline)\n",
    "     displacy.render(doc, style=\"dep\", jupyter=True)  # jupyter=False if not in Colab\n",
    "     ```\n",
    "   - Output: Interactive tree showing word relationships (e.g., “Apple” → nsubj → “releases”).\n",
    "3. **Constituency Parsing with NLTK (2 hours)**:\n",
    "   - Parse headlines using NLTK’s basic parser.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "     headline = df['headline'].iloc[0]\n",
    "     tokens = word_tokenize(headline)\n",
    "     pos_tags = pos_tag(tokens)\n",
    "     grammar = \"NP: {<DT>?<JJ>*<NN>}\"  # Simple noun phrase grammar\n",
    "     parser = RegexpParser(grammar)\n",
    "     tree = parser.parse(pos_tags)\n",
    "     print(tree)\n",
    "     ```\n",
    "   - Output: e.g., (S (NP The/DT new/JJ iPhone/NN) releases/VBZ).\n",
    "4. **Compare Parsers (2 hours)**:\n",
    "   - Apply both parsers to 5 headlines and note differences.\n",
    "   - Code:\n",
    "     ```python\n",
    "     for headline in df['headline'][:5]:\n",
    "         print(f\"\\nHeadline: {headline}\")\n",
    "         # SpaCy dependency\n",
    "         doc = nlp(headline)\n",
    "         print(\"Dependencies:\", [(token.text, token.dep_, token.head.text) for token in doc][:5])\n",
    "         # NLTK constituency\n",
    "         tokens = word_tokenize(headline)\n",
    "         pos_tags = pos_tag(tokens)\n",
    "         tree = parser.parse(pos_tags)\n",
    "         print(\"Constituency:\", tree)\n",
    "     ```\n",
    "   - Output: Compare dependency (word-to-word) vs. constituency (phrase-based) structures.\n",
    "\n",
    "#### **Debugging Tips**\n",
    "- SpaCy model fails? Run `python -m spacy download en_core_web_sm`.\n",
    "- DisplaCy not showing? Use Colab (`jupyter=True`) or save as HTML (`displacy.render(doc, style=\"dep\", page=True)`).\n",
    "- NLTK parser fails? Simplify grammar or check POS tags.\n",
    "- Memory issues? Limit to 100 headlines (`df[:100]`).\n",
    "- JSON error? Ensure correct path (e.g., `/content/Sarcasm_Headlines_Dataset_v2.json` in Colab).\n",
    "\n",
    "#### **Resources**\n",
    "- [SpaCy Dependency Parsing](https://spacy.io/usage/linguistic-features#dependency-parse).\n",
    "- [NLTK Book, Chapter 8](https://www.nltk.org/book/ch08.html): Constituency parsing.\n",
    "- [NLTK Chunking](https://www.nltk.org/book/ch07.html#chunking): Simple parsers.\n",
    "- [Kaggle Pandas](https://www.kaggle.com/learn/pandas): JSON handling.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mini-Project: Sentence Parser (3 hours)**\n",
    "\n",
    "*Goal*: Build a parser to analyze dependency and constituency structures of news headlines, saving results and visualizations, creating a portfolio piece for R&D.\n",
    "\n",
    "- **Task**: Process 10 headlines, extract dependency and constituency parses, and visualize one dependency tree.\n",
    "- **Input**: `Sarcasm_Headlines_Dataset_v2.json` (first 10 headlines).\n",
    "- **Output**: \n",
    "  - CSV with columns: `headline`, `dependencies`, `constituency`.\n",
    "  - Dependency tree visualization (`dep_tree.html`).\n",
    "- **Steps**:\n",
    "  1. Load and preprocess headlines (remove URLs, lowercase from Chapter 2).\n",
    "  2. Extract dependency parses with SpaCy and constituency parses with NLTK.\n",
    "  3. Save results to `headline_parses.csv`.\n",
    "  4. Visualize one headline’s dependency tree.\n",
    "- **Example Output**:\n",
    "  - CSV:\n",
    "    ```csv\n",
    "    headline,dependencies,constituency\n",
    "    \"Apple releases new iPhone\",\"[('Apple', 'nsubj', 'releases'), ('releases', 'ROOT', 'releases'), ('new', 'amod', 'iPhone'), ('iPhone', 'dobj', 'releases')]\",\"(S (NP Apple/NNP) releases/VBZ (NP new/JJ iPhone/NN))\"\n",
    "    ```\n",
    "  - Visualization: Dependency tree for “Apple releases new iPhone.”\n",
    "- **Code**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  import spacy\n",
    "  import nltk\n",
    "  from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "  from spacy import displacy\n",
    "  import re\n",
    "\n",
    "  # Setup\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "  nltk.download('punkt')\n",
    "  nltk.download('averaged_perceptron_tagger')\n",
    "  grammar = \"NP: {<DT>?<JJ>*<NN.*>}\"  # Noun phrase grammar\n",
    "  parser = RegexpParser(grammar)\n",
    "\n",
    "  # Preprocess\n",
    "  def preprocess(text):\n",
    "      return re.sub(r'http\\S+|[^\\x00-\\x7F]+', '', text.lower())\n",
    "\n",
    "  # Load data\n",
    "  df = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)[:10]\n",
    "  df['cleaned_headline'] = df['headline'].apply(preprocess)\n",
    "\n",
    "  # Parse\n",
    "  data = []\n",
    "  for headline in df['cleaned_headline']:\n",
    "      # Dependency parsing\n",
    "      doc = nlp(headline)\n",
    "      dependencies = [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "      # Constituency parsing\n",
    "      tokens = word_tokenize(headline)\n",
    "      pos_tags = pos_tag(tokens)\n",
    "      tree = parser.parse(pos_tags)\n",
    "      data.append([headline, str(dependencies[:5]), str(tree)])\n",
    "\n",
    "  # Save to CSV\n",
    "  pd.DataFrame(data, columns=['headline', 'dependencies', 'constituency']).to_csv('headline_parses.csv', index=False)\n",
    "\n",
    "  # Visualize dependency tree\n",
    "  doc = nlp(df['cleaned_headline'].iloc[0])\n",
    "  displacy.render(doc, style=\"dep\", options={\"compact\": True}, page=True)\n",
    "  with open('dep_tree.html', 'w') as f:\n",
    "      f.write(displacy.render(doc, style=\"dep\", page=True))\n",
    "  ```\n",
    "- **Tools**: SpaCy, NLTK, Pandas.\n",
    "- **Variation**: Try a more complex grammar for NLTK (e.g., add VP: `{<VB.*>}`) or use `en_core_web_lg` for SpaCy.\n",
    "- **Debugging Tips**:\n",
    "  - CSV not saving? Use absolute path (e.g., `/content/headline_parses.csv`).\n",
    "  - DisplaCy fails? Run in Colab or save as HTML.\n",
    "  - Constituency tree empty? Simplify grammar or check POS tags.\n",
    "- **Resources**:\n",
    "  - [SpaCy Visualizers](https://spacy.io/usage/visualizers).\n",
    "  - [NLTK Parsing](https://www.nltk.org/book/ch08.html).\n",
    "- **R&D Tip**: Add this project to your GitHub portfolio. Document parsing choices to show research rigor.\n",
    "\n",
    "---\n",
    "\n",
    "### **Checkpoints**\n",
    "\n",
    "1. **Quiz (30 minutes)**:\n",
    "   - Questions:\n",
    "     1. What is syntactic parsing, and why is it important?\n",
    "     2. How does dependency parsing differ from constituency parsing?\n",
    "     3. What role do POS tags play in parsing?\n",
    "     4. What is UAS in dependency parsing evaluation?\n",
    "   - Answers (example):\n",
    "     1. Parsing analyzes sentence structure, enabling tasks like question answering.\n",
    "     2. Dependency parsing links words; constituency parsing groups phrases.\n",
    "     3. POS tags identify word roles (e.g., NOUN), guiding parsers.\n",
    "     4. UAS measures % of correct dependency edges.\n",
    "   - **Task**: Write answers in a notebook or share on X with #NLP.\n",
    "\n",
    "2. **Task (30 minutes)**:\n",
    "   - Check `headline_parses.csv`: Are dependencies logical (e.g., “Apple” as nsubj)?\n",
    "   - Inspect `dep_tree.html`: Does the tree show correct relationships?\n",
    "   - Save files to GitHub; share on X for feedback.\n",
    "   - **R&D Connection**: Validating parse trees is a research skill for model development.\n",
    "\n",
    "---\n",
    "\n",
    "### **R&D Focus**\n",
    "\n",
    "- **Why It Matters**: Parsing is critical for research tasks like semantic analysis and dialogue systems.\n",
    "- **Action**: Skim the introduction of [Dozat & Manning, 2017](https://nlp.stanford.edu/pubs/Dozat-Manning-2017.pdf) (5 minutes). Note how it improves dependency parsing.\n",
    "- **Community**: Share your CSV or dependency tree on X with #NLP or [Hugging Face Discord](https://huggingface.co/join-discord). Ask for feedback on tree accuracy.\n",
    "- **Research Insight**: Experiment with `en_core_web_sm` vs. `en_core_web_lg` to compare dependency accuracy, mimicking research evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Execution Plan**\n",
    "\n",
    "**Total Time**: ~15 hours (1–2 weeks, 7–10 hours/week).  \n",
    "- **Day 1–2**: Theory (4 hours). Read SpaCy guide, NLTK Chapter 8, note parsing types.  \n",
    "- **Day 3–5**: Practical (8 hours). Complete tasks (dependency, constituency, visualization).  \n",
    "- **Day 6–7**: Mini-Project (3 hours). Build Sentence Parser, save CSV/HTML, share on GitHub/X.  \n",
    "\n",
    "**Tips for Success**:\n",
    "- **Stay Motivated**: Think about using parsing for your R&D goal (e.g., analyzing X post syntax for chatbots).  \n",
    "- **Debugging**: Search errors on [Stack Overflow](https://stackoverflow.com/) or ask in Hugging Face Discord.  \n",
    "- **Portfolio**: Add `headline_parses.csv`, `dep_tree.html`, and code to GitHub with comments explaining steps.  \n",
    "- **Foundation Check**: If you complete the mini-project in <3 hours and parse trees are logical, you’re ready for Chapter 8 (Topic Modeling).  \n",
    "- **Variation**: If you prefer another dataset, try [Reuters News](https://www.kaggle.com/datasets/akrammohamed/reuters-news-dataset) for longer sentences.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Chapter is Ideal for You**\n",
    "\n",
    "- **Beginner-Friendly**: Simple explanations, step-by-step code, and free tools make parsing accessible.  \n",
    "- **Practical**: Hands-on tasks and a mini-project build coding skills for research applications.  \n",
    "- **Research-Oriented**: Connects parsing to research tasks, with paper references for R&D.  \n",
    "- **Engaging**: News headlines are concise and relatable, keeping your passion alive.  \n",
    "- **Structured**: Clear timeline, debugging tips, and checkpoints ensure progress.  \n",
    "\n",
    "This chapter strengthens your NLP foundation by mastering syntax and parsing, essential for R&D, while building a portfolio piece. If you want a detailed code walkthrough (e.g., constituency parsing), a different dataset (e.g., Reddit), or help with specific issues (e.g., displaCy), let me know! Ready to start with the theory or setup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271cca2-1861-42e4-a7aa-b46bf6162272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
