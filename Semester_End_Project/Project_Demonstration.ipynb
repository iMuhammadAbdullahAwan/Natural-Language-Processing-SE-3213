{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d57dcf6",
   "metadata": {},
   "source": [
    "# WebPage AI Chatbot Extension - Project Demonstration\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the NLP techniques and algorithms used in the WebPage AI Chatbot Chrome Extension project.\n",
    "\n",
    "**Course**: Natural Language Processing (SE-3213)  \n",
    "**Project**: Semester End Project  \n",
    "**Student**: [Your Name]  \n",
    "**University**: University of Azad Jammu & Kashmir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39072522",
   "metadata": {},
   "source": [
    "## 1. Project Architecture\n",
    "\n",
    "The Chrome extension consists of several components that work together:\n",
    "\n",
    "1. **Content Script**: Extracts and analyzes webpage content\n",
    "2. **Popup Interface**: User interaction and chat interface\n",
    "3. **Background Service**: Manages extension lifecycle and advanced processing\n",
    "4. **NLP Pipeline**: Text processing and analysis algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for demonstration\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff15676",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing Pipeline\n",
    "\n",
    "The first step in our NLP pipeline is to clean and preprocess the extracted webpage content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ad29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text content\"\"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?;:\\'\\\"-]', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenize text into sentences and words\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text.lower())\n",
    "        \n",
    "        # Filter out stop words and punctuation\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word.isalnum() and word not in self.stop_words and len(word) > 2\n",
    "        ]\n",
    "        \n",
    "        return sentences, filtered_words\n",
    "    \n",
    "    def extract_key_phrases(self, words, top_n=10):\n",
    "        \"\"\"Extract key phrases using frequency analysis\"\"\"\n",
    "        word_freq = Counter(words)\n",
    "        return word_freq.most_common(top_n)\n",
    "\n",
    "# Example usage\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Sample webpage content\n",
    "sample_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field that combines computer science, \n",
    "artificial intelligence, and linguistics to help computers understand human language. \n",
    "This technology powers many applications we use daily, including chatbots, search engines, \n",
    "and language translation services. Machine learning algorithms are essential for modern NLP, \n",
    "enabling systems to learn patterns from large datasets of text. Deep learning has revolutionized \n",
    "the field with transformer models like BERT and GPT, which can generate human-like text and \n",
    "understand context better than ever before.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the sample text\n",
    "clean_text = preprocessor.clean_text(sample_text)\n",
    "sentences, words = preprocessor.tokenize_text(clean_text)\n",
    "key_phrases = preprocessor.extract_key_phrases(words)\n",
    "\n",
    "print(f\"Original text length: {len(sample_text)} characters\")\n",
    "print(f\"Clean text length: {len(clean_text)} characters\")\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Number of unique words: {len(set(words))}\")\n",
    "print(f\"\\nTop key phrases: {key_phrases[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f41d3",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis Implementation\n",
    "\n",
    "Our extension includes sentiment analysis to understand the tone of webpage content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26569ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Define sentiment word lists\n",
    "        self.positive_words = {\n",
    "            'excellent', 'amazing', 'wonderful', 'fantastic', 'great', 'good', 'awesome',\n",
    "            'perfect', 'outstanding', 'brilliant', 'superb', 'magnificent', 'marvelous',\n",
    "            'love', 'like', 'enjoy', 'pleased', 'happy', 'delighted', 'satisfied',\n",
    "            'impressive', 'remarkable', 'exceptional', 'valuable', 'useful', 'helpful'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'terrible', 'awful', 'horrible', 'bad', 'worst', 'disgusting', 'disappointing',\n",
    "            'poor', 'fail', 'failed', 'wrong', 'error', 'problem', 'issue', 'difficult',\n",
    "            'hate', 'dislike', 'annoying', 'frustrating', 'useless', 'worthless',\n",
    "            'inadequate', 'insufficient', 'unacceptable', 'unsatisfactory', 'flawed'\n",
    "        }\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analyze sentiment of text using word-based approach\"\"\"\n",
    "        words = word_tokenize(text.lower())\n",
    "        \n",
    "        positive_score = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_score = sum(1 for word in words if word in self.negative_words)\n",
    "        \n",
    "        total_sentiment_words = positive_score + negative_score\n",
    "        \n",
    "        if total_sentiment_words == 0:\n",
    "            return 'neutral', 0.0\n",
    "        \n",
    "        positive_ratio = positive_score / total_sentiment_words\n",
    "        \n",
    "        if positive_ratio > 0.6:\n",
    "            return 'positive', positive_ratio\n",
    "        elif positive_ratio < 0.4:\n",
    "            return 'negative', 1 - positive_ratio\n",
    "        else:\n",
    "            return 'neutral', 0.5\n",
    "    \n",
    "    def get_sentiment_words(self, text):\n",
    "        \"\"\"Extract sentiment-bearing words from text\"\"\"\n",
    "        words = word_tokenize(text.lower())\n",
    "        \n",
    "        found_positive = [word for word in words if word in self.positive_words]\n",
    "        found_negative = [word for word in words if word in self.negative_words]\n",
    "        \n",
    "        return found_positive, found_negative\n",
    "\n",
    "# Test sentiment analysis\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "\n",
    "test_texts = [\n",
    "    \"This is an excellent tutorial that explains the concepts very well!\",\n",
    "    \"The interface is terrible and the features don't work properly.\",\n",
    "    \"The article provides information about machine learning algorithms.\",\n",
    "    \"I love how this amazing tool makes complex tasks so easy and wonderful!\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    sentiment, confidence = sentiment_analyzer.analyze_sentiment(text)\n",
    "    pos_words, neg_words = sentiment_analyzer.get_sentiment_words(text)\n",
    "    \n",
    "    print(f\"\\nText {i}: {text[:50]}...\")\n",
    "    print(f\"Sentiment: {sentiment} (confidence: {confidence:.2f})\")\n",
    "    if pos_words:\n",
    "        print(f\"Positive words: {pos_words}\")\n",
    "    if neg_words:\n",
    "        print(f\"Negative words: {neg_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6d047",
   "metadata": {},
   "source": [
    "## 4. Text Summarization Algorithm\n",
    "\n",
    "The extension uses extractive summarization to provide concise summaries of webpage content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizer:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def score_sentences(self, sentences, word_freq):\n",
    "        \"\"\"Score sentences based on word frequency and position\"\"\"\n",
    "        sentence_scores = {}\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            words = [word for word in words if word.isalnum() and word not in self.stop_words]\n",
    "            \n",
    "            if len(words) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate frequency score\n",
    "            freq_score = sum(word_freq.get(word, 0) for word in words) / len(words)\n",
    "            \n",
    "            # Position bonus (first and last sentences are often important)\n",
    "            position_score = 0\n",
    "            if i == 0:  # First sentence\n",
    "                position_score = 2\n",
    "            elif i == len(sentences) - 1:  # Last sentence\n",
    "                position_score = 1\n",
    "            \n",
    "            # Length penalty for very short or very long sentences\n",
    "            length_score = 0\n",
    "            word_count = len(words)\n",
    "            if 10 <= word_count <= 25:  # Ideal length\n",
    "                length_score = 1\n",
    "            elif word_count < 5:  # Too short\n",
    "                length_score = -1\n",
    "            \n",
    "            total_score = freq_score + position_score + length_score\n",
    "            sentence_scores[sentence] = total_score\n",
    "        \n",
    "        return sentence_scores\n",
    "    \n",
    "    def summarize(self, text, num_sentences=2):\n",
    "        \"\"\"Generate extractive summary\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        if len(sentences) <= num_sentences:\n",
    "            return text\n",
    "        \n",
    "        # Get word frequency\n",
    "        words = word_tokenize(text.lower())\n",
    "        words = [word for word in words if word.isalnum() and word not in self.stop_words]\n",
    "        word_freq = Counter(words)\n",
    "        \n",
    "        # Score sentences\n",
    "        sentence_scores = self.score_sentences(sentences, word_freq)\n",
    "        \n",
    "        # Select top sentences\n",
    "        top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
    "        \n",
    "        # Sort by original order\n",
    "        summary_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if any(sentence == s[0] for s in top_sentences):\n",
    "                summary_sentences.append(sentence)\n",
    "        \n",
    "        return ' '.join(summary_sentences)\n",
    "\n",
    "# Test summarization\n",
    "summarizer = TextSummarizer()\n",
    "\n",
    "long_text = \"\"\"\n",
    "Artificial Intelligence (AI) has become one of the most transformative technologies of our time. \n",
    "It encompasses various techniques including machine learning, deep learning, and neural networks. \n",
    "Machine learning algorithms can learn patterns from data without being explicitly programmed. \n",
    "Deep learning, a subset of machine learning, uses artificial neural networks with multiple layers. \n",
    "These networks can process complex data like images, text, and audio with remarkable accuracy. \n",
    "Natural Language Processing is a crucial branch of AI that focuses on language understanding. \n",
    "Computer vision enables machines to interpret and understand visual information from the world. \n",
    "AI applications are everywhere, from recommendation systems to autonomous vehicles. \n",
    "The future of AI holds even more promising developments in various fields. \n",
    "However, ethical considerations and responsible AI development remain important challenges.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer.summarize(long_text, num_sentences=3)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(long_text.strip())\n",
    "print(f\"\\nOriginal length: {len(long_text.split())} words\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary:\")\n",
    "print(summary)\n",
    "print(f\"\\nSummary length: {len(summary.split())} words\")\n",
    "print(f\"Compression ratio: {len(summary.split())/len(long_text.split()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b1746",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition\n",
    "\n",
    "The extension identifies various types of entities in webpage content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50155ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "class EntityRecognizer:\n",
    "    def __init__(self):\n",
    "        # Define regex patterns for different entity types\n",
    "        self.patterns = {\n",
    "            'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "            'url': r'https?://[^\\s]+',\n",
    "            'phone': r'\\b(?:\\+?1[-.]?)?\\(?([0-9]{3})\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})\\b',\n",
    "            'date': r'\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})\\b',\n",
    "            'time': r'\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\s?(?:AM|PM|am|pm)?\\b',\n",
    "            'currency': r'\\$\\d+(?:,\\d{3})*(?:\\.\\d{2})?|\\d+(?:,\\d{3})*(?:\\.\\d{2})?\\s?(?:USD|EUR|GBP|dollars?|euros?|pounds?)\\b',\n",
    "            'percentage': r'\\d+(?:\\.\\d+)?%',\n",
    "            'number': r'\\b\\d+(?:,\\d{3})*(?:\\.\\d+)?\\b'\n",
    "        }\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"Extract various types of entities from text\"\"\"\n",
    "        entities = {}\n",
    "        \n",
    "        for entity_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                entities[entity_type] = list(set(matches))  # Remove duplicates\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def extract_organizations(self, text):\n",
    "        \"\"\"Simple organization detection using common patterns\"\"\"\n",
    "        # Look for company-like patterns\n",
    "        org_patterns = [\n",
    "            r'\\b[A-Z][a-zA-Z]+\\s+(?:Inc|Corp|LLC|Ltd|Company|Corporation|Technologies|Systems|Solutions)\\b',\n",
    "            r'\\b(?:University|College|Institute)\\s+of\\s+[A-Z][a-zA-Z\\s]+\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]+\\s+(?:University|College|Institute)\\b'\n",
    "        ]\n",
    "        \n",
    "        organizations = []\n",
    "        for pattern in org_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            organizations.extend(matches)\n",
    "        \n",
    "        return list(set(organizations))\n",
    "    \n",
    "    def extract_locations(self, text):\n",
    "        \"\"\"Simple location detection\"\"\"\n",
    "        # Common location indicators\n",
    "        location_patterns = [\n",
    "            r'\\b[A-Z][a-zA-Z]+,\\s+[A-Z]{2}\\b',  # City, State\n",
    "            r'\\b(?:in|at|from|to)\\s+([A-Z][a-zA-Z\\s]+?)(?:\\s+(?:city|state|country|region))\\b'\n",
    "        ]\n",
    "        \n",
    "        locations = []\n",
    "        for pattern in location_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            locations.extend(matches)\n",
    "        \n",
    "        return list(set(locations))\n",
    "\n",
    "# Test entity recognition\n",
    "entity_recognizer = EntityRecognizer()\n",
    "\n",
    "test_text = \"\"\"\n",
    "Contact us at support@example.com or visit our website https://www.example.com for more information. \n",
    "You can also call us at (555) 123-4567 during business hours from 9:00 AM to 5:00 PM. \n",
    "Our office is located in New York, NY and we've been serving customers since 01/15/2020. \n",
    "The project budget is $50,000 and we expect 95% completion by the deadline. \n",
    "Microsoft Corporation and Google Inc. are major players in the technology industry. \n",
    "The University of California offers excellent computer science programs.\n",
    "\"\"\"\n",
    "\n",
    "# Extract entities\n",
    "entities = entity_recognizer.extract_entities(test_text)\n",
    "organizations = entity_recognizer.extract_organizations(test_text)\n",
    "locations = entity_recognizer.extract_locations(test_text)\n",
    "\n",
    "print(\"Named Entity Recognition Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for entity_type, entity_list in entities.items():\n",
    "    if entity_list:\n",
    "        print(f\"\\n{entity_type.upper()}:\")\n",
    "        for entity in entity_list:\n",
    "            print(f\"  - {entity}\")\n",
    "\n",
    "if organizations:\n",
    "    print(f\"\\nORGANIZATIONS:\")\n",
    "    for org in organizations:\n",
    "        print(f\"  - {org}\")\n",
    "\n",
    "if locations:\n",
    "    print(f\"\\nLOCATIONS:\")\n",
    "    for loc in locations:\n",
    "        print(f\"  - {loc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2952a",
   "metadata": {},
   "source": [
    "## 6. Readability Analysis\n",
    "\n",
    "The extension calculates readability scores to help users understand content complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadabilityAnalyzer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def count_syllables(self, word):\n",
    "        \"\"\"Count syllables in a word using a heuristic approach\"\"\"\n",
    "        word = word.lower()\n",
    "        if len(word) <= 3:\n",
    "            return 1\n",
    "        \n",
    "        # Remove common endings that don't add syllables\n",
    "        word = re.sub(r'(?:[^laeiouy]es|ed|[^laeiouy]e)$', '', word)\n",
    "        word = re.sub(r'^y', '', word)\n",
    "        \n",
    "        # Count vowel groups\n",
    "        matches = re.findall(r'[aeiouy]{1,2}', word)\n",
    "        syllable_count = len(matches) if matches else 1\n",
    "        \n",
    "        return max(1, syllable_count)\n",
    "    \n",
    "    def flesch_reading_ease(self, text):\n",
    "        \"\"\"Calculate Flesch Reading Ease score\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "        \n",
    "        if len(sentences) == 0 or len(words) == 0:\n",
    "            return 0, 'unknown'\n",
    "        \n",
    "        # Count syllables\n",
    "        total_syllables = sum(self.count_syllables(word) for word in words)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        avg_syllables_per_word = total_syllables / len(words)\n",
    "        \n",
    "        # Flesch Reading Ease formula\n",
    "        score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)\n",
    "        \n",
    "        # Interpret score\n",
    "        if score >= 90:\n",
    "            level = 'very easy'\n",
    "        elif score >= 80:\n",
    "            level = 'easy'\n",
    "        elif score >= 70:\n",
    "            level = 'fairly easy'\n",
    "        elif score >= 60:\n",
    "            level = 'standard'\n",
    "        elif score >= 50:\n",
    "            level = 'fairly difficult'\n",
    "        elif score >= 30:\n",
    "            level = 'difficult'\n",
    "        else:\n",
    "            level = 'very difficult'\n",
    "        \n",
    "        return score, level\n",
    "    \n",
    "    def analyze_complexity(self, text):\n",
    "        \"\"\"Comprehensive text complexity analysis\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "        \n",
    "        if len(sentences) == 0 or len(words) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Basic metrics\n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "        \n",
    "        # Vocabulary diversity (unique words / total words)\n",
    "        unique_words = len(set(word.lower() for word in words))\n",
    "        lexical_diversity = unique_words / len(words)\n",
    "        \n",
    "        # Complex word count (words with 3+ syllables)\n",
    "        complex_words = sum(1 for word in words if self.count_syllables(word) >= 3)\n",
    "        complex_word_ratio = complex_words / len(words)\n",
    "        \n",
    "        # Flesch Reading Ease\n",
    "        flesch_score, flesch_level = self.flesch_reading_ease(text)\n",
    "        \n",
    "        return {\n",
    "            'total_sentences': len(sentences),\n",
    "            'total_words': len(words),\n",
    "            'unique_words': unique_words,\n",
    "            'avg_sentence_length': round(avg_sentence_length, 2),\n",
    "            'avg_word_length': round(avg_word_length, 2),\n",
    "            'lexical_diversity': round(lexical_diversity, 3),\n",
    "            'complex_words': complex_words,\n",
    "            'complex_word_ratio': round(complex_word_ratio, 3),\n",
    "            'flesch_score': round(flesch_score, 2),\n",
    "            'flesch_level': flesch_level\n",
    "        }\n",
    "\n",
    "# Test readability analysis\n",
    "readability_analyzer = ReadabilityAnalyzer()\n",
    "\n",
    "# Test texts with different complexity levels\n",
    "test_texts = {\n",
    "    'Simple': \"The cat sat on the mat. It was a nice day. The sun was bright.\",\n",
    "    'Medium': \"Natural language processing enables computers to understand human language through various algorithms and techniques.\",\n",
    "    'Complex': \"The implementation of sophisticated machine learning architectures necessitates comprehensive understanding of mathematical optimization techniques and computational linguistics principles.\"\n",
    "}\n",
    "\n",
    "print(\"Readability Analysis Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for difficulty, text in test_texts.items():\n",
    "    analysis = readability_analyzer.analyze_complexity(text)\n",
    "    \n",
    "    print(f\"\\n{difficulty.upper()} TEXT:\")\n",
    "    print(f\"Text: {text[:60]}...\")\n",
    "    print(f\"Words: {analysis['total_words']}, Sentences: {analysis['total_sentences']}\")\n",
    "    print(f\"Avg sentence length: {analysis['avg_sentence_length']} words\")\n",
    "    print(f\"Avg word length: {analysis['avg_word_length']} characters\")\n",
    "    print(f\"Lexical diversity: {analysis['lexical_diversity']}\")\n",
    "    print(f\"Complex words: {analysis['complex_word_ratio']:.1%}\")\n",
    "    print(f\"Flesch score: {analysis['flesch_score']} ({analysis['flesch_level']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f257c1",
   "metadata": {},
   "source": [
    "## 7. Complete NLP Pipeline Integration\n",
    "\n",
    "Let's combine all the components into a single pipeline that mimics the extension's functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b72353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebPageAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.sentiment_analyzer = SentimentAnalyzer()\n",
    "        self.summarizer = TextSummarizer()\n",
    "        self.entity_recognizer = EntityRecognizer()\n",
    "        self.readability_analyzer = ReadabilityAnalyzer()\n",
    "    \n",
    "    def analyze_webpage_content(self, webpage_content):\n",
    "        \"\"\"Complete analysis of webpage content\"\"\"\n",
    "        # Preprocess text\n",
    "        clean_text = self.preprocessor.clean_text(webpage_content)\n",
    "        sentences, words = self.preprocessor.tokenize_text(clean_text)\n",
    "        key_phrases = self.preprocessor.extract_key_phrases(words)\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        sentiment, confidence = self.sentiment_analyzer.analyze_sentiment(clean_text)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = self.summarizer.summarize(clean_text, num_sentences=2)\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = self.entity_recognizer.extract_entities(clean_text)\n",
    "        \n",
    "        # Analyze readability\n",
    "        readability = self.readability_analyzer.analyze_complexity(clean_text)\n",
    "        \n",
    "        return {\n",
    "            'text_stats': {\n",
    "                'total_words': len(words),\n",
    "                'total_sentences': len(sentences),\n",
    "                'unique_words': len(set(words))\n",
    "            },\n",
    "            'key_phrases': key_phrases[:10],\n",
    "            'sentiment': {\n",
    "                'polarity': sentiment,\n",
    "                'confidence': confidence\n",
    "            },\n",
    "            'summary': summary,\n",
    "            'entities': entities,\n",
    "            'readability': readability\n",
    "        }\n",
    "    \n",
    "    def generate_response(self, query, analysis):\n",
    "        \"\"\"Generate intelligent response based on analysis\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if 'summary' in query_lower or 'about' in query_lower:\n",
    "            return f\"This content is about: {analysis['summary']}\"\n",
    "        \n",
    "        elif 'sentiment' in query_lower or 'tone' in query_lower:\n",
    "            sentiment = analysis['sentiment']\n",
    "            return f\"The overall tone is {sentiment['polarity']} with {sentiment['confidence']:.1%} confidence.\"\n",
    "        \n",
    "        elif 'topic' in query_lower or 'theme' in query_lower:\n",
    "            topics = [phrase[0] for phrase in analysis['key_phrases'][:5]]\n",
    "            return f\"Main topics include: {', '.join(topics)}\"\n",
    "        \n",
    "        elif 'difficult' in query_lower or 'readability' in query_lower:\n",
    "            level = analysis['readability']['flesch_level']\n",
    "            return f\"The content is {level} to read (Flesch score: {analysis['readability']['flesch_score']}).\"\n",
    "        \n",
    "        elif 'entities' in query_lower or 'contact' in query_lower:\n",
    "            entities = analysis['entities']\n",
    "            response_parts = []\n",
    "            if 'email' in entities:\n",
    "                response_parts.append(f\"Emails: {', '.join(entities['email'])}\")\n",
    "            if 'phone' in entities:\n",
    "                response_parts.append(f\"Phone numbers: {', '.join(entities['phone'])}\")\n",
    "            if 'url' in entities:\n",
    "                response_parts.append(f\"URLs: {', '.join(entities['url'][:3])}\")\n",
    "            \n",
    "            return '; '.join(response_parts) if response_parts else \"No contact entities found.\"\n",
    "        \n",
    "        else:\n",
    "            return f\"I can help with questions about this content. Try asking about the summary, topics, sentiment, readability, or entities.\"\n",
    "\n",
    "# Demonstrate the complete pipeline\n",
    "analyzer = WebPageAnalyzer()\n",
    "\n",
    "sample_webpage = \"\"\"\n",
    "Welcome to TechCorp Solutions - Your Premier Technology Partner\n",
    "\n",
    "At TechCorp Solutions, we provide exceptional software development services that transform businesses. \n",
    "Our team of expert developers creates innovative solutions using cutting-edge technologies like artificial intelligence, \n",
    "machine learning, and cloud computing. We have successfully delivered over 500 projects with 98% client satisfaction.\n",
    "\n",
    "Contact us today at info@techcorp.com or call (555) 123-4567 to discuss your project requirements. \n",
    "Visit our website at https://www.techcorp.com for more information about our services.\n",
    "\n",
    "Our comprehensive services include:\n",
    "- Custom software development\n",
    "- Mobile application development\n",
    "- Web development and design\n",
    "- Cloud migration services\n",
    "- AI and machine learning solutions\n",
    "\n",
    "Founded in 2015, TechCorp Solutions has been at the forefront of technological innovation. \n",
    "We believe in delivering outstanding results that exceed client expectations. \n",
    "Our agile development methodology ensures rapid delivery without compromising quality.\n",
    "\"\"\"\n",
    "\n",
    "# Analyze the sample webpage\n",
    "analysis = analyzer.analyze_webpage_content(sample_webpage)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE WEBPAGE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä TEXT STATISTICS:\")\n",
    "stats = analysis['text_stats']\n",
    "print(f\"   Words: {stats['total_words']}, Sentences: {stats['total_sentences']}, Unique words: {stats['unique_words']}\")\n",
    "\n",
    "print(f\"\\nüîë KEY PHRASES:\")\n",
    "for phrase, freq in analysis['key_phrases'][:5]:\n",
    "    print(f\"   {phrase} ({freq} times)\")\n",
    "\n",
    "print(f\"\\nüòä SENTIMENT ANALYSIS:\")\n",
    "sentiment = analysis['sentiment']\n",
    "print(f\"   Polarity: {sentiment['polarity']} (confidence: {sentiment['confidence']:.1%})\")\n",
    "\n",
    "print(f\"\\nüìù SUMMARY:\")\n",
    "print(f\"   {analysis['summary']}\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è ENTITIES:\")\n",
    "for entity_type, entity_list in analysis['entities'].items():\n",
    "    if entity_list:\n",
    "        print(f\"   {entity_type}: {', '.join(entity_list[:3])}\")\n",
    "\n",
    "print(f\"\\nüìñ READABILITY:\")\n",
    "readability = analysis['readability']\n",
    "print(f\"   Level: {readability['flesch_level']} (score: {readability['flesch_score']})\")\n",
    "print(f\"   Avg sentence length: {readability['avg_sentence_length']} words\")\n",
    "print(f\"   Complex words: {readability['complex_word_ratio']:.1%}\")\n",
    "\n",
    "# Test the response generation\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"CHATBOT RESPONSE EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is this page about?\",\n",
    "    \"What's the sentiment of this content?\",\n",
    "    \"What are the main topics?\",\n",
    "    \"How difficult is this to read?\",\n",
    "    \"What contact information is available?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    response = analyzer.generate_response(query, analysis)\n",
    "    print(f\"\\nQ: {query}\")\n",
    "    print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc4dd4",
   "metadata": {},
   "source": [
    "## 8. Visualization of Analysis Results\n",
    "\n",
    "Let's create some visualizations to better understand the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3cbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Create visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('WebPage AI Chatbot - NLP Analysis Visualization', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Key Phrases Bar Chart\n",
    "phrases = [phrase[0] for phrase in analysis['key_phrases'][:8]]\n",
    "frequencies = [phrase[1] for phrase in analysis['key_phrases'][:8]]\n",
    "\n",
    "bars = ax1.barh(phrases, frequencies, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "ax1.set_xlabel('Frequency')\n",
    "ax1.set_title('Top Key Phrases', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.1, bar.get_y() + bar.get_height()/2, f'{int(width)}', \n",
    "             ha='left', va='center', fontweight='bold')\n",
    "\n",
    "# 2. Readability Metrics\n",
    "readability_metrics = {\n",
    "    'Avg Sentence\\nLength': analysis['readability']['avg_sentence_length'],\n",
    "    'Avg Word\\nLength': analysis['readability']['avg_word_length'],\n",
    "    'Lexical\\nDiversity': analysis['readability']['lexical_diversity'] * 100,  # Convert to percentage\n",
    "    'Complex Words\\n(%)': analysis['readability']['complex_word_ratio'] * 100\n",
    "}\n",
    "\n",
    "metrics = list(readability_metrics.keys())\n",
    "values = list(readability_metrics.values())\n",
    "colors = ['lightcoral', 'lightsalmon', 'lightgreen', 'lightblue']\n",
    "\n",
    "bars = ax2.bar(metrics, values, color=colors, edgecolor='black', alpha=0.7)\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Readability Metrics', fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, height + max(values)*0.01, f'{value:.1f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Entity Distribution Pie Chart\n",
    "entity_counts = {}\n",
    "for entity_type, entity_list in analysis['entities'].items():\n",
    "    if entity_list:\n",
    "        entity_counts[entity_type.title()] = len(entity_list)\n",
    "\n",
    "if entity_counts:\n",
    "    wedges, texts, autotexts = ax3.pie(entity_counts.values(), labels=entity_counts.keys(), \n",
    "                                       autopct='%1.0f', startangle=90, \n",
    "                                       colors=['gold', 'lightcoral', 'lightskyblue', 'lightgreen'])\n",
    "    ax3.set_title('Entity Distribution', fontweight='bold')\n",
    "    \n",
    "    # Enhance the pie chart appearance\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No entities found', ha='center', va='center', \n",
    "             transform=ax3.transAxes, fontsize=12)\n",
    "    ax3.set_title('Entity Distribution', fontweight='bold')\n",
    "\n",
    "# 4. Sentiment and Text Statistics\n",
    "stats_data = {\n",
    "    'Total Words': analysis['text_stats']['total_words'],\n",
    "    'Total Sentences': analysis['text_stats']['total_sentences'],\n",
    "    'Unique Words': analysis['text_stats']['unique_words'],\n",
    "    'Flesch Score': analysis['readability']['flesch_score']\n",
    "}\n",
    "\n",
    "# Create a more informative display\n",
    "ax4.axis('off')\n",
    "ax4.set_title('Text Statistics & Sentiment', fontweight='bold', pad=20)\n",
    "\n",
    "# Display statistics\n",
    "y_positions = [0.8, 0.65, 0.5, 0.35, 0.2]\n",
    "labels = list(stats_data.keys()) + ['Sentiment']\n",
    "values = list(stats_data.values()) + [f\"{analysis['sentiment']['polarity']} ({analysis['sentiment']['confidence']:.1%})\"]\n",
    "\n",
    "for i, (label, value) in enumerate(zip(labels, values)):\n",
    "    ax4.text(0.1, y_positions[i], f'{label}:', fontweight='bold', fontsize=12, \n",
    "             transform=ax4.transAxes)\n",
    "    ax4.text(0.6, y_positions[i], str(value), fontsize=12, \n",
    "             transform=ax4.transAxes)\n",
    "\n",
    "# Add a background box\n",
    "from matplotlib.patches import Rectangle\n",
    "rect = Rectangle((0.05, 0.1), 0.9, 0.8, linewidth=2, edgecolor='gray', \n",
    "                facecolor='lightgray', alpha=0.1, transform=ax4.transAxes)\n",
    "ax4.add_patch(rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a word cloud\n",
    "if analysis['key_phrases']:\n",
    "    # Prepare text for word cloud\n",
    "    word_freq_dict = dict(analysis['key_phrases'])\n",
    "    \n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                         colormap='viridis', max_words=50).generate_from_frequencies(word_freq_dict)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Key Terms Word Cloud', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations generated successfully!\")\n",
    "print(\"\\nThe Chrome extension uses these same NLP techniques to analyze webpage content in real-time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edbd258",
   "metadata": {},
   "source": [
    "## 9. Project Summary and Conclusions\n",
    "\n",
    "### Key Features Implemented:\n",
    "\n",
    "1. **Text Preprocessing**: Clean and normalize webpage content\n",
    "2. **Sentiment Analysis**: Determine the emotional tone of content\n",
    "3. **Text Summarization**: Generate concise summaries using extractive methods\n",
    "4. **Named Entity Recognition**: Identify emails, URLs, phone numbers, etc.\n",
    "5. **Readability Analysis**: Calculate content complexity using Flesch scores\n",
    "6. **Keyword Extraction**: Identify important terms and phrases\n",
    "7. **Interactive Chat Interface**: Natural language query processing\n",
    "\n",
    "### Technical Achievements:\n",
    "\n",
    "- **Chrome Extension Architecture**: Complete browser extension with popup, content scripts, and background service worker\n",
    "- **Real-time Processing**: Instant analysis of webpage content as users navigate\n",
    "- **Context-Aware Responses**: Tailored answers based on specific webpage content\n",
    "- **Multiple Interfaces**: Both popup and embedded widget options\n",
    "- **Persistent Storage**: Chat history and user preferences\n",
    "\n",
    "### NLP Techniques Used:\n",
    "\n",
    "- **Statistical Text Analysis**: Frequency-based keyword extraction\n",
    "- **Rule-based Processing**: Pattern matching for entity recognition\n",
    "- **Heuristic Algorithms**: Sentence scoring for summarization\n",
    "- **Lexical Analysis**: Syllable counting for readability metrics\n",
    "- **Template-based Generation**: Structured response creation\n",
    "\n",
    "### Educational Value:\n",
    "\n",
    "This project demonstrates practical application of NLP concepts in a real-world scenario, combining theoretical knowledge with hands-on implementation. It showcases how various NLP techniques can be integrated to create a useful tool for content analysis and user interaction.\n",
    "\n",
    "### Future Enhancements:\n",
    "\n",
    "- Integration with advanced AI APIs (OpenAI, Google Gemini)\n",
    "- Deep learning models for better understanding\n",
    "- Multi-language support\n",
    "- Voice interaction capabilities\n",
    "- Advanced topic modeling with LDA\n",
    "\n",
    "---\n",
    "\n",
    "**This completes the demonstration of the WebPage AI Chatbot Chrome Extension project for the Natural Language Processing course.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
