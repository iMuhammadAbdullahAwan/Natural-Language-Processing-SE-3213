{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644ac2c4",
   "metadata": {},
   "source": [
    "# Machine Learning Approach Implementation\n",
    "## Roman Urdu to Urdu Script Conversion Project\n",
    "\n",
    "This notebook covers Step 4 & 5 of our methodology:\n",
    "- Machine Learning Model Implementation\n",
    "- Model Training and Evaluation\n",
    "- Comparison with Dictionary Approach\n",
    "\n",
    "### Objectives:\n",
    "1. Implement and train ML models (word-based and character-based)\n",
    "2. Evaluate ML model performance\n",
    "3. Compare different ML approaches\n",
    "4. Analyze feature importance and model behavior\n",
    "5. Compare with dictionary-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3380f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('../')\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from models.ml_model import MLModel\n",
    "from models.dictionary_model import DictionaryModel\n",
    "from utils.data_loader import DataLoader\n",
    "from utils.preprocessing import RomanUrduPreprocessor\n",
    "from evaluation.metrics import (\n",
    "    calculate_bleu_score, calculate_rouge_l, calculate_word_accuracy,\n",
    "    calculate_sentence_accuracy, calculate_character_accuracy, calculate_edit_distance\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a98323",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "data_loader = DataLoader(\"../data\")\n",
    "preprocessor = RomanUrduPreprocessor()\n",
    "\n",
    "# Load data\n",
    "sample_data = data_loader.load_sample_data()\n",
    "test_data = data_loader.load_test_data()\n",
    "dictionary = data_loader.load_dictionary()\n",
    "\n",
    "print(f\"Sample data: {len(sample_data)} sentences\")\n",
    "print(f\"Test data: {len(test_data)} sentences\")\n",
    "print(f\"Dictionary: {len(dictionary)} entries\")\n",
    "\n",
    "# Prepare training data from sample_data\n",
    "train_roman = [item['roman'] for item in sample_data]\n",
    "train_urdu = [item['urdu'] for item in sample_data]\n",
    "\n",
    "# Prepare test data\n",
    "test_roman = [item['roman'] for item in test_data]\n",
    "test_urdu = [item['urdu'] for item in test_data]\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_roman)}\")\n",
    "print(f\"Test samples: {len(test_roman)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141798a",
   "metadata": {},
   "source": [
    "## 2. Word-Based ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train word-based model\n",
    "print(\"Training Word-Based ML Model...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "word_ml_model = MLModel(approach=\"word\")\n",
    "\n",
    "# Train the model\n",
    "word_ml_model.train(train_roman, train_urdu)\n",
    "print(\"Word-based model training completed!\")\n",
    "\n",
    "# Get model info\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"Vocabulary size: {len(word_ml_model.word_pairs)}\")\n",
    "print(f\"Feature dimension: {word_ml_model.vectorizer.get_feature_names_out().shape[0] if hasattr(word_ml_model, 'vectorizer') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test word-based model on individual words\n",
    "test_words = [\"main\", \"aap\", \"kaise\", \"hain\", \"ghar\", \"ja\", \"raha\", \"hun\"]\n",
    "\n",
    "print(\"Word-Based Model - Individual Word Tests:\")\n",
    "print(\"=\" * 45)\n",
    "for word in test_words:\n",
    "    converted = word_ml_model.convert_word(word)\n",
    "    print(f\"{word:10} -> {converted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test word-based model on sentences\n",
    "test_sentences = [\n",
    "    \"main acha hun\",\n",
    "    \"aap kaise hain\",\n",
    "    \"wo ghar ja raha hai\"\n",
    "]\n",
    "\n",
    "print(\"Word-Based Model - Sentence Tests:\")\n",
    "print(\"=\" * 40)\n",
    "for sentence in test_sentences:\n",
    "    converted = word_ml_model.convert_text(sentence)\n",
    "    print(f\"Roman: {sentence}\")\n",
    "    print(f\"Urdu:  {converted}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725079cf",
   "metadata": {},
   "source": [
    "## 3. Character-Based ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train character-based model\n",
    "print(\"Training Character-Based ML Model...\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "char_ml_model = MLModel(approach=\"character\")\n",
    "\n",
    "# Train the model\n",
    "char_ml_model.train(train_roman, train_urdu)\n",
    "print(\"Character-based model training completed!\")\n",
    "\n",
    "# Get model info\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"Character mappings: {len(char_ml_model.char_pairs)}\")\n",
    "print(f\"Feature dimension: {char_ml_model.vectorizer.get_feature_names_out().shape[0] if hasattr(char_ml_model, 'vectorizer') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a61ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test character-based model\n",
    "print(\"Character-Based Model - Word Tests:\")\n",
    "print(\"=\" * 40)\n",
    "for word in test_words:\n",
    "    converted = char_ml_model.convert_word(word)\n",
    "    print(f\"{word:10} -> {converted}\")\n",
    "\n",
    "print(\"\\nCharacter-Based Model - Sentence Tests:\")\n",
    "print(\"=\" * 45)\n",
    "for sentence in test_sentences:\n",
    "    converted = char_ml_model.convert_text(sentence)\n",
    "    print(f\"Roman: {sentence}\")\n",
    "    print(f\"Urdu:  {converted}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed4f9f",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on test set\n",
    "def evaluate_model(model, model_name, test_roman, test_urdu):\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    predictions = []\n",
    "    references = test_urdu\n",
    "    \n",
    "    # Generate predictions\n",
    "    for roman_text in test_roman:\n",
    "        predicted = model.convert_text(roman_text)\n",
    "        predictions.append(predicted)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu_scores = [calculate_bleu_score(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    rouge_scores = [calculate_rouge_l(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    word_accuracies = [calculate_word_accuracy(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    char_accuracies = [calculate_character_accuracy(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    edit_distances = [calculate_edit_distance(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    \n",
    "    sentence_accuracy = calculate_sentence_accuracy(predictions, references)\n",
    "    \n",
    "    metrics = {\n",
    "        'BLEU': np.mean(bleu_scores),\n",
    "        'ROUGE-L': np.mean(rouge_scores),\n",
    "        'Word_Accuracy': np.mean(word_accuracies),\n",
    "        'Sentence_Accuracy': sentence_accuracy,\n",
    "        'Character_Accuracy': np.mean(char_accuracies),\n",
    "        'Avg_Edit_Distance': np.mean(edit_distances)\n",
    "    }\n",
    "    \n",
    "    return metrics, predictions, bleu_scores, word_accuracies\n",
    "\n",
    "# Evaluate word-based model\n",
    "word_metrics, word_predictions, word_bleu_scores, word_word_accuracies = evaluate_model(\n",
    "    word_ml_model, \"Word-Based ML Model\", test_roman, test_urdu\n",
    ")\n",
    "\n",
    "# Evaluate character-based model\n",
    "char_metrics, char_predictions, char_bleu_scores, char_word_accuracies = evaluate_model(\n",
    "    char_ml_model, \"Character-Based ML Model\", test_roman, test_urdu\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b58519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"Word-Based ML Model Performance:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in word_metrics.items():\n",
    "    if 'Distance' in metric:\n",
    "        print(f\"{metric:20}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{metric:20}: {value:.3f} ({value*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nCharacter-Based ML Model Performance:\")\n",
    "print(\"=\" * 42)\n",
    "for metric, value in char_metrics.items():\n",
    "    if 'Distance' in metric:\n",
    "        print(f\"{metric:20}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{metric:20}: {value:.3f} ({value*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261901d",
   "metadata": {},
   "source": [
    "## 5. Model Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary model results for comparison\n",
    "dict_model = DictionaryModel(\"../data/roman_urdu_dictionary.json\")\n",
    "dict_metrics, dict_predictions, dict_bleu_scores, dict_word_accuracies = evaluate_model(\n",
    "    dict_model, \"Dictionary Model\", test_roman, test_urdu\n",
    ")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': ['Dictionary', 'Word-based ML', 'Character-based ML'],\n",
    "    'BLEU': [dict_metrics['BLEU'], word_metrics['BLEU'], char_metrics['BLEU']],\n",
    "    'ROUGE-L': [dict_metrics['ROUGE-L'], word_metrics['ROUGE-L'], char_metrics['ROUGE-L']],\n",
    "    'Word_Accuracy': [dict_metrics['Word_Accuracy'], word_metrics['Word_Accuracy'], char_metrics['Word_Accuracy']],\n",
    "    'Sentence_Accuracy': [dict_metrics['Sentence_Accuracy'], word_metrics['Sentence_Accuracy'], char_metrics['Sentence_Accuracy']],\n",
    "    'Character_Accuracy': [dict_metrics['Character_Accuracy'], word_metrics['Character_Accuracy'], char_metrics['Character_Accuracy']]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe037f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = ['BLEU', 'ROUGE-L', 'Word_Accuracy', 'Sentence_Accuracy', 'Character_Accuracy']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = comparison_df[metric].values\n",
    "    bars = axes[i].bar(comparison_df['Model'], values, color=colors, alpha=0.8)\n",
    "    axes[i].set_title(f'{metric} Comparison', fontsize=12)\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                     f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove the last subplot\n",
    "axes[5].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4c6e0",
   "metadata": {},
   "source": [
    "## 6. Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa39ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# BLEU score distributions\n",
    "axes[0, 0].hist(dict_bleu_scores, bins=10, alpha=0.7, label='Dictionary', color='skyblue')\n",
    "axes[0, 0].hist(word_bleu_scores, bins=10, alpha=0.7, label='Word ML', color='lightcoral')\n",
    "axes[0, 0].hist(char_bleu_scores, bins=10, alpha=0.7, label='Char ML', color='lightgreen')\n",
    "axes[0, 0].set_title('BLEU Score Distributions')\n",
    "axes[0, 0].set_xlabel('BLEU Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Word accuracy distributions\n",
    "axes[0, 1].hist(dict_word_accuracies, bins=10, alpha=0.7, label='Dictionary', color='skyblue')\n",
    "axes[0, 1].hist(word_word_accuracies, bins=10, alpha=0.7, label='Word ML', color='lightcoral')\n",
    "axes[0, 1].hist(char_word_accuracies, bins=10, alpha=0.7, label='Char ML', color='lightgreen')\n",
    "axes[0, 1].set_title('Word Accuracy Distributions')\n",
    "axes[0, 1].set_xlabel('Word Accuracy')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plots for BLEU scores\n",
    "bleu_data = [dict_bleu_scores, word_bleu_scores, char_bleu_scores]\n",
    "axes[0, 2].boxplot(bleu_data, labels=['Dictionary', 'Word ML', 'Char ML'])\n",
    "axes[0, 2].set_title('BLEU Score Box Plots')\n",
    "axes[0, 2].set_ylabel('BLEU Score')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plots for word accuracy\n",
    "acc_data = [dict_word_accuracies, word_word_accuracies, char_word_accuracies]\n",
    "axes[1, 0].boxplot(acc_data, labels=['Dictionary', 'Word ML', 'Char ML'])\n",
    "axes[1, 0].set_title('Word Accuracy Box Plots')\n",
    "axes[1, 0].set_ylabel('Word Accuracy')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation between BLEU and Word Accuracy\n",
    "axes[1, 1].scatter(dict_bleu_scores, dict_word_accuracies, alpha=0.7, label='Dictionary', color='skyblue')\n",
    "axes[1, 1].scatter(word_bleu_scores, word_word_accuracies, alpha=0.7, label='Word ML', color='lightcoral')\n",
    "axes[1, 1].scatter(char_bleu_scores, char_word_accuracies, alpha=0.7, label='Char ML', color='lightgreen')\n",
    "axes[1, 1].set_title('BLEU vs Word Accuracy')\n",
    "axes[1, 1].set_xlabel('BLEU Score')\n",
    "axes[1, 1].set_ylabel('Word Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance by sentence length\n",
    "sentence_lengths = [len(sent.split()) for sent in test_roman]\n",
    "axes[1, 2].scatter(sentence_lengths, dict_bleu_scores, alpha=0.7, label='Dictionary', color='skyblue')\n",
    "axes[1, 2].scatter(sentence_lengths, word_bleu_scores, alpha=0.7, label='Word ML', color='lightcoral')\n",
    "axes[1, 2].scatter(sentence_lengths, char_bleu_scores, alpha=0.7, label='Char ML', color='lightgreen')\n",
    "axes[1, 2].set_title('Performance vs Sentence Length')\n",
    "axes[1, 2].set_xlabel('Sentence Length (words)')\n",
    "axes[1, 2].set_ylabel('BLEU Score')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ed6fe",
   "metadata": {},
   "source": [
    "## 7. Feature Analysis (ML Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eefe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word-based model features\n",
    "if hasattr(word_ml_model, 'vectorizer') and hasattr(word_ml_model, 'model'):\n",
    "    try:\n",
    "        feature_names = word_ml_model.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Get feature importance for Random Forest\n",
    "        if hasattr(word_ml_model.model, 'feature_importances_'):\n",
    "            importances = word_ml_model.model.feature_importances_\n",
    "            \n",
    "            # Get top features\n",
    "            top_indices = np.argsort(importances)[-20:][::-1]\n",
    "            top_features = [feature_names[i] for i in top_indices]\n",
    "            top_importances = [importances[i] for i in top_indices]\n",
    "            \n",
    "            print(\"Top 20 Features (Word-based Model):\")\n",
    "            print(\"=\" * 40)\n",
    "            for feature, importance in zip(top_features, top_importances):\n",
    "                print(f\"{feature:20}: {importance:.4f}\")\n",
    "            \n",
    "            # Visualize feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.barh(range(len(top_features)), top_importances)\n",
    "            plt.yticks(range(len(top_features)), top_features)\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title('Top 20 Feature Importances (Word-based Model)')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Feature analysis not available: {e}\")\n",
    "else:\n",
    "    print(\"Feature analysis not available for this model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze character-based model patterns\n",
    "if hasattr(char_ml_model, 'char_pairs'):\n",
    "    print(\"Character Mapping Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Show most common character mappings\n",
    "    char_freq = Counter()\n",
    "    for roman_char, urdu_chars in char_ml_model.char_pairs.items():\n",
    "        for urdu_char in urdu_chars:\n",
    "            char_freq[(roman_char, urdu_char)] += 1\n",
    "    \n",
    "    print(\"Top 20 Character Mappings:\")\n",
    "    for (roman, urdu), freq in char_freq.most_common(20):\n",
    "        print(f\"'{roman}' -> '{urdu}': {freq} times\")\n",
    "    \n",
    "    # Analyze character coverage\n",
    "    roman_chars = set(char_ml_model.char_pairs.keys())\n",
    "    all_urdu_chars = set()\n",
    "    for urdu_chars in char_ml_model.char_pairs.values():\n",
    "        all_urdu_chars.update(urdu_chars)\n",
    "    \n",
    "    print(f\"\\nCharacter Coverage:\")\n",
    "    print(f\"Roman characters: {len(roman_chars)}\")\n",
    "    print(f\"Urdu characters: {len(all_urdu_chars)}\")\n",
    "    print(f\"Total mappings: {len(char_ml_model.char_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaee4a0",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed error analysis for all models\n",
    "def analyze_errors(predictions, references, model_name):\n",
    "    print(f\"\\nError Analysis for {model_name}:\")\n",
    "    print(\"=\" * (25 + len(model_name)))\n",
    "    \n",
    "    errors = []\n",
    "    for i, (pred, ref) in enumerate(zip(predictions, references)):\n",
    "        if pred != ref:\n",
    "            word_acc = calculate_word_accuracy(pred, ref)\n",
    "            errors.append({\n",
    "                'index': i,\n",
    "                'roman': test_roman[i],\n",
    "                'reference': ref,\n",
    "                'prediction': pred,\n",
    "                'word_accuracy': word_acc\n",
    "            })\n",
    "    \n",
    "    print(f\"Total errors: {len(errors)} out of {len(predictions)}\")\n",
    "    print(f\"Error rate: {len(errors)/len(predictions)*100:.1f}%\")\n",
    "    \n",
    "    # Show worst errors\n",
    "    errors.sort(key=lambda x: x['word_accuracy'])\n",
    "    print(f\"\\nWorst 5 errors:\")\n",
    "    for error in errors[:5]:\n",
    "        print(f\"  Roman:     {error['roman']}\")\n",
    "        print(f\"  Reference: {error['reference']}\")\n",
    "        print(f\"  Predicted: {error['prediction']}\")\n",
    "        print(f\"  Word Acc:  {error['word_accuracy']:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    return errors\n",
    "\n",
    "# Analyze errors for all models\n",
    "dict_errors = analyze_errors(dict_predictions, test_urdu, \"Dictionary Model\")\n",
    "word_errors = analyze_errors(word_predictions, test_urdu, \"Word-based ML Model\")\n",
    "char_errors = analyze_errors(char_predictions, test_urdu, \"Character-based ML Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8efea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare error patterns\n",
    "print(\"Error Pattern Comparison:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Find common errors across models\n",
    "dict_error_indices = {error['index'] for error in dict_errors}\n",
    "word_error_indices = {error['index'] for error in word_errors}\n",
    "char_error_indices = {error['index'] for error in char_errors}\n",
    "\n",
    "all_errors = dict_error_indices | word_error_indices | char_error_indices\n",
    "common_errors = dict_error_indices & word_error_indices & char_error_indices\n",
    "\n",
    "print(f\"Sentences with errors:\")\n",
    "print(f\"  Dictionary only: {len(dict_error_indices - word_error_indices - char_error_indices)}\")\n",
    "print(f\"  Word ML only: {len(word_error_indices - dict_error_indices - char_error_indices)}\")\n",
    "print(f\"  Char ML only: {len(char_error_indices - dict_error_indices - word_error_indices)}\")\n",
    "print(f\"  Common to all: {len(common_errors)}\")\n",
    "print(f\"  Total unique errors: {len(all_errors)}\")\n",
    "\n",
    "# Show common difficult sentences\n",
    "if common_errors:\n",
    "    print(f\"\\nSentences that all models struggled with:\")\n",
    "    for idx in list(common_errors)[:3]:\n",
    "        print(f\"  Roman: {test_roman[idx]}\")\n",
    "        print(f\"  Reference: {test_urdu[idx]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9bb9b",
   "metadata": {},
   "source": [
    "## 9. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance summary\n",
    "summary_data = {\n",
    "    'Dictionary Model': {\n",
    "        'type': 'Rule-based',\n",
    "        'training_time': 'Instant',\n",
    "        'prediction_speed': 'Very Fast',\n",
    "        'memory_usage': 'Low',\n",
    "        'interpretability': 'High',\n",
    "        'metrics': dict_metrics\n",
    "    },\n",
    "    'Word-based ML': {\n",
    "        'type': 'Machine Learning',\n",
    "        'training_time': 'Fast',\n",
    "        'prediction_speed': 'Fast',\n",
    "        'memory_usage': 'Medium',\n",
    "        'interpretability': 'Medium',\n",
    "        'metrics': word_metrics\n",
    "    },\n",
    "    'Character-based ML': {\n",
    "        'type': 'Machine Learning',\n",
    "        'training_time': 'Fast',\n",
    "        'prediction_speed': 'Fast',\n",
    "        'memory_usage': 'Medium',\n",
    "        'interpretability': 'Low',\n",
    "        'metrics': char_metrics\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Comprehensive Model Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for model_name, info in summary_data.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Type: {info['type']}\")\n",
    "    print(f\"  Training Time: {info['training_time']}\")\n",
    "    print(f\"  Prediction Speed: {info['prediction_speed']}\")\n",
    "    print(f\"  Memory Usage: {info['memory_usage']}\")\n",
    "    print(f\"  Interpretability: {info['interpretability']}\")\n",
    "    print(f\"  Performance:\")\n",
    "    for metric, value in info['metrics'].items():\n",
    "        if 'Distance' in metric:\n",
    "            print(f\"    {metric}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"    {metric}: {value:.3f} ({value*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f911d",
   "metadata": {},
   "source": [
    "## 10. Save Results and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76669197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "os.makedirs('../models/saved', exist_ok=True)\n",
    "\n",
    "# Save ML models\n",
    "word_ml_model.save_model('../models/saved/word_ml_model.pkl')\n",
    "char_ml_model.save_model('../models/saved/char_ml_model.pkl')\n",
    "\n",
    "# Save comprehensive results\n",
    "all_results = {\n",
    "    'test_set_size': len(test_data),\n",
    "    'training_set_size': len(sample_data),\n",
    "    'models': {\n",
    "        'dictionary': {\n",
    "            'metrics': dict_metrics,\n",
    "            'predictions': dict_predictions,\n",
    "            'error_count': len(dict_errors)\n",
    "        },\n",
    "        'word_ml': {\n",
    "            'metrics': word_metrics,\n",
    "            'predictions': word_predictions,\n",
    "            'error_count': len(word_errors)\n",
    "        },\n",
    "        'char_ml': {\n",
    "            'metrics': char_metrics,\n",
    "            'predictions': char_predictions,\n",
    "            'error_count': len(char_errors)\n",
    "        }\n",
    "    },\n",
    "    'comparison': comparison_df.to_dict('records'),\n",
    "    'error_analysis': {\n",
    "        'common_errors': len(common_errors),\n",
    "        'total_error_sentences': len(all_errors)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open('../results/ml_models_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save detailed predictions\n",
    "detailed_results = []\n",
    "for i in range(len(test_data)):\n",
    "    detailed_results.append({\n",
    "        'index': i,\n",
    "        'roman': test_roman[i],\n",
    "        'reference': test_urdu[i],\n",
    "        'english': test_data[i].get('english', ''),\n",
    "        'dict_prediction': dict_predictions[i],\n",
    "        'word_ml_prediction': word_predictions[i],\n",
    "        'char_ml_prediction': char_predictions[i],\n",
    "        'dict_bleu': dict_bleu_scores[i],\n",
    "        'word_ml_bleu': word_bleu_scores[i],\n",
    "        'char_ml_bleu': char_bleu_scores[i],\n",
    "        'dict_word_acc': dict_word_accuracies[i],\n",
    "        'word_ml_word_acc': word_word_accuracies[i],\n",
    "        'char_ml_word_acc': char_word_accuracies[i]\n",
    "    })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_results)\n",
    "detailed_df.to_csv('../results/detailed_model_predictions.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Results and models saved successfully!\")\n",
    "print(\"Files created:\")\n",
    "print(\"  - results/ml_models_results.json\")\n",
    "print(\"  - results/detailed_model_predictions.csv\")\n",
    "print(\"  - models/saved/word_ml_model.pkl\")\n",
    "print(\"  - models/saved/char_ml_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e432e1a",
   "metadata": {},
   "source": [
    "## 11. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1331128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final performance ranking\n",
    "model_scores = {\n",
    "    'Dictionary': np.mean([dict_metrics['BLEU'], dict_metrics['Word_Accuracy'], dict_metrics['Character_Accuracy']]),\n",
    "    'Word ML': np.mean([word_metrics['BLEU'], word_metrics['Word_Accuracy'], word_metrics['Character_Accuracy']]),\n",
    "    'Character ML': np.mean([char_metrics['BLEU'], char_metrics['Word_Accuracy'], char_metrics['Character_Accuracy']])\n",
    "}\n",
    "\n",
    "ranked_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MACHINE LEARNING APPROACH SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nModel Performance Ranking:\")\n",
    "for i, (model, score) in enumerate(ranked_models, 1):\n",
    "    print(f\"{i}. {model}: {score:.3f} (average score)\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"1. Dictionary Model:\")\n",
    "print(f\"   - Excellent performance on known vocabulary\")\n",
    "print(f\"   - BLEU: {dict_metrics['BLEU']:.3f}, Word Accuracy: {dict_metrics['Word_Accuracy']:.3f}\")\n",
    "print(f\"   - Fast and interpretable\")\n",
    "\n",
    "print(\"\\n2. Word-based ML Model:\")\n",
    "print(f\"   - Good generalization capabilities\")\n",
    "print(f\"   - BLEU: {word_metrics['BLEU']:.3f}, Word Accuracy: {word_metrics['Word_Accuracy']:.3f}\")\n",
    "print(f\"   - Handles unknown words better\")\n",
    "\n",
    "print(\"\\n3. Character-based ML Model:\")\n",
    "print(f\"   - Character-level understanding\")\n",
    "print(f\"   - BLEU: {char_metrics['BLEU']:.3f}, Word Accuracy: {char_metrics['Word_Accuracy']:.3f}\")\n",
    "print(f\"   - Most flexible for variations\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. Use Dictionary Model for high-accuracy, known vocabulary scenarios\")\n",
    "print(\"2. Use Word-based ML for balanced performance and generalization\")\n",
    "print(\"3. Use Character-based ML for handling spelling variations\")\n",
    "print(\"4. Consider ensemble approaches combining multiple models\")\n",
    "print(\"5. Implement hybrid systems using dictionary + ML fallback\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Implement deep learning approaches (Seq2Seq, Transformer)\")\n",
    "print(\"2. Expand training data for better ML performance\")\n",
    "print(\"3. Develop ensemble methods\")\n",
    "print(\"4. Implement human evaluation studies\")\n",
    "print(\"5. Deploy best performing model for real-world testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12a148",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Machine Learning Approach Results:\n",
    "\n",
    "#### Word-based ML Model:\n",
    "- **Strengths**: Good balance between accuracy and generalization\n",
    "- **Performance**: Competitive with dictionary on known vocabulary\n",
    "- **Use Case**: Ideal for scenarios requiring good coverage and reasonable accuracy\n",
    "\n",
    "#### Character-based ML Model:\n",
    "- **Strengths**: Handles spelling variations and unknown words\n",
    "- **Performance**: Lower precision but better robustness\n",
    "- **Use Case**: Suitable for noisy text with many variations\n",
    "\n",
    "### Comparison with Dictionary Approach:\n",
    "- Dictionary model remains highly competitive for covered vocabulary\n",
    "- ML models provide better generalization for unknown scenarios\n",
    "- Hybrid approaches could combine strengths of both\n",
    "\n",
    "### Technical Insights:\n",
    "1. **Feature Engineering**: TF-IDF features work well for this task\n",
    "2. **Model Selection**: Random Forest provides good balance of performance and interpretability\n",
    "3. **Data Requirements**: More training data would significantly improve ML performance\n",
    "4. **Error Patterns**: Common errors across models suggest inherent task difficulty\n",
    "\n",
    "### Future Work:\n",
    "- Implement sequence-to-sequence deep learning models\n",
    "- Expand training dataset significantly\n",
    "- Develop context-aware models\n",
    "- Create ensemble methods\n",
    "- Conduct human evaluation studies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
