{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341f7911",
   "metadata": {},
   "source": [
    "# Dictionary-Based Approach Implementation\n",
    "## Roman Urdu to Urdu Script Conversion Project\n",
    "\n",
    "This notebook covers Step 3 of our methodology:\n",
    "- Dictionary-Based Model Implementation\n",
    "- Model Testing and Evaluation\n",
    "- Performance Analysis\n",
    "\n",
    "### Objectives:\n",
    "1. Load and test the dictionary-based conversion model\n",
    "2. Evaluate performance on test data\n",
    "3. Analyze strengths and limitations\n",
    "4. Implement improvements and optimizations\n",
    "5. Compare different dictionary strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e775b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('../')\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from models.dictionary_model import DictionaryModel\n",
    "from utils.data_loader import DataLoader\n",
    "from utils.preprocessing import RomanUrduPreprocessor\n",
    "from evaluation.metrics import (\n",
    "    calculate_bleu_score, calculate_rouge_l, calculate_word_accuracy,\n",
    "    calculate_sentence_accuracy, calculate_character_accuracy, calculate_edit_distance\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b506be2",
   "metadata": {},
   "source": [
    "## 1. Model Initialization and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "data_loader = DataLoader(\"../data\")\n",
    "preprocessor = RomanUrduPreprocessor()\n",
    "\n",
    "# Load data\n",
    "dictionary = data_loader.load_dictionary()\n",
    "test_data = data_loader.load_test_data()\n",
    "sample_data = data_loader.load_sample_data()\n",
    "\n",
    "# Initialize dictionary model\n",
    "dict_model = DictionaryModel(\"../data/roman_urdu_dictionary.json\")\n",
    "\n",
    "print(f\"Dictionary loaded with {len(dictionary)} entries\")\n",
    "print(f\"Test data: {len(test_data)} sentences\")\n",
    "print(f\"Sample data: {len(sample_data)} sentences\")\n",
    "print(f\"Dictionary model initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba4e5e3",
   "metadata": {},
   "source": [
    "## 2. Basic Dictionary Conversion Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945bb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual word conversions\n",
    "test_words = [\n",
    "    \"main\", \"aap\", \"kaise\", \"hain\", \"ghar\", \"ja\", \"raha\", \"hun\",\n",
    "    \"kitab\", \"parh\", \"kya\", \"kar\", \"rahe\", \"ho\", \"time\", \"school\"\n",
    "]\n",
    "\n",
    "print(\"Individual Word Conversions:\")\n",
    "print(\"=\" * 40)\n",
    "for word in test_words:\n",
    "    converted = dict_model.convert_word(word)\n",
    "    print(f\"{word:12} -> {converted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90dfeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentence conversions\n",
    "test_sentences = [\n",
    "    \"main acha hun\",\n",
    "    \"aap kaise hain\",\n",
    "    \"wo ghar ja raha hai\",\n",
    "    \"main kitab parh raha hun\",\n",
    "    \"aap kya kar rahe hain\"\n",
    "]\n",
    "\n",
    "print(\"\\nSentence Conversions:\")\n",
    "print(\"=\" * 50)\n",
    "for sentence in test_sentences:\n",
    "    converted = dict_model.convert_text(sentence)\n",
    "    print(f\"Roman: {sentence}\")\n",
    "    print(f\"Urdu:  {converted}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc14de",
   "metadata": {},
   "source": [
    "## 3. Performance Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9abdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "predictions = []\n",
    "references = []\n",
    "conversion_details = []\n",
    "\n",
    "print(\"Evaluating Dictionary Model on Test Data:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, item in enumerate(test_data):\n",
    "    roman_text = item['roman']\n",
    "    reference_urdu = item['urdu']\n",
    "    \n",
    "    # Convert using dictionary model\n",
    "    predicted_urdu = dict_model.convert_text(roman_text)\n",
    "    \n",
    "    predictions.append(predicted_urdu)\n",
    "    references.append(reference_urdu)\n",
    "    \n",
    "    conversion_details.append({\n",
    "        'index': i,\n",
    "        'roman': roman_text,\n",
    "        'reference': reference_urdu,\n",
    "        'prediction': predicted_urdu,\n",
    "        'english': item.get('english', '')\n",
    "    })\n",
    "    \n",
    "    if i < 5:  # Show first 5 examples\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Roman:     {roman_text}\")\n",
    "        print(f\"  Reference: {reference_urdu}\")\n",
    "        print(f\"  Predicted: {predicted_urdu}\")\n",
    "        print(f\"  English:   {item.get('english', 'N/A')}\")\n",
    "        print()\n",
    "\n",
    "print(f\"\\nTotal conversions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2e288",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1391155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all evaluation metrics\n",
    "metrics_results = {}\n",
    "\n",
    "# BLEU Score\n",
    "bleu_scores = []\n",
    "for pred, ref in zip(predictions, references):\n",
    "    bleu = calculate_bleu_score(pred, ref)\n",
    "    bleu_scores.append(bleu)\n",
    "metrics_results['BLEU'] = np.mean(bleu_scores)\n",
    "\n",
    "# ROUGE-L Score\n",
    "rouge_scores = []\n",
    "for pred, ref in zip(predictions, references):\n",
    "    rouge = calculate_rouge_l(pred, ref)\n",
    "    rouge_scores.append(rouge)\n",
    "metrics_results['ROUGE-L'] = np.mean(rouge_scores)\n",
    "\n",
    "# Word Accuracy\n",
    "word_accuracies = []\n",
    "for pred, ref in zip(predictions, references):\n",
    "    acc = calculate_word_accuracy(pred, ref)\n",
    "    word_accuracies.append(acc)\n",
    "metrics_results['Word_Accuracy'] = np.mean(word_accuracies)\n",
    "\n",
    "# Sentence Accuracy\n",
    "sentence_accuracy = calculate_sentence_accuracy(predictions, references)\n",
    "metrics_results['Sentence_Accuracy'] = sentence_accuracy\n",
    "\n",
    "# Character Accuracy\n",
    "char_accuracies = []\n",
    "for pred, ref in zip(predictions, references):\n",
    "    acc = calculate_character_accuracy(pred, ref)\n",
    "    char_accuracies.append(acc)\n",
    "metrics_results['Character_Accuracy'] = np.mean(char_accuracies)\n",
    "\n",
    "# Edit Distance\n",
    "edit_distances = []\n",
    "for pred, ref in zip(predictions, references):\n",
    "    dist = calculate_edit_distance(pred, ref)\n",
    "    edit_distances.append(dist)\n",
    "metrics_results['Avg_Edit_Distance'] = np.mean(edit_distances)\n",
    "\n",
    "print(\"Dictionary Model Performance Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in metrics_results.items():\n",
    "    if 'Distance' in metric:\n",
    "        print(f\"{metric:20}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{metric:20}: {value:.3f} ({value*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7202818",
   "metadata": {},
   "source": [
    "## 5. Detailed Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze conversion errors\n",
    "word_level_errors = []\n",
    "unknown_words = set()\n",
    "partial_matches = []\n",
    "\n",
    "for detail in conversion_details:\n",
    "    roman_words = detail['roman'].split()\n",
    "    ref_words = detail['reference'].split()\n",
    "    pred_words = detail['prediction'].split()\n",
    "    \n",
    "    # Track unknown words\n",
    "    for word in roman_words:\n",
    "        normalized = preprocessor.normalize_spelling(word.lower())\n",
    "        if normalized not in dictionary:\n",
    "            unknown_words.add(word)\n",
    "    \n",
    "    # Word-level error analysis\n",
    "    max_len = max(len(ref_words), len(pred_words))\n",
    "    for i in range(max_len):\n",
    "        ref_word = ref_words[i] if i < len(ref_words) else ''\n",
    "        pred_word = pred_words[i] if i < len(pred_words) else ''\n",
    "        \n",
    "        if ref_word != pred_word:\n",
    "            word_level_errors.append({\n",
    "                'sentence_idx': detail['index'],\n",
    "                'position': i,\n",
    "                'reference': ref_word,\n",
    "                'prediction': pred_word,\n",
    "                'roman_context': detail['roman']\n",
    "            })\n",
    "\n",
    "print(f\"Error Analysis Summary:\")\n",
    "print(f\"Total word-level errors: {len(word_level_errors)}\")\n",
    "print(f\"Unknown words: {len(unknown_words)}\")\n",
    "\n",
    "print(f\"\\nSample unknown words:\")\n",
    "for word in list(unknown_words)[:15]:\n",
    "    print(f\"  {word}\")\n",
    "\n",
    "print(f\"\\nSample word-level errors:\")\n",
    "for error in word_level_errors[:10]:\n",
    "    print(f\"  Sentence {error['sentence_idx']}: '{error['reference']}' vs '{error['prediction']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage analysis\n",
    "total_test_words = 0\n",
    "covered_test_words = 0\n",
    "\n",
    "for item in test_data:\n",
    "    words = preprocessor.tokenize(item['roman'])\n",
    "    total_test_words += len(words)\n",
    "    \n",
    "    for word in words:\n",
    "        normalized = preprocessor.normalize_spelling(word.lower())\n",
    "        if normalized in dictionary:\n",
    "            covered_test_words += 1\n",
    "\n",
    "test_coverage = (covered_test_words / total_test_words) * 100 if total_test_words > 0 else 0\n",
    "\n",
    "print(f\"Test Data Coverage Analysis:\")\n",
    "print(f\"Total words in test data: {total_test_words}\")\n",
    "print(f\"Covered by dictionary: {covered_test_words}\")\n",
    "print(f\"Coverage percentage: {test_coverage:.2f}%\")\n",
    "print(f\"Uncovered words: {total_test_words - covered_test_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492a019",
   "metadata": {},
   "source": [
    "## 6. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# BLEU scores distribution\n",
    "ax1.hist(bleu_scores, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('BLEU Scores Distribution')\n",
    "ax1.set_xlabel('BLEU Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(np.mean(bleu_scores), color='red', linestyle='--', label=f'Mean: {np.mean(bleu_scores):.3f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Word accuracy distribution\n",
    "ax2.hist(word_accuracies, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "ax2.set_title('Word Accuracy Distribution')\n",
    "ax2.set_xlabel('Word Accuracy')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(np.mean(word_accuracies), color='red', linestyle='--', label=f'Mean: {np.mean(word_accuracies):.3f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Edit distance distribution\n",
    "ax3.hist(edit_distances, bins=15, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "ax3.set_title('Edit Distance Distribution')\n",
    "ax3.set_xlabel('Edit Distance')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.axvline(np.mean(edit_distances), color='red', linestyle='--', label=f'Mean: {np.mean(edit_distances):.1f}')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Overall metrics comparison\n",
    "metrics_names = ['BLEU', 'ROUGE-L', 'Word Acc', 'Char Acc']\n",
    "metrics_values = [\n",
    "    metrics_results['BLEU'],\n",
    "    metrics_results['ROUGE-L'],\n",
    "    metrics_results['Word_Accuracy'],\n",
    "    metrics_results['Character_Accuracy']\n",
    "]\n",
    "\n",
    "bars = ax4.bar(metrics_names, metrics_values, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "ax4.set_title('Overall Performance Metrics')\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec24b75",
   "metadata": {},
   "source": [
    "## 7. Dictionary Enhancement Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fuzzy matching performance\n",
    "fuzzy_test_words = [\n",
    "    \"mein\",     # should match \"main\"\n",
    "    \"kese\",     # should match \"kaise\"\n",
    "    \"hen\",      # should match \"hain\"\n",
    "    \"rahay\",    # should match \"rahe\"\n",
    "    \"kry\",      # should match \"kar\"\n",
    "]\n",
    "\n",
    "print(\"Fuzzy Matching Test:\")\n",
    "print(\"=\" * 30)\n",
    "for word in fuzzy_test_words:\n",
    "    converted = dict_model.convert_word(word)\n",
    "    best_match = dict_model.find_best_match(word)\n",
    "    print(f\"{word:10} -> {converted:15} (best match: {best_match})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test spelling variations\n",
    "variation_tests = {\n",
    "    \"main\": [\"mein\", \"mian\", \"men\"],\n",
    "    \"kaise\": [\"kese\", \"keyse\", \"kaisy\"],\n",
    "    \"hain\": [\"hen\", \"han\", \"hein\"],\n",
    "    \"rahe\": [\"rahay\", \"rhy\", \"rehy\"]\n",
    "}\n",
    "\n",
    "print(\"Spelling Variation Handling:\")\n",
    "print(\"=\" * 40)\n",
    "for standard, variations in variation_tests.items():\n",
    "    standard_result = dict_model.convert_word(standard)\n",
    "    print(f\"Standard '{standard}' -> {standard_result}\")\n",
    "    \n",
    "    for variation in variations:\n",
    "        var_result = dict_model.convert_word(variation)\n",
    "        match_score = dict_model.calculate_similarity(standard, variation)\n",
    "        print(f\"  Variation '{variation}' -> {var_result} (similarity: {match_score:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6a8c9",
   "metadata": {},
   "source": [
    "## 8. Model Statistics and Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model statistics\n",
    "stats = dict_model.get_stats()\n",
    "\n",
    "print(\"Dictionary Model Statistics:\")\n",
    "print(\"=\" * 30)\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key:25}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key:25}: {value}\")\n",
    "\n",
    "# Performance breakdown by sentence length\n",
    "length_performance = defaultdict(list)\n",
    "\n",
    "for i, detail in enumerate(conversion_details):\n",
    "    sentence_length = len(detail['roman'].split())\n",
    "    word_acc = word_accuracies[i]\n",
    "    length_performance[sentence_length].append(word_acc)\n",
    "\n",
    "print(\"\\nPerformance by Sentence Length:\")\n",
    "print(\"=\" * 35)\n",
    "for length in sorted(length_performance.keys()):\n",
    "    avg_acc = np.mean(length_performance[length])\n",
    "    count = len(length_performance[length])\n",
    "    print(f\"{length:2} words: {avg_acc:.3f} accuracy ({count} sentences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e709f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance by sentence length\n",
    "lengths = list(length_performance.keys())\n",
    "avg_accuracies = [np.mean(length_performance[length]) for length in lengths]\n",
    "counts = [len(length_performance[length]) for length in lengths]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Performance by length\n",
    "ax1.plot(lengths, avg_accuracies, marker='o', linewidth=2, markersize=8)\n",
    "ax1.set_title('Performance vs Sentence Length')\n",
    "ax1.set_xlabel('Sentence Length (words)')\n",
    "ax1.set_ylabel('Average Word Accuracy')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Sample count by length\n",
    "ax2.bar(lengths, counts, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "ax2.set_title('Sample Count by Sentence Length')\n",
    "ax2.set_xlabel('Sentence Length (words)')\n",
    "ax2.set_ylabel('Number of Sentences')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c0e9d",
   "metadata": {},
   "source": [
    "## 9. Comparison with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple baseline (no conversion)\n",
    "baseline_predictions = [item['roman'] for item in test_data]\n",
    "\n",
    "# Calculate baseline metrics\n",
    "baseline_bleu = np.mean([calculate_bleu_score(pred, ref) for pred, ref in zip(baseline_predictions, references)])\n",
    "baseline_word_acc = np.mean([calculate_word_accuracy(pred, ref) for pred, ref in zip(baseline_predictions, references)])\n",
    "baseline_char_acc = np.mean([calculate_character_accuracy(pred, ref) for pred, ref in zip(baseline_predictions, references)])\n",
    "\n",
    "# Comparison\n",
    "comparison_data = {\n",
    "    'Metric': ['BLEU', 'Word Accuracy', 'Character Accuracy'],\n",
    "    'Baseline': [baseline_bleu, baseline_word_acc, baseline_char_acc],\n",
    "    'Dictionary Model': [\n",
    "        metrics_results['BLEU'],\n",
    "        metrics_results['Word_Accuracy'],\n",
    "        metrics_results['Character_Accuracy']\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['Improvement'] = comparison_df['Dictionary Model'] - comparison_df['Baseline']\n",
    "\n",
    "print(\"Model vs Baseline Comparison:\")\n",
    "print(\"=\" * 45)\n",
    "print(comparison_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison_data['Metric']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_data['Baseline'], width, label='Baseline', alpha=0.7)\n",
    "bars2 = ax.bar(x + width/2, comparison_data['Dictionary Model'], width, label='Dictionary Model', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Dictionary Model vs Baseline Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_data['Metric'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bf834",
   "metadata": {},
   "source": [
    "## 10. Export Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947dd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comprehensive results\n",
    "results = {\n",
    "    'model_type': 'Dictionary-Based',\n",
    "    'test_set_size': len(test_data),\n",
    "    'dictionary_size': len(dictionary),\n",
    "    'metrics': metrics_results,\n",
    "    'coverage': {\n",
    "        'total_words': total_test_words,\n",
    "        'covered_words': covered_test_words,\n",
    "        'coverage_percentage': test_coverage\n",
    "    },\n",
    "    'error_analysis': {\n",
    "        'total_errors': len(word_level_errors),\n",
    "        'unknown_words_count': len(unknown_words),\n",
    "        'unknown_words': list(unknown_words)\n",
    "    },\n",
    "    'model_stats': stats\n",
    "}\n",
    "\n",
    "# Save detailed results\n",
    "with open('../results/dictionary_model_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save predictions for further analysis\n",
    "predictions_df = pd.DataFrame(conversion_details)\n",
    "predictions_df['bleu_score'] = bleu_scores\n",
    "predictions_df['word_accuracy'] = word_accuracies\n",
    "predictions_df['edit_distance'] = edit_distances\n",
    "\n",
    "predictions_df.to_csv('../results/dictionary_model_predictions.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Results exported successfully!\")\n",
    "print(\"Files created:\")\n",
    "print(\"  - results/dictionary_model_results.json\")\n",
    "print(\"  - results/dictionary_model_predictions.csv\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DICTIONARY MODEL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Overall Performance:\")\n",
    "print(f\"  BLEU Score:      {metrics_results['BLEU']:.3f}\")\n",
    "print(f\"  Word Accuracy:   {metrics_results['Word_Accuracy']:.3f}\")\n",
    "print(f\"  Sentence Acc:    {metrics_results['Sentence_Accuracy']:.3f}\")\n",
    "print(f\"  Coverage:        {test_coverage:.1f}%\")\n",
    "print(f\"\\nStrengths:\")\n",
    "print(f\"  - Fast and efficient conversion\")\n",
    "print(f\"  - High accuracy for known words\")\n",
    "print(f\"  - Interpretable results\")\n",
    "print(f\"\\nLimitations:\")\n",
    "print(f\"  - Limited to dictionary vocabulary\")\n",
    "print(f\"  - {len(unknown_words)} unknown words in test set\")\n",
    "print(f\"  - No context awareness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44833ac",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "1. **Performance**: The dictionary model achieves good performance on covered vocabulary\n",
    "2. **Coverage**: Dictionary coverage is a critical limiting factor\n",
    "3. **Fuzzy Matching**: Helps handle spelling variations effectively\n",
    "4. **Speed**: Very fast conversion suitable for real-time applications\n",
    "\n",
    "### Strengths:\n",
    "- Simple and interpretable\n",
    "- Fast execution\n",
    "- High precision for known words\n",
    "- Easy to update and maintain\n",
    "\n",
    "### Limitations:\n",
    "- Limited vocabulary coverage\n",
    "- No context awareness\n",
    "- Cannot handle new/unknown words well\n",
    "- Fixed mapping without learning capability\n",
    "\n",
    "### Recommendations:\n",
    "1. Expand dictionary with more common words\n",
    "2. Implement better fuzzy matching algorithms\n",
    "3. Add context-aware word disambiguation\n",
    "4. Combine with ML models for unknown words\n",
    "\n",
    "### Next Steps:\n",
    "- Implement machine learning models for comparison\n",
    "- Develop hybrid approaches combining dictionary and ML\n",
    "- Evaluate ensemble methods for improved performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
