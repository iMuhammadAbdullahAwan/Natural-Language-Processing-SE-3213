# Roman Urdu to Urdu Conversion Project - Quick Guide

This project provides a complete Roman Urdu to Urdu conversion system with multiple models, comprehensive evaluation, and user interfaces.

## ğŸ“ Project Structure

```
Roman_Urdu_Conversion_Project/
â”œâ”€â”€ ğŸ“Š report_images/                    # Performance metric images for reports
â”‚   â”œâ”€â”€ metrics_overview_bar.png         # Overall model comparison
â”‚   â”œâ”€â”€ avg_edit_distance_bar.png        # Edit distance analysis
â”‚   â”œâ”€â”€ error_types_by_model.png         # Error pattern analysis
â”‚   â”œâ”€â”€ avg_lengths_by_model.png         # Length statistics
â”‚   â”œâ”€â”€ length_ratio_by_model.png        # Length ratio analysis
â”‚   â””â”€â”€ metric_summary_table.png         # Comprehensive metrics table
â”‚
â”œâ”€â”€ ğŸ“‹ Reports & Documentation
â”‚   â”œâ”€â”€ PROJECT_REPORT.md                # Complete detailed project report
â”‚   â”œâ”€â”€ EVALUATION_REPORT.md             # Generated evaluation summary
â”‚   â””â”€â”€ README.md                        # This quick guide
â”‚
â”œâ”€â”€ ğŸ¤– Models & Core System
â”‚   â”œâ”€â”€ models/                          # Model implementations
â”‚   â”œâ”€â”€ evaluation/                      # Evaluation framework
â”‚   â”œâ”€â”€ utils/                          # Utilities and preprocessing
â”‚   â””â”€â”€ data/                           # Training and test data
â”‚
â””â”€â”€ ğŸ–¥ï¸ User Interfaces
    â”œâ”€â”€ streamlit_app.py                 # Web interface
    â””â”€â”€ main.py                          # Command line interface
```

## ğŸš€ Quick Start

### 1. Installation
```bash
# Install dependencies
pip install numpy pandas matplotlib seaborn scikit-learn joblib arabic-reshaper python-bidi tabulate

# For web interface
pip install streamlit
```

### 2. Run Web Interface
```bash
streamlit run streamlit_app.py
```

### 3. Run Evaluation (Generate New Images)
```bash
python evaluation/evaluate.py
```

### 4. Command Line Usage
```bash
python main.py --text "aap kaise hain" --model dictionary
```

## ğŸ“Š Performance Results

| Model | Word Accuracy | BLEU Score | ROUGE-L | Best For |
|-------|---------------|------------|---------|----------|
| **ML Word-based** | **55.9%** | **0.171** | **0.566** | Overall performance |
| Dictionary-based | 34.2% | 0.091 | 0.369 | Fast lookup, common words |
| ML Character-based | 0.0% | 0.000 | 0.000 | Needs improvement |

## ğŸ“ˆ Generated Images (in report_images/)

All performance metric visualizations are automatically saved as PNG files in the `report_images/` folder:

1. **metrics_overview_bar.png** - Side-by-side comparison of all models across main metrics
2. **avg_edit_distance_bar.png** - Edit distance comparison (lower is better)
3. **error_types_by_model.png** - Breakdown of error types by model
4. **avg_lengths_by_model.png** - Average text length analysis
5. **length_ratio_by_model.png** - Prediction vs reference length ratios
6. **metric_summary_table.png** - Complete metrics table as image

## ğŸ“– Documentation

### Complete Project Report
ğŸ‘‰ **[PROJECT_REPORT.md](PROJECT_REPORT.md)** - Comprehensive 50+ page report including:
- Executive Summary
- Literature Review
- Methodology & Implementation
- Detailed Results Analysis
- Future Work & Recommendations
- Complete Technical Documentation

### Evaluation Report
ğŸ‘‰ **[EVALUATION_REPORT.md](EVALUATION_REPORT.md)** - Auto-generated evaluation summary with:
- Performance metrics table
- Model recommendations
- Links to all visualization images

## ğŸ”§ System Components

### Models Implemented
- **Dictionary Model**: Rule-based lookup with fuzzy matching
- **ML Word Model**: TF-IDF + Logistic Regression (word-level)
- **ML Character Model**: Context windows + Random Forest (character-level)
- **Seq2Seq Framework**: Ready for future deep learning models

### Evaluation Metrics
- Word Accuracy, Sentence Accuracy, Character Accuracy
- BLEU Score, ROUGE-L, METEOR Score
- Edit Distance, Error Type Analysis
- Length and Coverage Analysis

### User Interfaces
- **Streamlit Web App**: Interactive conversion with model selection
- **Command Line**: Batch processing and evaluation tools

## ğŸ¯ Key Features

âœ… **Multiple Model Architectures** - Dictionary, ML, and deep learning approaches  
âœ… **Comprehensive Evaluation** - 6+ metrics with visual analysis  
âœ… **Performance Images** - Ready-to-use charts for presentations/reports  
âœ… **User-Friendly Interface** - Web app for real-time conversion  
âœ… **Extensible Design** - Easy to add new models and metrics  
âœ… **Complete Documentation** - Detailed technical and user documentation  

## ğŸ“Š Usage Examples

### Web Interface
1. Open `streamlit run streamlit_app.py`
2. Enter Roman Urdu text
3. Select model
4. Get instant Urdu conversion

### Evaluation
```bash
# Run full evaluation and generate new images
python evaluation/evaluate.py

# Images will be saved to report_images/
# Reports will be updated with latest results
```

### CLI Conversion
```bash
# Single text conversion
python main.py --text "main acha hun" --model ml_word

# Batch file processing
python main.py --file input.txt --output output.txt --model dictionary
```

## ğŸ” For Report Writing

All necessary components for academic/technical reports:

1. **Images**: Use any from `report_images/` folder
2. **Metrics**: Reference `EVALUATION_REPORT.md` for latest numbers
3. **Methodology**: See `PROJECT_REPORT.md` sections 4-8
4. **Results**: Tables and analysis in `PROJECT_REPORT.md` section 9
5. **Code**: All source available in organized modules

## ğŸš€ Next Steps

1. **Immediate Use**: Images and reports are ready for presentations
2. **Model Improvement**: Add more training data and try deep learning models
3. **Feature Enhancement**: Add context awareness and better error handling
4. **Deployment**: Scale up for production use

---

**Quick Access Links:**
- ğŸ“„ [Complete Project Report](PROJECT_REPORT.md)
- ğŸ“Š [Evaluation Results](EVALUATION_REPORT.md)
- ğŸ–¼ï¸ [Performance Images](report_images/)
- ğŸŒ [Web Interface](streamlit_app.py)

*Generated: September 2, 2025*
