{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf14364c-6843-4cd7-b1d7-03a374e7ed19",
   "metadata": {},
   "source": [
    "As a beginner with a passion for NLP and a long-term goal in R&D, you’re progressing through the updated course outline to build a strong foundation, having completed Chapters 1–5 (Introduction to NLP, Text Preprocessing, Text Representation, Basic NLP Tasks, Text Classification and Sentiment Analysis). Below is a detailed, beginner-friendly version of **Chapter 6: Language Models and N-Grams**, designed for someone with basic Python skills and knowledge from prior chapters. This chapter introduces language models and N-grams, foundational concepts for text generation and understanding, essential for NLP research and applications like chatbots or predictive text. It aligns with your R&D aspirations by emphasizing hands-on practice, research connections, and portfolio-building, while keeping the content engaging and accessible.\n",
    "\n",
    "This chapter includes:\n",
    "- **Theory**: Clear explanations of language models, N-grams, and Markov chains, tailored for beginners.\n",
    "- **Practical**: Step-by-step tasks using free tools (NLTK, Python) and a new dataset (X posts) to avoid repetition.\n",
    "- **Mini-Project**: A Text Generator to produce short sentences, building coding skills and a portfolio piece.\n",
    "- **Resources**: Free, beginner-friendly materials.\n",
    "- **Debugging Tips**: Solutions to common beginner issues.\n",
    "- **Checkpoints**: Quizzes and tasks to confirm mastery.\n",
    "- **R&D Focus**: Links to research concepts (e.g., perplexity) to inspire your long-term goal.\n",
    "\n",
    "The dataset (X posts) is fresh compared to your previous work (e.g., Gutenberg, IMDB, BBC News, Wikipedia), ensuring variety and relevance to real-world NLP. The content is structured for self-paced learning, with clear steps to build confidence and prepare for advanced NLP research.\n",
    "\n",
    "**Time Estimate**: ~15 hours (spread over 1–2 weeks, 7–10 hours/week).  \n",
    "**Tools**: Free (Google Colab, NLTK, Pandas).  \n",
    "**Dataset**: [Sentiment140 (Twitter-like X posts)](https://www.kaggle.com/datasets/kazanova/sentiment140) (free on Kaggle).  \n",
    "**Prerequisites**: Basic Python (Chapter 1), text preprocessing (Chapter 2), text representation (Chapter 3), POS/NER (Chapter 4), text classification (Chapter 5); Colab or Anaconda setup with libraries (`pip install nltk pandas`).  \n",
    "**Date**: June 23, 2025.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter 6: Language Models and N-Grams**\n",
    "\n",
    "*Goal*: Understand language models and N-grams, build a simple text generator using Markov chains, and learn evaluation techniques, preparing for research-grade NLP tasks.\n",
    "\n",
    "### **Theory (4 hours)**\n",
    "\n",
    "#### **What are Language Models?**\n",
    "- **Definition**: Models that predict the probability of a word or sequence of words in a given context.\n",
    "  - Example: Given “I love to,” a model predicts the next word (e.g., “eat”).\n",
    "- **Why They Matter**: Power applications like autocomplete, chatbots, and machine translation.\n",
    "- **R&D Relevance**: In research, language models (e.g., GPT, BERT) are central to tasks like text generation and dialogue systems.\n",
    "\n",
    "#### **Key Concepts**\n",
    "1. **N-Grams**:\n",
    "   - **What**: Sequences of N consecutive words used to model language.\n",
    "   - **Types**:\n",
    "     - Unigram (N=1): Single words (e.g., “I,” “love”).\n",
    "     - Bigram (N=2): Word pairs (e.g., “I love,” “love to”).\n",
    "     - Trigram (N=3): Three-word sequences (e.g., “I love to”).\n",
    "   - **Example**: For “I love to eat,” bigrams are [(“I”, “love”), (“love”, “to”), (“to”, “eat”)].\n",
    "   - **Why**: Captures local context (e.g., “love to” is more likely than “love apple”).\n",
    "   - **Pros**: Simple, fast, interpretable.\n",
    "   - **Cons**: Limited context (e.g., bigrams ignore words beyond pairs).\n",
    "2. **Language Models**:\n",
    "   - **What**: Assign probabilities to word sequences based on training data.\n",
    "   - **Example**: P(“eat” | “I love to”) = count(“I love to eat”) / count(“I love to”).\n",
    "   - **Types**:\n",
    "     - Statistical: Based on N-grams (this chapter).\n",
    "     - Neural: Based on networks like RNNs or transformers (Chapter 10).\n",
    "   - **Why**: Predicts next words for generation or scores text likelihood.\n",
    "3. **Markov Chains for Text Generation**:\n",
    "   - **What**: A model where the next word depends only on the current state (e.g., previous word for bigrams).\n",
    "   - **Example**: Given “I love,” pick “to” based on bigram probabilities.\n",
    "   - **Why**: Simple way to generate text using N-grams.\n",
    "   - **Process**:\n",
    "     - Build transition probabilities (e.g., “love” → “to” with 0.5 probability).\n",
    "     - Generate text by sampling next words.\n",
    "4. **Evaluation**:\n",
    "   - **Perplexity**: Measures how well a model predicts text (lower is better).\n",
    "     - Intuition: How “surprised” the model is by new text.\n",
    "     - Formula: Perplexity = 2^(-average log probability).\n",
    "   - **Human Evaluation**: Check generated text for coherence (e.g., does it sound natural?).\n",
    "   - **Research Insight**: Perplexity is widely used in NLP papers to compare language models.\n",
    "\n",
    "#### **Trade-Offs**\n",
    "- **N-Grams**:\n",
    "  - Bigrams: Simple but short context.\n",
    "  - Trigrams+: Better context but sparse data (fewer occurrences).\n",
    "- **Markov Chains**: Fast but limited by N-gram context; neural models (later chapters) are more powerful.\n",
    "- **Challenges**: Rare words or long dependencies (e.g., connecting words across sentences) are hard for N-grams.\n",
    "\n",
    "#### **Resources**\n",
    "- [NLTK Language Models](https://www.nltk.org/api/nltk.lm.html): Beginner guide to N-grams.\n",
    "- [Jurafsky’s NLP Book, Chapter 3](https://web.stanford.edu/~jurafsky/slp3/3.pdf): Free PDF on language models.\n",
    "- [Stanford CS224N Lecture 4](https://www.youtube.com/watch?v=rmVRLeJRklI): Free video on N-grams (optional).\n",
    "- **R&D Resource**: Skim the introduction of [Bengio, 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) (5 minutes) for early neural language models.\n",
    "\n",
    "#### **Learning Tips**\n",
    "- Note why bigrams capture more context than unigrams.\n",
    "- Search X for #NLP or #LanguageModels to see discussions (I can analyze posts if you share links).\n",
    "- Think about using language models for your R&D goal (e.g., generating X post replies).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical (8 hours)**\n",
    "\n",
    "*Goal*: Build an N-gram model and use Markov chains to generate text, applying preprocessing skills.\n",
    "\n",
    "#### **Setup**\n",
    "- **Environment**: Google Colab (free GPU) or Anaconda (from Chapter 1).\n",
    "- **Libraries**: Install (run in Colab or terminal):\n",
    "  ```bash\n",
    "  pip install nltk pandas\n",
    "  ```\n",
    "  ```python\n",
    "  import nltk\n",
    "  nltk.download('punkt')\n",
    "  nltk.download('stopwords')\n",
    "  ```\n",
    "- **Dataset**: [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140).\n",
    "  - Download: Sign up for Kaggle, download `training.1600000.processed.noemoticon.csv`.\n",
    "  - Why? Short, real-world text (Twitter-like X posts) ideal for N-gram modeling.\n",
    "  - Columns: `text` (post content), `target` (sentiment, ignored here).\n",
    "- **Load Data**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None, usecols=[5])[:1000]  # Use 1000 posts\n",
    "  df.columns = ['text']\n",
    "  print(df.head())\n",
    "  ```\n",
    "\n",
    "#### **Tasks**\n",
    "1. **Preprocessing (2 hours)**:\n",
    "   - Clean and preprocess X posts using Chapter 2 skills (remove URLs, lowercase, remove stopwords).\n",
    "   - Code:\n",
    "     ```python\n",
    "     import re\n",
    "     from nltk.tokenize import word_tokenize\n",
    "     from nltk.corpus import stopwords\n",
    "     stop_words = set(stopwords.words('english'))\n",
    "     def preprocess(text):\n",
    "         cleaned = re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', text.lower())\n",
    "         tokens = word_tokenize(cleaned)\n",
    "         return [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "     df['tokens'] = df['text'].apply(preprocess)\n",
    "     print(df['tokens'].head())\n",
    "     ```\n",
    "   - Output: Token lists (e.g., [“love”, “movie”, “great”]).\n",
    "2. **Build Bigram Model (2 hours)**:\n",
    "   - Create bigrams and count transitions using NLTK.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from nltk import bigrams\n",
    "     from collections import defaultdict, Counter\n",
    "     bigram_counts = defaultdict(Counter)\n",
    "     for tokens in df['tokens']:\n",
    "         for w1, w2 in bigrams(tokens, pad_right=True, pad_left=True):\n",
    "             bigram_counts[w1][w2] += 1\n",
    "     print(list(bigram_counts['love'].items())[:5])  # Top transitions from \"love\"\n",
    "     ```\n",
    "   - Output: e.g., [(“movie”, 10), (“great”, 8)].\n",
    "3. **Generate Text with Markov Chain (2 hours)**:\n",
    "   - Generate text by sampling from bigram transitions.\n",
    "   - Code:\n",
    "     ```python\n",
    "     import random\n",
    "     def generate_text(start_word, length=10):\n",
    "         text = [start_word]\n",
    "         for _ in range(length-1):\n",
    "             next_words = bigram_counts[text[-1]]\n",
    "             if not next_words:\n",
    "                 break\n",
    "             next_word = random.choices(list(next_words.keys()), weights=list(next_words.values()))[0]\n",
    "             text.append(next_word)\n",
    "         return ' '.join(text)\n",
    "     print(generate_text('love'))\n",
    "     ```\n",
    "   - Output: e.g., “love movie great watch”.\n",
    "4. **Compute Perplexity (2 hours)**:\n",
    "   - Evaluate the model on a small test set (simplified perplexity).\n",
    "   - Code:\n",
    "     ```python\n",
    "     test_tokens = df['tokens'].iloc[-100:].explode().tolist()  # Last 100 posts\n",
    "     log_prob = 0\n",
    "     count = 0\n",
    "     for w1, w2 in bigrams(test_tokens, pad_right=True, pad_left=True):\n",
    "         total = sum(bigram_counts[w1].values())\n",
    "         if total > 0 and w2 in bigram_counts[w1]:\n",
    "             log_prob += -np.log2(bigram_counts[w1][w2] / total)\n",
    "             count += 1\n",
    "     perplexity = 2 ** (log_prob / count if count > 0 else 1)\n",
    "     print(f\"Perplexity: {perplexity:.2f}\")\n",
    "     ```\n",
    "   - Output: e.g., Perplexity: 50 (lower is better).\n",
    "\n",
    "#### **Debugging Tips**\n",
    "- NLTK download fails? Run `nltk.download('punkt')` or `nltk.download('stopwords')`.\n",
    "- Empty bigrams? Check preprocessing (ensure tokens aren’t empty).\n",
    "- Generation stops early? Add a fallback (e.g., random word from vocabulary if no transitions).\n",
    "- Perplexity error? Ensure `total > 0` and handle zero probabilities with smoothing (add 1 to counts).\n",
    "- Memory issues? Reduce to 500 posts (`df[:500]`).\n",
    "\n",
    "#### **Resources**\n",
    "- [NLTK Language Models](https://www.nltk.org/api/nltk/lm.html): N-gram guide.\n",
    "- [NLTK Bigrams](https://www.nltk.org/api/nltk.util.html#nltk.util.bigrams): Bigram utilities.\n",
    "- [Kaggle Pandas](https://www.kaggle.com/learn/pandas): Data handling.\n",
    "- [Perplexity Intro](https://en.wikipedia.org/wiki/Perplexity): Simple explanation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mini-Project: Text Generator (3 hours)**\n",
    "\n",
    "*Goal*: Build a bigram-based Markov chain text generator for X posts, producing coherent sentences and evaluating the model, creating a portfolio piece for R&D.\n",
    "\n",
    "- **Task**: Generate 5 sentences from 1,000 X posts using a bigram model and report perplexity.\n",
    "- **Input**: `training.1600000.processed.noemoticon.csv` (first 1,000 posts).\n",
    "- **Output**: \n",
    "  - Text file with 5 generated sentences: `generated_sentences.txt`.\n",
    "  - CSV with perplexity: `model_stats.txt`.\n",
    "- **Steps**:\n",
    "  1. Preprocess X posts (clean, tokenize, remove stopwords).\n",
    "  2. Build bigram model and generate 5 sentences.\n",
    "  3. Compute perplexity on a test set (100 posts).\n",
    "  4. Save results.\n",
    "- **Example Output**:\n",
    "  - Text:\n",
    "    ```\n",
    "    love movie great\n",
    "    happy day awesome\n",
    "    good food eat\n",
    "    watch game fun\n",
    "    sad friend miss\n",
    "    ```\n",
    "  - CSV:\n",
    "    ```\n",
    "    Perplexity: 50.32\n",
    "    ```\n",
    "- **Code**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  import re\n",
    "  import nltk\n",
    "  from nltk.tokenize import word_tokenize\n",
    "  from nltk.corpus import stopwords\n",
    "  from nltk import bigrams\n",
    "  from collections import defaultdict, Counter\n",
    "  import random\n",
    "  import numpy as np\n",
    "\n",
    "  # NLTK setup\n",
    "  nltk.download('punkt')\n",
    "  # Download stopwords\n",
    "  nltk.download('stopwords')\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "\n",
    "  # Preprocess\n",
    "  def preprocess(text):\n",
    "      cleaned = re.sub(r'http\\S+|[^\\w\\s]|@[\\w]+', '', text.lower())\n",
    "      tokens = word_tokenize(cleaned)\n",
    "      return [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "\n",
    "  # Load data\n",
    "  df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None, usecols=[5])[:1000]\n",
    "  df.columns = ['text']\n",
    "  df['tokens'] = df['text'].apply(preprocess)\n",
    "\n",
    "  # Build bigram model\n",
    "  bigram_counts = defaultdict(Counter)\n",
    "  for tokens in df['tokens'][:900]:  # Train on 900\n",
    "      for w1, w2 in bigrams(tokens, pad_right=True, pad_left=True):\n",
    "          bigram_counts[w1][w2] += 1\n",
    "\n",
    "  # Generate text\n",
    "  def generate_text(start, length=10):\n",
    "      text = [start]\n",
    "      for _ in range(length-1):\n",
    "          next_words = bigram_counts[text[-1]]\n",
    "          if not next_words:\n",
    "              break\n",
    "          next_word = random.choices(list(next_words.keys()), weights=list(next_words.values()))[0]\n",
    "          text.append(next_word)\n",
    "      return ' '.join([w for w in text if w is not None]).strip()\n",
    "\n",
    "  # Generate 5 sentences\n",
    "  vocab = set(word for tokens in df['tokens'] for word in tokens)\n",
    "  sentences = [generate_text(random.choice(list(vocab))) for _ in range(5)]\n",
    "  with open('generated_sentences.txt', 'w') as f:\n",
    "      for s in sentences:\n",
    "          f.write(f\"{s}\\n\")\n",
    "\n",
    "  # Compute perplexity\n",
    "  test_tokens = df['tokens'].iloc[-100:].explode().tolist()  # Test on 100\n",
    "  log_prob = 0\n",
    "  count = 0\n",
    "  for w1, w2 in bigrams(test_tokens, pad_right=True, pad_left=True):\n",
    "      total = sum(bigram_counts[w1].values()) + len(vocab)  # Smoothing\n",
    "      prob = (bigram_counts[w1][w2] + 1) / total\n",
    "      log_prob += -np.log2(prob)\n",
    "      count += 1\n",
    "  perplexity = 2 ** (log_prob / count if count > 0 else 1)\n",
    "  with open('predictions.csv', 'w') as f:\n",
    "      f.write(f\"Perplexity:{perplexity:.2f}\\n\")\n",
    "  ```\n",
    "- **Tools**: NLTK, Pandas, random, numpy.\n",
    "- **Variation**: Try trigrams (use `nltk.trigrams`) or increase dataset size for better coherence.\n",
    "- **Debugging Tips**:\n",
    "  - Text file empty? Check if `vocab` is populated.\n",
    "  - Perplexity infinite? Add smoothing (e.g., +1 to counts).\n",
    "  - Generation incoherent? Increase training data or filter rare words.\n",
    "- **Resources**:\n",
    "  - [NLTK Language Models](https://www.nltk.org/api/nltk/lm.html).\n",
    "  - [Markov Chain Basics](https://en.wikipedia.org/wiki/Markov_chain).\n",
    "- **R&D Tip**: Add this project to your GitHub portfolio. Document preprocessing and perplexity calculation to show research rigor.\n",
    "\n",
    "---\n",
    "\n",
    "### **Checkpoints**\n",
    "\n",
    "1. **Quiz (30 minutes)**:\n",
    "   - Questions:\n",
    "     1. What is a language model, and what does it predict?\n",
    "     2. How do bigrams differ from unigrams?\n",
    "     3. What is a Markov chain in text generation?\n",
    "     4. Why is perplexity used to evaluate language models?\n",
    "   - Answers (example):\n",
    "     1. A language model predicts the probability of words or sequences based on context.\n",
    "     2. Bigrams are word pairs; unigrams are single words.\n",
    "     3. A Markov chain predicts the next word based on the current word(s) using probabilities.\n",
    "     4. Perplexity measures how well a model predicts new text (lower is better).\n",
    "   - **Task**: Write answers in a notebook or share on X with #NLP.\n",
    "\n",
    "2. **Task (30 minutes)**:\n",
    "   - Check `generated_sentences.txt`: Are sentences coherent (e.g., “love movie great” vs. random words)?\n",
    "   - Inspect `model_stats.txt`: Is perplexity reasonable (<100)? If not, add smoothing.\n",
    "   - Save files to GitHub; share on X for feedback.\n",
    "   - **R&D Connection**: Evaluating text coherence and perplexity is a research skill for model validation.\n",
    "\n",
    "---\n",
    "\n",
    "### **R&D Focus**\n",
    "\n",
    "- **Why It Matters**: Language models are central to NLP research, powering chatbots, translation, and more.\n",
    "- **Action**: Skim the introduction of [Bengio, 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) (5 minutes). Note how it discusses moving beyond N-grams.\n",
    "- **Community**: Share your generated sentences on X with #NLP or [Hugging Face Discord](https://huggingface.co/join-discord). Ask for feedback on coherence.\n",
    "- **Research Insight**: Experiment with smoothing (e.g., +1 vs. +0.1 to counts) to see its impact on perplexity, mimicking research optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Execution Plan**\n",
    "\n",
    "**Total Time**: ~15 hours (1–2 weeks, 7–10 hours/week).  \n",
    "- **Day 1–2**: Theory (4 hours). Read NLTK guide, Jurafsky Chapter 3, note N-grams and perplexity.  \n",
    "- **Day 3–5**: Practical (8 hours). Complete tasks (preprocessing, bigrams, generation, perplexity).  \n",
    "- **Day 6–7**: Mini-Project (3 hours). Build Text Generator, save text/CSV, share on GitHub/X.  \n",
    "\n",
    "**Tips for Success**:\n",
    "- **Stay Motivated**: Think about using text generation for your R&D goal (e.g., generating X post replies).  \n",
    "- **Debugging**: Search errors on [Stack Overflow](https://stackoverflow.com/) or ask in Hugging Face Discord.  \n",
    "- **Portfolio**: Add `generated_sentences.txt`, `model_stats.txt`, and code to GitHub with comments explaining steps.  \n",
    "- **Foundation Check**: If you complete the mini-project in <3 hours and generate coherent sentences, you’re ready for Chapter 7 (Syntax and Parsing).  \n",
    "- **Variation**: If you prefer another dataset, try [Reddit Comments](https://www.kaggle.com/datasets/sherinclaudia/reddit-comment-dataset) for conversational text.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Chapter is Ideal for You**\n",
    "\n",
    "- **Beginner-Friendly**: Simple explanations, step-by-step code, and free tools make language models accessible.  \n",
    "- **Practical**: Hands-on tasks and a mini-project build coding skills for research applications.  \n",
    "- **Research-Oriented**: Connects N-grams to research tasks, with paper references for R&D.  \n",
    "- **Engaging**: X posts are short and relatable, keeping your passion alive.  \n",
    "- **Structured**: Clear timeline, debugging tips, and checkpoints ensure progress.  \n",
    "\n",
    "This chapter strengthens your NLP foundation by mastering language models and N-grams, essential for R&D, while building a portfolio piece. If you want a detailed code walkthrough (e.g., perplexity), a different dataset (e.g., Reddit), or help with specific issues (e.g., incoherent generation), let me know! Ready to start with the theory or setup?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
