{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a63ef1-8f0e-4b92-a730-3026b6ac5813",
   "metadata": {},
   "source": [
    "As a beginner with a passion for NLP and a long-term goal in R&D, you‚Äôre revisiting the basics to build a strong foundation. Since you‚Äôve completed Chapter 1 (Introduction to NLP) from the updated course outline, I‚Äôll provide a detailed, beginner-friendly version of **Chapter 2: Text Preprocessing**. This chapter is critical for NLP as it teaches you how to clean and prepare text data for analysis, a foundational skill for research and practical applications. It‚Äôs designed for someone with minimal programming experience (basic Python from Chapter 1) and aligns with your R&D aspirations by emphasizing practical skills, research connections, and portfolio-building.\n",
    "\n",
    "This content includes:\n",
    "- **Theory**: Simple explanations of preprocessing techniques (tokenization, stopword removal, stemming, lemmatization, cleaning).\n",
    "- **Practical**: Step-by-step tasks using free tools (NLTK, SpaCy) and a new dataset to keep it fresh.\n",
    "- **Mini-Project**: A Text Cleaner to process IMDB reviews, reinforcing coding and preparing for R&D tasks.\n",
    "- **Resources**: Free, beginner-friendly materials.\n",
    "- **Debugging Tips**: Solutions to common issues for beginners.\n",
    "- **Checkpoints**: Quizzes and tasks to confirm mastery.\n",
    "- **R&D Focus**: Links to research concepts (e.g., preprocessing‚Äôs impact on model performance).\n",
    "\n",
    "The chapter uses a new dataset (IMDB reviews) to avoid repetition from your previous work (e.g., Twitter or Gutenberg). It‚Äôs structured for self-paced learning, with clear steps to build confidence and skills for future NLP research.\n",
    "\n",
    "**Time Estimate**: ~15 hours (spread over 1‚Äì2 weeks, 7‚Äì10 hours/week).  \n",
    "**Tools**: Free (Google Colab, NLTK, SpaCy, Pandas, regex).  \n",
    "**Dataset**: [IMDB Dataset of 50k Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) (free on Kaggle).  \n",
    "**Prerequisites**: Basic Python (lists, loops, file I/O from Chapter 1); Colab or Anaconda setup with NLTK, SpaCy, Pandas installed.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter 2: Text Preprocessing**\n",
    "\n",
    "*Goal*: Master cleaning and preparing text data for NLP tasks, understanding trade-offs between techniques, and building skills for research-grade data preparation.\n",
    "\n",
    "### **Theory (4 hours)**\n",
    "\n",
    "#### **What is Text Preprocessing?**\n",
    "- **Definition**: The process of cleaning and transforming raw text into a format suitable for NLP models.\n",
    "  - Example: Turning ‚ÄúI loved this movie!! üòä https://t.co/link‚Äù into [‚Äúlove‚Äù, ‚Äúmovie‚Äù] for analysis.\n",
    "- **Why It Matters**: Raw text (e.g., tweets, reviews) is messy (URLs, emojis, typos). Preprocessing ensures models focus on meaningful data, improving accuracy.\n",
    "- **R&D Relevance**: In research, preprocessing choices (e.g., lemmatization vs. stemming) can significantly affect model performance, a key focus in papers.\n",
    "\n",
    "#### **Key Preprocessing Techniques**\n",
    "1. **Tokenization**:\n",
    "   - Splitting text into smaller units (tokens) like words or sentences.\n",
    "   - Example: ‚ÄúI love NLP‚Äù ‚Üí Words: [‚ÄúI‚Äù, ‚Äúlove‚Äù, ‚ÄúNLP‚Äù]; Sentences: [‚ÄúI love NLP.‚Äù].\n",
    "   - Types: Word, sentence, subword (used in advanced models like BERT).\n",
    "   - Why? Tokens are the building blocks for NLP models.\n",
    "2. **Stopword Removal**:\n",
    "   - Removing common words (e.g., ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúand‚Äù) that add little meaning.\n",
    "   - Example: ‚ÄúThe movie is great‚Äù ‚Üí [‚Äúmovie‚Äù, ‚Äúgreat‚Äù].\n",
    "   - Why? Reduces noise, focusing models on important words.\n",
    "3. **Stemming**:\n",
    "   - Reducing words to their root form by removing suffixes.\n",
    "   - Example: ‚Äúrunning,‚Äù ‚Äúruns‚Äù ‚Üí ‚Äúrun‚Äù; ‚Äúbetter‚Äù ‚Üí ‚Äúbett‚Äù (not always accurate).\n",
    "   - Tool: Porter Stemmer (NLTK).\n",
    "   - Why? Fast and simple, but can lose meaning (e.g., ‚Äúbetter‚Äù ‚Üí ‚Äúbett‚Äù).\n",
    "4. **Lemmatization**:\n",
    "   - Reducing words to their dictionary form (lemma) using linguistic rules.\n",
    "   - Example: ‚Äúrunning,‚Äù ‚Äúruns‚Äù ‚Üí ‚Äúrun‚Äù; ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù.\n",
    "   - Tool: SpaCy‚Äôs lemmatizer.\n",
    "   - Why? More accurate than stemming, ideal for meaning-driven tasks like sentiment analysis.\n",
    "5. **Cleaning**:\n",
    "   - Removing noise: URLs, emojis, punctuation, special characters; converting to lowercase.\n",
    "   - Example: ‚ÄúI loved this!! üòä https://t.co/link‚Äù ‚Üí ‚Äúi loved this‚Äù.\n",
    "   - Why? Standardizes text, reducing model confusion.\n",
    "\n",
    "#### **Trade-Offs**\n",
    "- **Stemming vs. Lemmatization**:\n",
    "  - Stemming: Faster, less accurate (e.g., ‚Äúbetter‚Äù ‚Üí ‚Äúbett‚Äù).\n",
    "  - Lemmatization: Slower, more accurate (e.g., ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù).\n",
    "  - Research Insight: Lemmatization is preferred for tasks like sentiment analysis where meaning matters.\n",
    "- **Stopword Removal**: May remove context in some cases (e.g., ‚Äúnot‚Äù is a stopword but critical for sentiment).\n",
    "- **Cleaning**: Over-cleaning (e.g., removing all punctuation) can lose structure (e.g., sentence boundaries).\n",
    "\n",
    "#### **Resources**\n",
    "- [NLTK Book, Chapter 3](https://www.nltk.org/book/ch03.html): Covers tokenization, stemming, and more.\n",
    "- [SpaCy Linguistic Features](https://spacy.io/usage/linguistic-features): Beginner guide to preprocessing.\n",
    "- [Regex101](https://regex101.com/): Interactive tool to learn regex for cleaning.\n",
    "- **R&D Resource**: Skim the preprocessing section of [this sentiment analysis paper](https://arxiv.org/abs/1708.02002) (5 minutes) to see how researchers handle text cleaning.\n",
    "\n",
    "#### **Learning Tips**\n",
    "- Take notes on why lemmatization is better for sentiment analysis.\n",
    "- Search X for #NLP to see preprocessing discussions (I can analyze posts if you share links).\n",
    "- Think about how preprocessing could help your dream R&D project (e.g., cleaning X posts for bias detection).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical (8 hours)**\n",
    "\n",
    "*Goal*: Apply preprocessing techniques to a real dataset, building coding skills and confidence.\n",
    "\n",
    "#### **Setup**\n",
    "- **Environment**: Use Google Colab (free, no installation) or local Anaconda (from Chapter 1).\n",
    "- **Libraries**: Ensure installed (run in Colab or terminal):\n",
    "  ```bash\n",
    "  pip install nltk spacy pandas\n",
    "  python -m spacy download en_core_web_sm\n",
    "  ```\n",
    "  ```python\n",
    "  import nltk\n",
    "  nltk.download('punkt')\n",
    "  nltk.download('stopwords')\n",
    "  ```\n",
    "- **Dataset**: [IMDB Dataset of 50k Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n",
    "  - Download: Sign up for Kaggle, download `IMDB Dataset.csv`.\n",
    "  - Why? Movie reviews are rich, messy text (e.g., slang, emojis), perfect for practicing preprocessing.\n",
    "  - Alternative: If Kaggle access is an issue, use a smaller sample (I can provide a snippet or use [this smaller dataset](https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis)).\n",
    "- **Load Data**:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  df = pd.read_csv('IMDB Dataset.csv')[:200]  # Use 200 reviews for speed\n",
    "  print(df.head())\n",
    "  ```\n",
    "\n",
    "#### **Tasks**\n",
    "1. **Tokenization (2 hours)**:\n",
    "   - Split reviews into words and sentences using NLTK and SpaCy.\n",
    "   - Code (NLTK):\n",
    "     ```python\n",
    "     from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "     review = df['review'].iloc[0]\n",
    "     words = word_tokenize(review)\n",
    "     sentences = sent_tokenize(review)\n",
    "     print(f\"First 10 words: {words[:10]}\")\n",
    "     print(f\"First 2 sentences: {sentences[:2]}\")\n",
    "     ```\n",
    "   - Code (SpaCy):\n",
    "     ```python\n",
    "     import spacy\n",
    "     nlp = spacy.load(\"en_core_web_sm\")\n",
    "     doc = nlp(review)\n",
    "     words = [token.text for token in doc]\n",
    "     sentences = [sent.text for sent in doc.sents]\n",
    "     print(f\"First 10 words: {words[:10]}\")\n",
    "     print(f\"First 2 sentences: {sentences[:2]}\")\n",
    "     ```\n",
    "   - Compare: Note differences (e.g., SpaCy handles contractions like ‚Äúdon‚Äôt‚Äù better).\n",
    "2. **Stopword Removal (1 hour)**:\n",
    "   - Remove stopwords using NLTK or SpaCy.\n",
    "   - Code (NLTK):\n",
    "     ```python\n",
    "     from nltk.corpus import stopwords\n",
    "     stop_words = set(stopwords.words('english'))\n",
    "     filtered_words = [w for w in word_tokenize(review.lower()) if w not in stop_words]\n",
    "     print(f\"First 10 filtered words: {filtered_words[:10]}\")\n",
    "     ```\n",
    "   - Code (SpaCy):\n",
    "     ```python\n",
    "     doc = nlp(review.lower())\n",
    "     filtered_words = [token.text for token in doc if token.text not in nlp.Defaults.stop_words]\n",
    "     print(f\"First 10 filtered words: {filtered_words[:10]}\")\n",
    "     ```\n",
    "3. **Stemming (1 hour)**:\n",
    "   - Apply NLTK‚Äôs Porter Stemmer.\n",
    "   - Code:\n",
    "     ```python\n",
    "     from nltk.stem.porter import PorterStemmer\n",
    "     ps = PorterStemmer()\n",
    "     stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "     print(f\"First 10 stemmed words: {stemmed_words[:10]}\")\n",
    "     ```\n",
    "   - Example: ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù, ‚Äúbetter‚Äù ‚Üí ‚Äúbett‚Äù.\n",
    "4. **Lemmatization (2 hours)**:\n",
    "   - Use SpaCy‚Äôs lemmatizer.\n",
    "   - Code:\n",
    "     ```python\n",
    "     doc = nlp(review.lower())\n",
    "     lemmas = [token.lemma_ for token in doc if token.text not in nlp.Defaults.stop_words]\n",
    "     print(f\"First 10 lemmas: {lemmas[:10]}\")\n",
    "     ```\n",
    "   - Example: ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù, ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù.\n",
    "5. **Cleaning (2 hours)**:\n",
    "   - Remove URLs, emojis, punctuation; convert to lowercase.\n",
    "   - Code:\n",
    "     ```python\n",
    "     import re\n",
    "     cleaned_review = re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', review.lower())\n",
    "     print(f\"Cleaned review: {cleaned_review[:100]}\")\n",
    "     ```\n",
    "   - Test regex on [regex101.com](https://regex101.com/) with sample text.\n",
    "\n",
    "#### **Debugging Tips**\n",
    "- SpaCy model fails? Run `python -m spacy download en_core_web_sm` in terminal/Colab.\n",
    "- NLTK stopwords missing? Run `nltk.download('stopwords')`.\n",
    "- Regex not working? Test patterns on [regex101.com](https://regex101.com/) (e.g., `http\\S+` removes URLs).\n",
    "- Memory issues? Limit to 100 reviews (`df[:100]`) or process in batches.\n",
    "- Pandas error? Ensure CSV is in the correct directory or use `/content/IMDB Dataset.csv` in Colab.\n",
    "\n",
    "#### **Resources**\n",
    "- [NLTK Book, Chapter 3](https://www.nltk.org/book/ch03.html): Tokenization and stemming.\n",
    "- [SpaCy Preprocessing](https://spacy.io/usage/linguistic-features#tokenization): Lemmatization guide.\n",
    "- [Kaggle Pandas Tutorial](https://www.kaggle.com/learn/pandas): Loading CSVs.\n",
    "- [Regex101](https://regex101.com/): Interactive regex testing.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mini-Project: Text Cleaner (3 hours)**\n",
    "\n",
    "*Goal*: Create a preprocessing pipeline to clean and transform IMDB reviews, producing a CSV for analysis and portfolio-building.\n",
    "\n",
    "- **Task**: Process 200 IMDB reviews, applying cleaning, tokenization, stopword removal, and lemmatization; save results to a CSV.\n",
    "- **Input**: `IMDB Dataset.csv` (first 200 reviews).\n",
    "- **Output**: CSV with columns: `original_text`, `cleaned_text`, `tokens`, `lemmas`.\n",
    "- **Steps**:\n",
    "  1. Load reviews with Pandas.\n",
    "  2. Clean: Remove URLs, emojis, punctuation; lowercase.\n",
    "  3. Tokenize and remove stopwords with SpaCy.\n",
    "  4. Lemmatize with SpaCy.\n",
    "  5. Save to `cleaned_reviews.csv`.\n",
    "- **Example Output** (CSV snippet):\n",
    "  ```csv\n",
    "  original_text,cleaned_text,tokens,lemmas\n",
    "  \"I loved this movie!! üòä https://t.co/link\",\"i loved this movie\",\"['loved', 'movie']\",\"['love', 'movie']\"\n",
    "  \"Great film, but too long!\",\"great film but too long\",\"['great', 'film', 'long']\",\"['great', 'film', 'long']\"\n",
    "  ```\n",
    "- **Code**:\n",
    "  ```python\n",
    "  import spacy\n",
    "  import pandas as pd\n",
    "  import re\n",
    "\n",
    "  # Load SpaCy\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "  # Load data\n",
    "  df = pd.read_csv('IMDB Dataset.csv')[:200]  # Adjust path\n",
    "\n",
    "  # Preprocessing pipeline\n",
    "  cleaned_data = []\n",
    "  for review in df['review']:\n",
    "      # Clean\n",
    "      cleaned = re.sub(r'http\\S+|[^\\x00-\\x7F]+|[.,!?]', '', review.lower())\n",
    "      # Process with SpaCy\n",
    "      doc = nlp(cleaned)\n",
    "      # Tokens (no stopwords)\n",
    "      tokens = [token.text for token in doc if token.text not in nlp.Defaults.stop_words and token.is_alpha]\n",
    "      # Lemmas\n",
    "      lemmas = [token.lemma_ for token in doc if token.text not in nlp.Defaults.stop_words and token.is_alpha]\n",
    "      cleaned_data.append([review, cleaned, tokens, lemmas])\n",
    "\n",
    "  # Save to CSV\n",
    "  output_df = pd.DataFrame(cleaned_data, columns=['original_text', 'cleaned_text', 'tokens', 'lemmas'])\n",
    "  output_df.to_csv('cleaned_reviews.csv', index=False)\n",
    "  print(output_df.head())\n",
    "  ```\n",
    "- **Tools**: SpaCy, Pandas, `re` (regex).\n",
    "- **Variation**: If you used NLTK in Chapter 1, focus on SpaCy. Alternatively, try NLTK for stemming:\n",
    "  ```python\n",
    "  from nltk.stem.porter import PorterStemmer\n",
    "  ps = PorterStemmer()\n",
    "  stems = [ps.stem(w) for w in tokens]\n",
    "  ```\n",
    "- **Debugging Tips**:\n",
    "  - CSV not saving? Use absolute path (e.g., `/content/cleaned_reviews.csv` in Colab).\n",
    "  - SpaCy slow? Process in batches (e.g., 50 reviews at a time).\n",
    "  - Empty tokens? Check if stopwords are being removed correctly.\n",
    "- **Resources**:\n",
    "  - [SpaCy Tokenization](https://spacy.io/usage/linguistic-features#tokenization).\n",
    "  - [Pandas CSV Guide](https://ÂàÜÂºÄ\n",
    "\n",
    "System: - **Kaggle Pandas Tutorial**](https://www.kaggle.com/learn/pandas): Saving data to CSV.  \n",
    "- **R&D Tip**: Add this project to your GitHub portfolio. Document the preprocessing steps to showcase research skills.\n",
    "\n",
    "---\n",
    "\n",
    "### **Checkpoints**\n",
    "\n",
    "1. **Quiz (30 minutes)**:\n",
    "   - Questions:\n",
    "     1. What is text preprocessing, and why is it important?\n",
    "     2. How does tokenization differ from sentence splitting?\n",
    "     3. Why is lemmatization preferred over stemming for sentiment analysis?\n",
    "     4. What does cleaning remove from text data?\n",
    "   - Answers (example):\n",
    "     1. Preprocessing cleans and formats text for NLP models, improving accuracy.\n",
    "     2. Tokenization splits text into words; sentence splitting divides text into sentences.\n",
    "     3. Lemmatization preserves meaning (e.g., ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù), which is key for sentiment.\n",
    "     4. Cleaning removes URLs, emojis, punctuation, and converts to lowercase.\n",
    "   - **Task**: Write answers in a notebook or share on X with #NLP for feedback.\n",
    "\n",
    "2. **Task (30 minutes)**:\n",
    "   - Compare stemming vs. lemmatization for 5 words (e.g., ‚Äúrunning,‚Äù ‚Äúbetter,‚Äù ‚Äúmovies,‚Äù ‚Äúloved,‚Äù ‚Äúgoing‚Äù).\n",
    "     - Example: Stemming: ‚Äúrunning‚Äù ‚Üí ‚Äúrun,‚Äù ‚Äúbetter‚Äù ‚Üí ‚Äúbett‚Äù; Lemmatization: ‚Äúrunning‚Äù ‚Üí ‚Äúrun,‚Äù ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù.\n",
    "   - Check CSV output (`cleaned_reviews.csv`). Ensure tokens/lemmas are correct (e.g., no stopwords).\n",
    "   - **R&D Connection**: Verify preprocessing quality, as researchers do to ensure model reliability.\n",
    "\n",
    "---\n",
    "\n",
    "### **R&D Focus**\n",
    "\n",
    "- **Why It Matters**: Preprocessing is a critical step in NLP research, as it directly impacts model performance (e.g., poor cleaning can introduce noise).\n",
    "- **Action**: Skim the preprocessing section of [this sentiment analysis paper](https://arxiv.org/abs/1708.02002) (5 minutes). Note how they handle stopwords and lemmatization for sentiment tasks.\n",
    "- **Community**: Share your CSV output or preprocessing code on X with #NLP or [Hugging Face Discord](https://huggingface.co/join-discord). Ask for feedback on preprocessing choices (I can analyze responses if you share links).\n",
    "- **Research Insight**: Experiment with keeping vs. removing stopwords (e.g., ‚Äúnot‚Äù) to see the impact on a simple word count analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Execution Plan**\n",
    "\n",
    "**Total Time**: ~15 hours (1‚Äì2 weeks, 7‚Äì10 hours/week).  \n",
    "- **Day 1‚Äì2**: Theory (4 hours). Read NLTK Book Chapter 3, SpaCy guide, take notes on trade-offs.  \n",
    "- **Day 3‚Äì5**: Practical (8 hours). Complete tasks (tokenization, stopword removal, stemming, lemmatization, cleaning).  \n",
    "- **Day 6‚Äì7**: Mini-Project (3 hours). Build Text Cleaner, save CSV, share on GitHub/X.  \n",
    "\n",
    "**Tips for Success**:\n",
    "- **Stay Motivated**: Think about using preprocessing for your dream R&D project (e.g., cleaning X posts for sentiment analysis).  \n",
    "- **Debugging**: Search errors on [Stack Overflow](https://stackoverflow.com/) or ask in Hugging Face Discord.  \n",
    "- **Portfolio**: Add `cleaned_reviews.csv` and code to your GitHub repo with comments explaining each step.  \n",
    "- **Foundation Check**: If you complete the mini-project in <3 hours and understand quiz answers, you‚Äôre ready for Chapter 3.  \n",
    "- **Variation**: If you used Twitter data previously, IMDB reviews offer a new challenge (e.g., longer, more complex text).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Chapter is Ideal for You**\n",
    "\n",
    "- **Beginner-Friendly**: Simple explanations, step-by-step code, and free tools make preprocessing accessible.  \n",
    "- **Practical**: Hands-on tasks and a mini-project build coding skills for research data preparation.  \n",
    "- **Research-Oriented**: Connects preprocessing to model performance, a key R&D focus, with paper references.  \n",
    "- **Engaging**: IMDB reviews are relatable (movie-related), keeping your passion alive.  \n",
    "- **Structured**: Clear timeline, debugging tips, and checkpoints ensure progress.  \n",
    "\n",
    "This chapter builds a strong foundation in text preprocessing, a core skill for NLP R&D, while keeping you motivated with practical, portfolio-worthy work. If you want a detailed code walkthrough, a different dataset (e.g., Reddit), or help with specific issues (e.g., regex), let me know! Ready to start with the theory or setup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec51fe6-6d15-4e5c-86d6-b1782f40cb66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
